{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced Data-Driven Methods for Mapping Arctic Sea Ice Roughness \n",
    "## RS Course Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Presenting some initial results \n",
    "- To present in our group meeting to have your feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some background about the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Association between sea ice roughness and ice thickness, ice age, and melt pond extent, sea ice albedo.\n",
    "- Roughness introduces anisotropic reflectance patterns in MISR data\n",
    "- Modeling sea ice roughness is a nonlinear problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### importnace of sea ice and MISR and why to model it\n",
    "\n",
    "### what association is? plots? pictures? maps?  MISR image? MISR sea ice? Landi paper? whyt seaice is important? albedo, season comparispn? why ML will help us in this problem? high level not in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Main hypothesis\n",
    "\n",
    "- If nonlinear algorithms can improve our modeling results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Experiment\n",
    "\n",
    "- To compare different linear vs. nonlinear algorithms on sea ice roughness dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics to be discussed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- sea ice surface roughness modeling as a regression problem\n",
    "\n",
    "- Regression vs. classification problems\n",
    "\n",
    "- Linera vs. Nonlinear problems\n",
    "\n",
    "- Preprocesing input dataset (remove bad data; Outliers detection and removal ...)\n",
    "\n",
    "- Split dataset to train and test\n",
    "\n",
    "- Implimenting and experimenting different machine learning algorithms\n",
    "\n",
    "- RISE library for presenting coding experiments in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression algorithms that were implemented and tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1- Multiple Linear Regression\n",
    "\n",
    "2- SVM (SVR)\n",
    "\n",
    "3- Polynomial Linear Regression\n",
    "\n",
    "4- Decision Tree\n",
    "\n",
    "5- Random Forest\n",
    "\n",
    "6- Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest run on: 2022-06-01 14:41:28.971759\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "\n",
    "print('latest run on:', today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVyt9PtvxV_o",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "crdwNm-rwtLp",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py:109\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, docstring, rcsetup\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning, sanitize_sequence\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mplDeprecation  \u001b[38;5;66;03m# deprecated\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/matplotlib/rcsetup.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Don't let the original cycler collide with our validating cycler\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/matplotlib/fontconfig_pattern.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyparsing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Literal, ZeroOrMore, Optional, Regex, StringEnd,\n\u001b[1;32m     16\u001b[0m                        ParseException, Suppress)\n\u001b[1;32m     18\u001b[0m family_punc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-:,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m family_unescape \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m([\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m])\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m family_punc)\u001b[38;5;241m.\u001b[39msub\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pyparsing/__init__.py:122\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _builtin_exprs \u001b[38;5;28;01mas\u001b[39;00m core_builtin_exprs\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _builtin_exprs \u001b[38;5;28;01mas\u001b[39;00m helper_builtin_exprs\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01municode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unicode_set, UnicodeRangeList, pyparsing_unicode \u001b[38;5;28;01mas\u001b[39;00m unicode\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pyparsing/helpers.py:674\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    669\u001b[0m any_open_tag, any_close_tag \u001b[38;5;241m=\u001b[39m make_html_tags(\n\u001b[1;32m    670\u001b[0m     Word(alphas, alphanums \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mset_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many tag\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    671\u001b[0m )\n\u001b[1;32m    673\u001b[0m _htmlEntityMap \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m html\u001b[38;5;241m.\u001b[39mentities\u001b[38;5;241m.\u001b[39mhtml5\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 674\u001b[0m common_html_entity \u001b[38;5;241m=\u001b[39m \u001b[43mRegex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&(?P<entity>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m|\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_htmlEntityMap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m);\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mset_name(\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommon HTML entity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    676\u001b[0m )\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace_html_entity\u001b[39m(t):\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;124;03m\"\"\"Helper parser action to replace common HTML entities with their special characters\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pyparsing/core.py:2883\u001b[0m, in \u001b[0;36mRegex.__init__\u001b[0;34m(self, pattern, flags, as_group_list, as_match, asGroupList, asMatch)\u001b[0m\n\u001b[1;32m   2880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m=\u001b[39m flags\n\u001b[1;32m   2882\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2883\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mre \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreString \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sre_constants\u001b[38;5;241m.\u001b[39merror:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/re.py:252\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(pattern, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sre_compile\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msre_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/sre_compile.py:764\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[1;32m    763\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[0;32m--> 764\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43msre_parse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/sre_parse.py:948\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    945\u001b[0m state\u001b[38;5;241m.\u001b[39mstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSRE_FLAG_VERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Verbose:\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;66;03m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;66;03m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[1;32m    952\u001b[0m     state \u001b[38;5;241m=\u001b[39m State()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/sre_parse.py:443\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    441\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     itemsappend(\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/sre_parse.py:834\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(err\u001b[38;5;241m.\u001b[39mmsg, \u001b[38;5;28mlen\u001b[39m(name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    832\u001b[0m sub_verbose \u001b[38;5;241m=\u001b[39m ((verbose \u001b[38;5;129;01mor\u001b[39;00m (add_flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_VERBOSE)) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    833\u001b[0m                \u001b[38;5;129;01mnot\u001b[39;00m (del_flags \u001b[38;5;241m&\u001b[39m SRE_FLAG_VERBOSE))\n\u001b[0;32m--> 834\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing ), unterminated subpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    837\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m start)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/sre_parse.py:443\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    441\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     itemsappend(\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/sre_parse.py:500\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    498\u001b[0m subpatternappend \u001b[38;5;241m=\u001b[39m subpattern\u001b[38;5;241m.\u001b[39mappend\n\u001b[1;32m    499\u001b[0m sourceget \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mget\n\u001b[0;32m--> 500\u001b[0m sourcematch \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\n\u001b[1;32m    501\u001b[0m _len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m\n\u001b[1;32m    502\u001b[0m _ord \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#- main machine learning libraries used\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras  \n",
    "import sklearn\n",
    "\n",
    "#- other libraries used\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDmOxLu6U61B",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Setting pipeline parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XasFrGzqw76a",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "remove_outlier_manual = False\n",
    "remove_camera_outlier = False \n",
    "# deleting_rows_from = 100000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_linear_regression = True\n",
    "SVM = False\n",
    "polynomial = False\n",
    "decision_tree = False\n",
    "random_forest = False\n",
    "train_MLP = False\n",
    "mlp_ann_predict = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMpLz1f60xM1",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## check tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u85U_Pn5xbZ5",
    "outputId": "b791d155-d3a1-41d4-b5d6-3e2c969277c3",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print (tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1YdIGjAR2T5",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def remove_outliers_IQR(df, cutoff):\n",
    "    df.drop((df[df['rms'] > cutoff].index), inplace=True)\n",
    "    print('Removing outliers (IQR method): after dropping roughness values > %.1f in dataset; new shape= (%s,%s)' %(cutoff, df.shape[0], df.shape[1]))\n",
    "    return  # the same as None, cuz inplace=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_JXBEqkh4j4",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# def plotting():\n",
    "\n",
    "#   fig, ax = plt.subplots()\n",
    "\n",
    "#   ax.scatter(y_test, y_predict_mlr, edgecolors=(0, 0, 1))\n",
    "#   ax.plot([y_test.min(), y_test.max()], [y_predict_mlr.min(), y_predict_mlr.max()], 'r--', lw=3)\n",
    "#   # ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3) # changed to tes range for better plot\n",
    "\n",
    "#   ax.set_xlabel('Actual (y-test, obs) roughness')\n",
    "#   ax.set_ylabel('Predicted roughness')\n",
    "#   plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8UR3QFq1RQL",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## check how much memory is allocated to this ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6texdAoI1Rdf",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# !cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io_EP02Dz5P6",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## mount Google Drive and import dataset\n",
    "if we put dataset in Google Drive we need to mount the drive to be able to read data from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_sHrZpW0z1hY",
    "outputId": "b4e32672-ad28-4469-9546-d56fb1414229",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# # drive.flush_and_unmount() ???\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbPl2xY20gDj",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Check contents in GDrive or on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSomTsya0Vms",
    "outputId": "f7141b7a-97a4-4f68-a3eb-3718996179f7",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioCFXx_q0jne",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir = '/Users/ehsanmos/Documents/RnD/MISR_lab/ML_research'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huEUU62g1GIU",
    "outputId": "a2210b07-8cda-469b-904b-6c72fce12551",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isdir(dataset_dir) == False:\n",
    "    print(\"dataset directory NOT found!\")\n",
    "else:\n",
    "    print(\"dataset directory FOUND!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-sg-8CM2Wua",
    "outputId": "7a35e065-778e-4c5f-e950-0a19dfcd8ab5",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jws7ROFWCM22",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load input/ training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_ds = \"dataset_atmcalibrated_july_2016_3cameras.csv\"\n",
    "\n",
    "# in_ds = \"april_2016_9cam_4bands_final_merged_dataset_2.csv\"\n",
    "\n",
    "# in_ds = \"merged_april_2016_9cam_4bands_final_dataset_2.csv\"\n",
    "\n",
    "# in_ds = \"merged_april_2016_9cam_4bands_final_dataset_2_smallerDataSet.csv\"\n",
    "\n",
    "# in_ds = \"filtered_11_features_april2016.csv\"\n",
    "\n",
    "in_ds = \"filtered_positiveValues_11cameras_april2016.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### check if input dataset file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "in_ds_fullpath = os.path.join(dataset_dir, in_ds)\n",
    "print(in_ds_fullpath)\n",
    "\n",
    "if (not os.path.isfile(os.path.join(in_ds_fullpath))):\n",
    "    raise SystemExit()\n",
    "else:\n",
    "    print(\"input dataset found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read in dataset and look at dataset columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv(in_ds_fullpath, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop rows to make dataset smaller\n",
    "\n",
    "# df_total_rows = df_orig.shape[0]\n",
    "\n",
    "# df = df_orig.drop(labels=range(deleting_rows_from, df_total_rows), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EJwSq5T3eZf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocesing input dataset\n",
    "\n",
    "* clean the whole dataset before splitting to train-validate-test. To check quality of input dataset, we look at each column of dataset first\n",
    " remove missing values and fill values (and maybe detect and remove outliers?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giB2TEuD__Kk",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### drop unnecessary columns and clean dataset (keeping only the columns we need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### July 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2yn1waB__WF",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# #### July 2016\n",
    "\n",
    "# dataset_df = df.drop(['#path', ' orbit', ' img_block', ' line', ' sample', ' lat', ' lon', ' weight', ' npts', ' cloud', ' var'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### April 2016- several columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##- April 2016- several columns\n",
    "##- w/ 3 cameras\n",
    "# dataset_df = df.drop(['path', 'orbit', 'block', 'line', 'sample', 'lat', 'lon', 'Da_r', 'Ba_r', 'Aa_r', 'An_g', 'An_b', 'An_nir','Af_r', 'Bf_r', 'Df_r'], axis=1)\n",
    "\n",
    "##- w/ 9 cameras\n",
    "dataset_df = df.drop(['path', 'orbit', 'block', 'line', 'sample', 'lat', 'lon', 'An_g', 'An_b', 'An_nir'], axis=1)\n",
    "\n",
    "dataset_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##- rename columns\n",
    "##- for April 2016 + 3 cameras dataset, we rename column names to to match with the rest of the code here\n",
    "\n",
    "# dataset_df.rename(columns={'Ca_r':'ca', 'An_r':'an', 'Cf_r':'cf', 'mean_ATM_roughness':'rms'}, inplace=True)\n",
    "\n",
    "dataset_df.rename(columns={'mean_ATM_roughness':'rms'}, inplace=True)\n",
    "\n",
    "\n",
    "dataset_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffle dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "dataset_df = shuffle(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# visualize columns of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyul2Wmo0fY3",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### check what server/device we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSPsT5570OHU",
    "outputId": "24c7fb68-a8cb-4c1d-d365-2025bdcdb63a",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# device_lib.list_local_devices() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2rmt8RuHd1D",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## check DataFrame columns (& add column labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KE8q5uepA4uQ",
    "outputId": "2f9d0d14-587d-47b1-fe4b-40a03317fd37",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset_df.columns)\n",
    "print('shape of dataset df = (%s,%s)' %dataset_df.shape)\n",
    "\n",
    "#~ we add column labels to dataset\n",
    "# dataset_df.columns = ['an', 'ca', 'cf', 'rms']\n",
    "# print(dataset_df.columns)\n",
    "\n",
    "#~ another method is:\n",
    "# df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNUhI-XL35ZK",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visual check of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vPcQYdRGgMi",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "look at stats of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gj27kFw8Ggy3",
    "outputId": "bff821f0-4d21-4188-de97-ea03550282b9",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('AN, before cleaning bad data:')\n",
    "print('min: %s' % dataset_df['An_r'].min())\n",
    "print('max: %s' % dataset_df['An_r'].max())\n",
    "print('mean: %s' %dataset_df['An_r'].mean())\n",
    "print('median: %s' %dataset_df['An_r'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CA, before cleaning bad data:')\n",
    "print('min: %s' % dataset_df['Ca_r'].min())\n",
    "print('max: %s' % dataset_df['Ca_r'].max())\n",
    "print('mean: %s' %dataset_df['Ca_r'].mean())\n",
    "print('median: %s' %dataset_df['Ca_r'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CF, before cleaning bad data:')\n",
    "print('min: %s' % dataset_df['Cf_r'].min())\n",
    "print('max: %s' % dataset_df['Cf_r'].max())\n",
    "print('mean: %s' %dataset_df['Cf_r'].mean())\n",
    "print('median: %s' %dataset_df['Cf_r'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMS, before cleaning bad data:')\n",
    "print('min: %s' % dataset_df['rms'].min())\n",
    "print('max: %s' % dataset_df['rms'].max())\n",
    "print('mean: %s' %dataset_df['rms'].mean())\n",
    "print('median: %s' %dataset_df['rms'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeAYM1bDeStV",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### plot histogram of all columns of input dataset to visually check distribution of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "mjRLPMrn3vfh",
    "outputId": "8d6eb718-858a-438c-d617-1e59fa3c57dd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df.hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMeGOQAZHlGD",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## remove bad data from each row\n",
    "- we want to train our models, so we will remove negative values, NAN values, masked values such as land mask, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ubq-v0kHHlS3",
    "outputId": "3a82f2cf-dcdc-42e4-a445-1e92ccc8e91c",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('before dropping negative values: (%s, %s)' %dataset_df.shape)\n",
    "\n",
    "# #- remove/ drop negative values from all input features\n",
    "# dataset_df.drop((dataset_df[dataset_df['an'] < 0.0].index) | \n",
    "#                 (dataset_df[dataset_df['ca'] < 0.0].index) | \n",
    "#                 (dataset_df[dataset_df['cf'] < 0.0].index), inplace=True)\n",
    "\n",
    "\n",
    "      \n",
    "#- remove/ drop negative values from all input features\n",
    "dataset_df.drop((dataset_df[dataset_df['Da_r'] < 0.0].index) | \n",
    "                (dataset_df[dataset_df['Ca_r'] < 0.0].index) | \n",
    "                (dataset_df[dataset_df['Ba_r'] < 0.0].index) |\n",
    "                (dataset_df[dataset_df['Aa_r'] < 0.0].index) |\n",
    "                (dataset_df[dataset_df['An_r'] < 0.0].index) |\n",
    "                (dataset_df[dataset_df['Af_r'] < 0.0].index) |\n",
    "                (dataset_df[dataset_df['Bf_r'] < 0.0].index) |\n",
    "                (dataset_df[dataset_df['Cf_r'] < 0.0].index) |\n",
    "                (dataset_df[dataset_df['Df_r'] < 0.0].index), inplace=True)\n",
    "\n",
    "\n",
    "print('after dropping negative values: (%s, %s)' %dataset_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "######################################################## another method\n",
    "# new_df.drop(new_df[new_df[:<0,:]], inplace=True)\n",
    "# new_df[[' an']] > 1\n",
    "# new_df[[' ca', ' an']] > 1\n",
    "# new_df[' an'][1:5]\n",
    "# new_df[[' an',' ca']].shape\n",
    "# new_df[[' an',' ca']].head\n",
    "# print(new_df[new_df[[' an', ' ca', ' cf']] < 0])\n",
    "# new_df[new_df[' an'] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl9ZxFeJe4uN",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outliers detection and removal\n",
    "\n",
    "- identify and remove outliers in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removal of outliers of reflectance in cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_outlier = 2.0\n",
    "\n",
    "\n",
    "if (remove_camera_outlier == True):\n",
    "    print(\"removing camera outliers\")\n",
    "    dataset_df.drop(dataset_df[(dataset_df['Da_r'] > cam_outlier) |\n",
    "                               (dataset_df['Ca_r'] > cam_outlier) |\n",
    "                               (dataset_df['Ba_r'] > cam_outlier) |\n",
    "                               (dataset_df['Aa_r'] > cam_outlier) |\n",
    "                               (dataset_df['An_r'] > cam_outlier) |\n",
    "                               (dataset_df['Af_r'] > cam_outlier) |\n",
    "                               (dataset_df['Bf_r'] > cam_outlier) |\n",
    "                               (dataset_df['Cf_r'] > cam_outlier) |\n",
    "                               (dataset_df['Df_r'] > cam_outlier)].index, inplace=True)\n",
    "\n",
    "else:\n",
    "    print('we did NOT remove camera outliers manually!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbrKwaiFUfSI",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Method 1: Manual removal\n",
    "\n",
    "removing roughness values > 200 cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1XZ69gnUfe8",
    "outputId": "9483b40b-66b9-443c-e4aa-c1cf42f4e6fc",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if (remove_outlier_manual == True):\n",
    "    \n",
    "    dataset_df.drop((dataset_df[dataset_df['rms'] > 200].index), inplace=True)\n",
    "    print('removing outliers: after dropping values > 200: (%s, %s)' %dataset_df.shape)\n",
    "else:\n",
    "    print('we did NOT remove roughness outliers manually!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50ODBArmDq4x",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Method 2: Interquartile Range Method (IQR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Method: find 25 and 75 percentiles in data, calculate inter-quartile range (IQR), set the cut off threshold, and remove outliers from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IXHAPzdDrFL",
    "outputId": "58df4755-27d6-4efe-9c61-d8957e1c999d",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#- identify outliers in roughnes valueswith interquartile range (IQRange)\n",
    "from numpy import percentile\n",
    "\n",
    "#- extract data \n",
    "data = dataset_df.loc[:,'rms']\n",
    "print(type(data))\n",
    "#- calculate InterQuartile Range == IQR\n",
    "q25, q75 = percentile(data, 25), percentile(data, 75)\n",
    "iqr = q75 - q25\n",
    "print('Percentiles: 25th= %.3f (cm), 75th= %.3f (cm), InterQuartile Range= %.3f (cm)' % (q25, q75, iqr))\n",
    "#- calculate the outlier cutoff\n",
    "cut_off = iqr * 3\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "print('lower cutoff= %.1f, upper cutoff= %.1f' % (lower, upper))\n",
    "#- identify outliers\n",
    "outliers = [x for x in data if x > upper] # list comprehension method\n",
    "print('Identified outliers: %d \\n' % len(outliers))\n",
    "#- remove outliers\n",
    "remove_outliers_IQR(dataset_df, upper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2USTa_xQCWG",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "what is the difference between & and | and ( | and or) in this condition?\n",
    "\n",
    "\n",
    "do we need to use & or | here? to remove everty possibility?\n",
    "we should use | because we should delete any instance of condition < 0.\n",
    "\n",
    "source: https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "MwAGyALj_Q87",
    "outputId": "a3c35ff3-535c-4383-efd0-b2fae987a6dd",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G115-QA_B9cO",
    "outputId": "8e70a7ef-4455-4f1f-b166-23401a9734a2",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx8Goyq9YZBD",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## check for any NAN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yADlJLtYZMw",
    "outputId": "b8d72e85-1ff8-407b-f4eb-ac253ac2e62b",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#~ check all columns all together\n",
    "\n",
    "# if (dataset_df['an'].isnull().values.any() | \n",
    "#     dataset_df['ca'].isnull().values.any() | \n",
    "#     dataset_df['cf'].isnull().values.any()):\n",
    "\n",
    "\n",
    "if (dataset_df['Da_r'].isnull().values.any() | \n",
    "    dataset_df['Ca_r'].isnull().values.any() | \n",
    "    dataset_df['Ba_r'].isnull().values.any() |\n",
    "    dataset_df['Aa_r'].isnull().values.any() |\n",
    "    dataset_df['An_r'].isnull().values.any() |\n",
    "    dataset_df['Af_r'].isnull().values.any() |\n",
    "    dataset_df['Bf_r'].isnull().values.any() |\n",
    "    dataset_df['Cf_r'].isnull().values.any() |\n",
    "    dataset_df['Df_r'].isnull().values.any()):\n",
    "    \n",
    "    print('found NAN in one DF column')\n",
    "\n",
    "# #~ check each column seperately\n",
    "# if (new_df[' an'].isnull().values.any()):\n",
    "#   print('found Nan in AN')\n",
    "# elif (new_df[' ca'].isnull().values.any()):\n",
    "#   print('found Nan in CA')\n",
    "# elif (new_df[' cf'].isnull().values.any()):\n",
    "#   print('found Nan in CF')\n",
    "\n",
    "else:\n",
    "    print('did not find any NAN value in any column\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12ChS5WneFAy",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## check for any negative value\n",
    "values() function comes from numpy and is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNiAmXx1eFO3",
    "outputId": "2d0eb523-f04a-4505-ae81-3e81f19119c7",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#~ double check for any negative values in all columns\n",
    "if ((dataset_df.values < 0).any()):\n",
    "    print('found some negative values in DF! something is wrong!')\n",
    "else:\n",
    "    print('double check: no negative value found anywhere in DF!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFyiPIa3LqI8",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## histogram of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsUs7GaP4SA-",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "defining histogram bins and checking the hostogram of input data again after cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hT12p-bW4ApL",
    "outputId": "956b2c39-f672-454f-a872-806aecbea3f1",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df.hist(bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generate descriptive statistics of dataset\n",
    "\n",
    "Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUgh2eIFjIsx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Split dataset to train-test parts for training algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y23B8Yf2N1_o",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we devided to plit our dataset to 2 parts (2-part split)\n",
    "- Here we use the ‘train_test_split’ to split the data in 80:20 ratio i.e. 80% of the data will be used for training the model while 20% will be used for testing the model that is built out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "source: https://towardsdatascience.com/splitting-a-dataset-e328dab2760a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tb8rsZQYjI31",
    "outputId": "1938a423-8292-4a07-d1bf-6db524e85d1c",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print(new_df.head)\n",
    "\n",
    "X = dataset_df.iloc[:, :-1] # to select up to last column of dataset OR [:, 0:3]\n",
    "Y = dataset_df.iloc[:, -1:] # to select last column of DF\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxmeC3JmJuAo",
    "outputId": "125c869e-93eb-4a0c-ae68-f182cdd29545",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# print(X.head)\n",
    "# print(Y.head)\n",
    "\n",
    "#~ Q- what happened to Y columns?\n",
    "# print(type(X))\n",
    "# print(type(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TmMsOiEY-_H",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "questions:\n",
    "\n",
    "Q- how devide dataset for train-test step?\n",
    "\n",
    "Q- how about train-val-test (3 sections)? is this for DL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5m2IrZKNSsE",
    "outputId": "76ecadb1-7223-4d1f-9a6e-b95b717f9f98",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#- we use this function to split data-- from here because we are usiong SKlearn library, we change all data structures from Pandas DF to numpy\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), Y.to_numpy(), test_size=0.2, random_state=123) # Q- input is DF or numpy array?\n",
    "\n",
    "test_data_size = 0.3\n",
    "print(\"test size= %d percent\" %(test_data_size*100))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_data_size, random_state=123) # Q- input is DF or numpy array?\n",
    "\n",
    "print(\"train:\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"test:\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZWgKdiw1Gt5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Feature scalling\n",
    "this step should be done after splitting data to train-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using MinMaxScaler method to rescale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvPJ3Jwg00i9",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "##- import necessary libraries for Neural Nets\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzudZ8iKWvkS",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# # fit data\n",
    "# scaler_x = MinMaxScaler()\n",
    "# scaler_y = MinMaxScaler()\n",
    "\n",
    "# scaler_x.fit(X_train) # returns Fitted scaler\n",
    "# X_train_scaled = scaler_x.transform(X_train)  # transforms data\n",
    "\n",
    "# scaler_x.fit(X_test)\n",
    "# X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "# scaler_y.fit(y_train)\n",
    "# y_train_scaled = scaler_y.transform(y_train)\n",
    "\n",
    "# scaler_y.fit(y_test)\n",
    "# y_test_scaled = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ls-PHcew7Qml",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Check types of input dataset data structure; should be 2D arrays, or Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emBtILM57Qw7",
    "outputId": "cab35123-8aca-42ed-f5e4-bc4b29596801",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print(type(X_test))\n",
    "print(type(y_test))\n",
    "\n",
    "# print('\\n')\n",
    "# print(X_train.head)\n",
    "# print('\\n')\n",
    "# print(y_train.head)\n",
    "# print(X_test.head)\n",
    "# print(y_test.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H1J2GNujN0c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Investigate different ML algorithms for our regression problem (linear vs. nonlinear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algorithms implemented:\n",
    "\n",
    "1- Multiple Linear Regression\n",
    "\n",
    "2- SVM (SVR)\n",
    "\n",
    "3- Polynomial Linear Regression\n",
    "\n",
    "4- Decision Tree\n",
    "\n",
    "5- Random Forest\n",
    "\n",
    "6- Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "source: https://developer.ibm.com/technologies/data-science/tutorials/learn-regression-algorithms-using-python-and-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7t-sHlVpIwD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1- Multiple Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKzwDcbcQH3o",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "we will implement some regression models and will try them on our dataset.\n",
    "\n",
    "source: https://medium.com/analytics-vidhya/5-regression-algorithms-you-need-to-know-theory-implementation-37993382122d\n",
    "\n",
    "here we use a linear regression model to test our hypothesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHgE7duU4njT",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if (multiple_linear_regression == True):\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    ##- Create linear regression object \n",
    "    mlr_model = LinearRegression() # create an estimator obj\n",
    "    ##- Train the model using the training sets\n",
    "    ##- here we use X_train(3 features)\n",
    "    mlr_model.fit(X_train, y_train) # fit(X,y) will fit data to our algortihm and makes it learn from data == training step\n",
    "\n",
    "\n",
    "#     mlr_model_score = mlr_model.score(X_test, y_test) # R2  of self.predict(X) wrt. y.\n",
    "#     print('Accuracy (R2 of prediction): %.1f percent' %(mlr_model_score*100)) # shows the strength of linear relationship between 2 variables: observations and predictions\n",
    "\n",
    "    print('Intercept: %.2f' %mlr_model.intercept_)\n",
    "    print('model coefficients: %s' %mlr_model.coef_)     # ???? Estimated coefficients for the linear regression problem\n",
    "\n",
    "    #~ Predict using the linear model\n",
    "    y_pred_mlr = mlr_model.predict(X_test)\n",
    "#     y_predict_mlr_original = scaler_y.inverse_transform(y_pred_mlr)\n",
    "    \n",
    "    ##- here we use X_test w/3 features\n",
    "    \"\"\"Return the coefficient of determination (R2)  of the prediction; relative measure of how well the model fits dependent variables\n",
    "    \"\"\" \n",
    "    mlr_model_score = r2_score(y_test, y_pred_mlr)\n",
    "    print('Accuracy (R2 of prediction): %.1f percent' %(mlr_model_score*100)) # shows the strength of linear relationship between 2 variables: observations and predictions\n",
    "    \n",
    "    \n",
    "    # report accuracy/ model performance/ error metrics on test data\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    import math\n",
    "\n",
    "    # print('Test MSE: %.2f' %mean_squared_error(y_test, y_predict_mlr)) # absolute number on how much predicted results deviate from the actual number\n",
    "    print('Test RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_pred_mlr))) # square root of MSE.\n",
    "    # print('Test MAE: %.2f' %mean_absolute_error(y_test, y_predict_mlr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVpY82JLUTzV",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "prepare train and test datasets in a 2D array format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bv6jOeUIP2Nc",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# an_train = X_train[:,0]\n",
    "\n",
    "# an_test = X_test[:,0]\n",
    "# ca_test = X_test[:,1]\n",
    "# cf_test = X_test[:,2]\n",
    "\n",
    "\n",
    "# print(an_train.shape)\n",
    "# print(an_test.shape)\n",
    "\n",
    "# #~ reshape to 2d\n",
    "# an_train = np.reshape(an_train, (an_train.size, 1))\n",
    "# an_test = np.reshape(an_test, (an_test.size, 1))\n",
    "# print('after reshapping')\n",
    "# print(an_train.shape)\n",
    "# print(an_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-db0Q04UvNx",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Train the model by fitting data to it. Algorithm is fit on the trining dataset to get our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sNeMxd0-UJr5",
    "outputId": "bcbbe997-6e7e-44ce-cc29-04f07b7fb939",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# fit the linear model\n",
    "# linear_regr.fit(AN_train, y_train) # fit(X,y) will fit data to our algortihm and makes it learn from data == training step\n",
    "# here the model learns/finds the mapping f(.) between input and output \n",
    "# Q- X == input array, how many features can LinearRegression() use? can we use X(an, ca, cf)?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Q- how about evaluation step?\n",
    "# Q- how about test step?\n",
    "# Q- accuracy? score? how we know that our model is up to the desired accuracy/score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5HQ3WJJOabq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# #-- here we use X_test w/3 features\n",
    "# \"\"\"Return the coefficient of determination (R2)  of the prediction; relative measure of how well the model fits dependent variables\n",
    "# \"\"\" \n",
    "# mlr_model_score = mlr_model.score(X_test, y_test) # R2  of self.predict(X) wrt. y.\n",
    "# print('Accuracy (R2 of prediction): %.1f percent' %(mlr_model_score*100)) # shows the strength of linear relationship between 2 variables: observations and predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPM5riibV56z",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## intercept and coefficient???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFEjfO9TTy07",
    "outputId": "1b788f8e-0687-41de-9145-fe7a7bccf702",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# print('Intercept: %.2f' %mlr_model.intercept_)\n",
    "# print('model coefficients: %s' %mlr_model.coef_)     # ???? Estimated coefficients for the linear regression problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as8mUUHq2qxX",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "here we use test portion of data as the future/unseen data by making predictions on the test dataset\n",
    "\n",
    "we use model fit and make predictions on test data set and evaluate the model skill by measuring how well the model performs on prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChW8Y5UN6koc",
    "outputId": "357c42d2-80bc-4229-ec75-9cdccdce31f3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# #~ Predict using the linear model.\n",
    "# # predict_an_test = linear_regr.predict(an_test)\n",
    "\n",
    "# y_pred_mlr = mlr_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JREu9RxaoaX",
    "outputId": "5aaf915e-acd7-4cd4-bcbc-f80a223a160d",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# print(X_test[:,0])\n",
    "# print('X_test shape: (%s,%s)' %X_test.shape)\n",
    "# print('y_predicted shape: (%s,%s)' %y_predict_mlr.shape)\n",
    "\n",
    "# print(type(X_test))\n",
    "# y_predict_mlr[:10,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLQMt_QXCgKk",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Q- how evaluate performance metrics/skill/score of the trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K15YUX0s1s5j",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## plot predictions for each attribute\n",
    "Plot outputs for prediction on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "ref1:\n",
    "https://www.datacourses.com/evaluation-of-regression-models-in-scikit-learn-846/\n",
    "\n",
    "ref2:\n",
    "https://www.geeksforgeeks.org/regression-classification-supervised-machine-learning/#:~:text=A%20regression%20problem%20is%20when,which%20goes%20through%20the%20points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting scatter(y-test, y-pred). It is not the same as plotting a best fit line, but it shows you how well the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "HFJY52Lo4QMy",
    "outputId": "3620343b-eb43-4f55-9e5c-ffc94093fc30",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# # y_test_sorted = np.sort(y_test)\n",
    "# # y_predict_mlr_sorted = np.sort(y_predict_mlr)\n",
    "\n",
    "# ax.scatter(y_test, y_predict_mlr, color='g') #, edgecolors=(0, 0, 1))\n",
    "# ax.plot([y_test.min(), y_test.max()], [y_predict_mlr.min(), y_predict_mlr.max()], 'k--', lw=3) # order= ax.plot(x, y)\n",
    "\n",
    "# # ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3) # changed to test range for better plot\n",
    "# ## tested with sorted values\n",
    "# # ax.plot(y_test_sorted, y_predict_mlr_sorted, color='red', linewidth=3) # for score inputs & error metrics Q- sort values before line?\n",
    "# # ax.plot(y_test, y_predict_mlr, color='red', linewidth=3) # for score inputs & error metrics Q- sort values before line?\n",
    "# ax.set_xlabel('Measured roughness (y-test, obs)')\n",
    "# ax.set_ylabel('Predicted roughness (y-predict)')\n",
    "# plt.show()\n",
    "\n",
    "################################################\n",
    "\n",
    "if (multiple_linear_regression == True):\n",
    "    \n",
    "    plt.scatter(y_test, y_pred_mlr, color='black')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_pred_mlr.min(), y_pred_mlr.max()], 'red', lw=3) # order= ax.plot(x, y)\n",
    "    plt.xlabel(\"y-actual\")\n",
    "    plt.ylabel(\"y-predict\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(X_test[:,0], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gft7YXaavKx9",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "plot each cam seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q60jr91yvK_s",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1,2)\n",
    "\n",
    "# axs[0].scatter(X_test[0], y_test, color='g') #, edgecolors=(0, 0, 1))\n",
    "# # axs[0].plot([y_test.min(), y_test.max()], [y_predict_mlr.min(), y_predict_mlr.max()], 'k--', lw=3) # order= ax.plot(x, y)\n",
    "# # ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3) # changed to test range for better plot\n",
    "\n",
    "# axs[0].set_xlabel('Actual (y-test, obs) roughness')\n",
    "# axs[0].set_ylabel('Predicted roughness')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ylAJ2i8gy9_",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#~ old \n",
    "# plt.scatter(an_test, y_test,  color='black') \n",
    "# plt.title('Test Data') \n",
    "# plt.xlabel('an') \n",
    "# plt.ylabel('rms') \n",
    "# plt.xticks(()) \n",
    "# plt.yticks(()) \n",
    "#~ plot prediction line - Q- can we plot the prediction?\n",
    "# plt.plot(an_test, predict_X_test, color='red', linewidth=3) # plots regression line?\n",
    "# plt.plot(an_test, predict_an_test, color='red', linewidth=3) # plots regression line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxDlED3HVxjV",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Report error metrics on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXLrgEXwvOzQ",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Q- performance metrics/stats on test data? like prediction accuracy? regression error?\n",
    "\n",
    "source: https://towardsdatascience.com/what-are-the-best-metrics-to-evaluate-your-regression-model-418ca481755b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9hJPnHLwWgHI",
    "outputId": "2351d933-c593-4739-8530-4e9471637561",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# import math\n",
    "\n",
    "# # print('Test MSE: %.2f' %mean_squared_error(y_test, y_predict_mlr)) # absolute number on how much predicted results deviate from the actual number\n",
    "# print('Test RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_pred_mlr))) # square root of MSE.\n",
    "# # print('Test MAE: %.2f' %mean_absolute_error(y_test, y_predict_mlr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOH1IZX8gjOp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2- Support Vector Regression (svm.svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4lDxIeCWRPO",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# # import numpy as np\n",
    "# from sklearn.svm import SVR\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FX8R5bN_gwpw",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if (SVM == True):\n",
    "    \n",
    "    # import numpy as np\n",
    "    from sklearn.svm import SVR\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # build/initiate regression model\n",
    "    svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1, degree=3) # radial basis function kernel\n",
    "    svr_lin = SVR(kernel='linear', C=100, gamma='auto')\n",
    "    svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1)\n",
    "    \n",
    "    \n",
    "    from numpy import ravel\n",
    "    y_train_list = ravel(y_train)\n",
    "\n",
    "#     print(type(y_train_list))\n",
    "#     print(y_train_list[1:10])\n",
    "\n",
    "\n",
    "    svr_rbf.fit(X_train, y_train_list)\n",
    "    svr_lin.fit(X_train, y_train_list)\n",
    "    svr_poly.fit(X_train, y_train_list)\n",
    "\n",
    "    ##- reference: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "\n",
    "    #~ here we use X_test w/3 features\n",
    "    ## Return the coefficient of determination (R2)  of the prediction; \n",
    "    ## relative measure of how well the model fits dependent variables\n",
    "    ## regression model score\n",
    "\n",
    "    y_rbf_pred = svr_rbf.predict(X_test)\n",
    "    y_lin_pred = svr_lin.predict(X_test)\n",
    "    y_poly_pred = svr_poly.predict(X_test)\n",
    "\n",
    "    rbf_score = r2_score(y_test, y_rbf_pred)*100\n",
    "    linear_score = r2_score(y_test, y_lin_pred)*100\n",
    "    poly_score = r2_score(y_test, y_poly_pred)*100\n",
    "\n",
    "    print(\"R2 rbf= %.2f percent\" %rbf_score)\n",
    "    print(\"R2 linear= %.2f percent\" %linear_score)\n",
    "    print(\"R2 poly= %.2f percent\" %poly_score)\n",
    "\n",
    "\n",
    "    # print(svr_rbf.score(X_test, y_test)*100) \n",
    "    # # print(svr_lin.score(X_test, y_test)*100)\n",
    "    # # print(svr_poly.score(X_test, y_test)*100)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    import math\n",
    "    print(\"\\n\")\n",
    "    print('rbf kernel')\n",
    "    print('RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_rbf_pred))) # square root of MSE.\n",
    "\n",
    "    print('linear kernel')\n",
    "    print('RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_lin_pred))) # square root of MSE.\n",
    "\n",
    "    print('poly kernel')\n",
    "    print('RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_poly_pred))) # square root of MSE.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFc2NAmpjgmk",
    "outputId": "1992dc33-7e88-42db-cd2c-8dabe9c0ffc6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# import math\n",
    "# print('rbf kernel ...')\n",
    "# # print('MSE: %.2f' %mean_squared_error(y_test, y_rbf_predicted)) # absolute number on how much predicted results deviate from the actual number\n",
    "# print('RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_rbf_predicted))) # square root of MSE.\n",
    "# # print('Test MAE: %.2f' %mean_absolute_error(y_test, y_rbf_predicted))\n",
    "\n",
    "# # print('linear kernel ...')\n",
    "# # # print('MSE: %.2f' %mean_squared_error(y_test, y_lin_predicted)) # absolute number on how much predicted results deviate from the actual number\n",
    "# # print('RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_lin_predicted))) # square root of MSE.\n",
    "# # # print('Test MAE: %.2f' %mean_absolute_error(y_test, y_lin_predicted))\n",
    "\n",
    "# # print('poly kernel ...')\n",
    "# # # print('MSE: %.2f' %mean_squared_error(y_test, y_poly_predicted)) # absolute number on how much predicted results deviate from the actual number\n",
    "# # print('RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_poly_predicted))) # square root of MSE.\n",
    "# # # print('Test MAE: %.2f' %mean_absolute_error(y_test, y_poly_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaTNIOfuyG9B",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3- Polynomial linear regression\n",
    "\n",
    "- Although this model allows for a nonlinear relationship between Y and X, polynomial regression is still considered linear regression since it is linear in the regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGBZv3kBjrPM",
    "outputId": "77cbb5fd-3594-49b8-f5bc-086c540c2ffc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "if (polynomial == True):\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    import math\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=3) # what degree is suitable for my problem?\n",
    "    X_train_transformed = polynomial_features.fit_transform(X_train)\n",
    "\n",
    "    # plRegressor = LinearRegression()\n",
    "    # print(polynomial_features)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_train_transformed.shape) # Q- why degree= 3, 3 features turned to 20 features?\n",
    "\n",
    "    # from sklearn.pipeline import Pipeline\n",
    "    # plr_model = Pipeline(steps= [('polyFeatures', polynomial_features), ('regressor', plRegressor)])\n",
    "    model_name = 'Polinomial Linear Regression '\n",
    "    print('model: %s' %model_name)\n",
    "    poly_linear_reg_model = LinearRegression(polynomial_features)\n",
    "    print(poly_linear_reg_model)\n",
    "\n",
    "    poly_linear_reg_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_plr = poly_linear_reg_model.predict(X_test)\n",
    "\n",
    "    score_plr = r2_score(y_test, y_pred_plr)\n",
    "    print(\"accuracy: %.2f percent\" %(score_plr*100))\n",
    "\n",
    "    print('Test RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_pred_plr))) # square root of MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rXYeNSbeUHB",
    "outputId": "bc8c641d-d24b-4ade-de98-daf0f09d2587",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print('Test MSE: %.2f' %mean_squared_error(y_test, y_predict_plr)) # absolute number on how much predicted results deviate from the actual number\n",
    "# print('Test RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_pred_plr))) # square root of MSE.\n",
    "# print('Test MAE: %.2f' %mean_absolute_error(y_test, y_predict_plr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_2gUoIFcBsX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4- Decision Tree (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "source: https://gdcoder.com/decision-tree-regressor-explained-in-depth/\n",
    "\n",
    "sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html?highlight=decision%20tree#sklearn.tree.DecisionTreeRegressor\n",
    "\n",
    "tree: https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQdTgKeRcB7Q",
    "outputId": "2be5882c-250b-4840-9220-50d0d54e374d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if (decision_tree == True):\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    import math\n",
    "    from sklearn import tree\n",
    "\n",
    "    # Q- how tune DT alg? which param is most effective?\n",
    "    DT_model = tree.DecisionTreeRegressor(max_features='auto', \n",
    "                                          min_samples_split=10, \n",
    "                                          min_samples_leaf=20, \n",
    "                                          max_depth=4, \n",
    "                                          criterion='mse', \n",
    "                                          random_state=0) # criterion= for feature selection= MAE, MSE, ... differs from classification\n",
    "\n",
    "    # dtree_model = Pipeline(steps=[('preprocessorAll', preprocessorForAllColumns), ('regressor', DTRegressor)])  # we preprocessed data ourselves! \n",
    "    # DT_model = Pipeline(steps=[('regressor', DT_regressor)])\n",
    "\n",
    "    print(DT_model)\n",
    "    # print(X_train.head)\n",
    "\n",
    "    DT_model.fit(X_train, y_train) # train the model\n",
    "\n",
    "    y_pred_dtree = DT_model.predict(X_test)\n",
    "\n",
    "    # score_dtree = DT_model.score(X_test, y_test)  # Return the coefficient of determination (R2) of the prediction\n",
    "    score_dtree = r2_score(y_test, y_pred_dtree)\n",
    "\n",
    "    print('\\nDecisionTree accuracy: %.2f percent' %(score_dtree*100))\n",
    "\n",
    "    print('Test RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_pred_dtree))) # square root of MSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "SBo-1z4MdXtT",
    "outputId": "e3aeb3e9-4a00-48ee-d45d-5eb61a4ff065",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# import graphviz \n",
    "\n",
    "# # dot_data = tree.export_graphviz(DT_model, out_file=None, filled=True, rounded=True, special_characters=False, impurity=False, feature_names=X_train.columns) \n",
    "# dot_data = tree.export_graphviz(DT_model, out_file=None, filled=True, rounded=True, feature_names=X_train.columns) \n",
    "# graph = graphviz.Source(dot_data) \n",
    "# graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwrV2zOBZiYS",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "plot using subplots()\n",
    "\n",
    "source: https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html\n",
    "\n",
    "source: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Use prediction on test data and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "EdwyO48jge5z",
    "outputId": "67cb74ba-89eb-45be-d98a-4b3b2d45a401",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.scatter(y_test, y_predict_dtree, edgecolors=(0, 0, 1))\n",
    "# # ax.plot([y_test.min(), y_test.max()], [y_predict_dtree.min(), y_predict_dtree.max()], 'r--', lw=3)\n",
    "# ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3) # changed to test range for better plot\n",
    "\n",
    "# ax.set_xlabel('Actual (y-test, obs) roughness')\n",
    "# ax.set_ylabel('Predicted roughness')\n",
    "\n",
    "# plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Report error metrics on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nnt-4BYsiY6l",
    "outputId": "95c36577-11ce-4458-bc2f-c20e3aa18137",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print('Test MSE: %.2f' %mean_squared_error(y_test, y_predict_dtree)) # absolute number on how much predicted results deviate from the actual number\n",
    "# print('Test RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_predict_dtree))) # square root of MSE.\n",
    "# print('Test MAE: %.2f' %mean_absolute_error(y_test, y_predict_dtree))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXsbcjT4bYgd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmVqF-5abYuc",
    "outputId": "d4fba58b-dcce-4012-bf2a-b59e10adb49e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "if (random_forest == True):\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    import math\n",
    "    from sklearn import tree\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    # from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    from numpy import ravel\n",
    "    y_train_list = ravel(y_train)\n",
    "\n",
    "    rfr_model = RandomForestRegressor(n_estimators=50, max_depth=4, random_state=0, max_features='auto', min_samples_split=10, min_samples_leaf=20, criterion='mse')\n",
    "\n",
    "    # rfr_model = Pipeline(steps=[('preprocessorAll', preprocessorForAllColumns), ('regressor', RandomForestRegressor)])  # we preprocessed data ourselves! \n",
    "\n",
    "    rfr_model.fit(X_train, y_train_list) # train the model\n",
    "\n",
    "    print(rfr_model)\n",
    "\n",
    "    y_pred_rfr = rfr_model.predict(X_test)\n",
    "\n",
    "    # score_rfr = rfr_model.score(X_test, y_test)  # Return the coefficient of determination (R2) of the prediction\n",
    "    score_randomForest = r2_score(y_test, y_pred_rfr)\n",
    "    print('\\n RandomForest accuracy: %.2f percent' %(score_randomForest*100))\n",
    "\n",
    "    print('Test RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_pred_rfr))) # square root of MSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Report error metrics on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2cSDDgNcxfu",
    "outputId": "a698317e-e8a7-43de-ff82-e9a718c99f9b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print('Test MSE: %.2f' %mean_squared_error(y_test, y_predict_rfr)) # absolute number on how much predicted results deviate from the actual number\n",
    "# print('Test RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_predict_rfr))) # square root of MSE.\n",
    "# print('Test MAE: %.2f' %mean_absolute_error(y_test, y_predict_rfr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AypsPg2rd8EH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6- Neural Network (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6mhjQ5QZAjo",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalizing data as an important step\n",
    "\n",
    "- We rescale data after we split data to train-test\n",
    "- all features have the same scale to reduce bias in data \n",
    "- perform this step before splitting data into train-test split\n",
    "- We normalize data using the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7Vcpt_35cXH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Network Configuration\n",
    "we only use 1 hidden layer because a single hidden layer is suitable for most data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "source: https://keras.io/api/models/model_training_apis/#evaluate-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iS5VpPumd8Ru",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#-- implementation-1\n",
    "\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from tensorflow.python.keras.layers import Dense\n",
    "# model = Sequential()\n",
    "# model.add(Dense(3, input_dim=3, kernel_initializer='normal', activation='relu'))  #, name='input layer'))\n",
    "# model.add(Dense(878, activation='relu')) #, name='hidden layer-1'))\n",
    "# model.add(Dense(1, activation='linear')) #, name='output layer'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZt6aEtsbsXo",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFvuENT8W1Rd",
    "outputId": "455b2b79-fe65-40c8-f8bd-02df98cfd19e",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "##- implementation-2\n",
    "##- from tensorflow import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "if (train_MLP==True):\n",
    "    \n",
    "    # Define Sequential model with 3 layers\n",
    "    mlp_model = tf.keras.Sequential(\n",
    "           [\n",
    "            # hidden layers\n",
    "            layers.Dense(9, input_dim=9, activation=\"relu\", kernel_initializer='normal', name=\"hidden-layer-1\"), # input_dim=3 == input shape will build our model automatically\n",
    "            layers.Dense(9, activation=\"relu\", kernel_initializer='normal', name=\"hidden-layer-2\"),  # name should be attached; one single word!\n",
    "            layers.Dense(9, activation=\"relu\", kernel_initializer='normal', name=\"hidden-layer-3\"),  # name should be attached; one single word!\n",
    "            #layers.Dense(9, activation=\"relu\", kernel_initializer='normal', name=\"hidden-layer-4\"),  # name should be attached; one single word!\n",
    "\n",
    "            # output layer\n",
    "            layers.Dense(1, activation='linear', kernel_initializer='normal', name=\"output-layer-SIR\"),  # linear activation?\n",
    "          ]\n",
    "        )\n",
    "\n",
    "    mlp_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (train_MLP==True):\n",
    "    \n",
    "    from tensorflow.keras.utils import plot_model\n",
    "\n",
    "    plot_model(mlp_model, to_file='model.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKpq6Jlob0K2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9evHRT9axqu",
    "outputId": "615b78d0-9a4c-4843-fee1-4e9db7ee466b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if (train_MLP==True):\n",
    "    \n",
    "    ##- compile model\n",
    "    mlp_model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "\n",
    "    ##- train the network\n",
    "    train_history = mlp_model.fit(X_train, \n",
    "                                  y_train, \n",
    "                                  epochs=50,\n",
    "                                  batch_size=100, \n",
    "                                  verbose=1, \n",
    "                                  validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (train_MLP==True):\n",
    "    # save model and architecture to single file\n",
    "    model_name = \"trained_model_3L9N_50epoch.h5\"\n",
    "    \n",
    "    model_fp = os.path.join(dataset_dir, model_name) \n",
    "    mlp_model.save(model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0tj0OojnpVk",
    "outputId": "7498fec3-b536-4c61-8d0c-f8919453d817",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if (train_MLP==True):\n",
    "\n",
    "    print(train_history.history.keys())\n",
    "    print('\\n')\n",
    "    # print(\"-> model: %s\" % train_model)\n",
    "    print(\"Training Loss: %.2f\" % train_history.history['loss'][-1])  # will return the loss in the last training epoch\n",
    "    print(\"Training MSE: %.2f (cm^2 roughness)\" % (train_history.history['mse'][-1]))\n",
    "    print(\"Training MAE: %.2f (cm roughness)\" % (train_history.history['mae'][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27FfyU3BrMIn",
    "outputId": "237cb285-99eb-4deb-c2f2-5507c979c5c1",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# print(train_history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Plot loss of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "BWPmMrcmm1sc",
    "outputId": "08850351-3e48-4478-c98c-105b14c2a8ea",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if (train_MLP==True):\n",
    "\n",
    "    print(train_history.history.keys())\n",
    "    # \"Loss\" val==validation==test\n",
    "    plt.plot(train_history.history['loss'])\n",
    "    plt.plot(train_history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThVzq7tKar3o",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing the NN model and report error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mlp_model.metrics_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POLRhD4foM7Q",
    "outputId": "4fb70cd9-3e90-481c-b5b1-b04180b859fc",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if (train_MLP==True):\n",
    "    \n",
    "    ## Returns the loss value & metrics values for the model in test mode.\n",
    "    ## similar to test \n",
    "\n",
    "    test_metrics = mlp_model.evaluate(X_test, y_test, batch_size=20, verbose=1) # Returns the loss value & metrics values for the model in test mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (train_MLP==True):\n",
    "    \n",
    "    # print(test_metrics)\n",
    "    import math\n",
    "\n",
    "    mse_test = test_metrics[1]\n",
    "    print(\"Test RMSE: %.2f (cm roughness)\" %math.sqrt(mse_test))\n",
    "#     print(\"Test MAE: %.2f (cm roughness)\" %(test_metrics[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idlhdhgFbcQh",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# model prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3buusTIa970",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if (train_MLP==True):\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "    y_pred_mlp = mlp_model.predict(X_test)  # Generates output predictions for the input samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale back the predictions\n",
    "\n",
    "# if (train_MLP==True):\n",
    "    \n",
    "#     y_pred_ann_original = scaler_y.inverse_transform(y_pred_ann_scaled)\n",
    "\n",
    "#     print('Test RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_pred_ann_original))) # square root of MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (train_MLP==True):\n",
    "    \n",
    "    if (multiple_linear_regression == True):\n",
    "\n",
    "        plt.scatter(y_test, y_pred_mlp, color='black')\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_pred_mlp.min(), y_pred_mlp.max()], 'red', lw=3) # order= ax.plot(x, y)\n",
    "        plt.xlabel(\"y-actual\")\n",
    "        plt.ylabel(\"y-predict\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model to predict SIRoughness values for a path of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_name = \"trained_model_2L50N_100epoch.h5\"\n",
    "model_dir = '/home/ehsan/misr_lab/MISR-roughness/prediction_MLP'\n",
    "\n",
    "model_fp = os.path.join(model_dir, model_name) \n",
    "\n",
    "# load model\n",
    "mlp_model_best = load_model(model_fp)\n",
    "# summarize model.\n",
    "mlp_model_best.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# best_model.predict(misr_block_in_for_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = mlp_model_best.evaluate(X_test, y_test, batch_size=20, verbose=1) # Returns the loss value & metrics values for the model in test mode\n",
    "\n",
    "import math\n",
    "\n",
    "mse_test = test_metrics[1]\n",
    "print(\"Test RMSE: %.2f (cm roughness)\" %math.sqrt(mse_test))\n",
    "#     print(\"Test MAE: %.2f (cm roughness)\" %(test_metrics[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get a list of toa_refl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "toa_refl_dir = \"/media/ehsan/6T_part1/14528_apr2016/toa_refl_april_2016_9cam4bands_day1_30_p1_233_b1_46/toa_files_in_range_2016_4_25\"\n",
    "toa_refl_filePattern = \"toa_refl_P*.dat\"\n",
    "\n",
    "toa_refl_list = glob(os.path.join(toa_refl_dir, toa_refl_filePattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(toa_refl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_black_regions(in_arr, cam_name):\n",
    "    print(\"trimming camera: %s\" %cam_name)\n",
    "    \n",
    "    for i in range(2048):\n",
    "        if (in_arr[0,i] != -1.0):\n",
    "            colL1 = i\n",
    "#             print(\"colL1: %s\" %colL1)\n",
    "            break\n",
    "            \n",
    "    for i in range(2048):\n",
    "        if (in_arr[511,i] != -1.0):\n",
    "            colL2 = i\n",
    "#             print(\"colL2: %s\" %colL2)\n",
    "            break\n",
    "        \n",
    "    cut_left = max(colL1, colL2)\n",
    "    \n",
    "#     print(\"cut-left: %s\" %cut_left)\n",
    "    if (cut_left != 0.0):\n",
    "        in_arr = np.delete(in_arr, slice(0, cut_left), 1)\n",
    "#         print(in_arr.shape)\n",
    "    \n",
    "#     print(in_arr.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "    for j in range(in_arr.shape[1]):\n",
    "        if(in_arr[0,j] == -1.0):\n",
    "            colR1 = j\n",
    "#             print(\"colR1: %s\" %colR1)\n",
    "            break\n",
    "        else:\n",
    "            colR1 = in_arr.shape[1]\n",
    "            \n",
    "    for j in range(in_arr.shape[1]):\n",
    "        if(in_arr[511,j] == -1.0):\n",
    "            colR2 = j\n",
    "#             print(\"colR2: %s\" %colR2)\n",
    "            break\n",
    "        else:\n",
    "            colR2 = in_arr.shape[1]\n",
    "        \n",
    "    cut_right = min(colR1, colR2)\n",
    "\n",
    "#     print(\"cut-right: %s\" %cut_right)\n",
    "\n",
    "    if (cut_right != 0.0):\n",
    "        in_arr = np.delete(in_arr, slice(cut_right, in_arr.shape[1]), 1)\n",
    "\n",
    "#     print(\"shape after trimming:\")\n",
    "#     print(in_arr.shape)\n",
    "    arr_shape = in_arr.shape\n",
    "\n",
    "    return in_arr, arr_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da_r_arr = np.fromfile(os.path.join(toa_refl_dir, da_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "# np.median(da_r_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what happens if pixel is negative==land? we shoudl add it straight to final prediction file- it should not go into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mlp_ann_predict==True):\n",
    "    ## order of neurons is important\n",
    "    # open toa_refl.dat\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from PIL import Image\n",
    "    from datetime import datetime\n",
    "\n",
    "    t1 = datetime.now()\n",
    "\n",
    "    # image_dir = \"/Users/ehsanmos/Documents/RnD/MISR_lab/ML_research\"\n",
    "    image_dir = toa_refl_dir\n",
    "\n",
    "    p_o_process_list = []\n",
    "\n",
    "    for toa_refl in toa_refl_list:\n",
    "        #print(toa_refl)\n",
    "        file_name = toa_refl.split('/')[-1]\n",
    "        #print(file_name.split('_')[2:4])\n",
    "        p_o_list = file_name.split('_')[2:4]\n",
    "        p_o = p_o_list[0]+\"_\"+p_o_list[1]\n",
    "        #print(p_o)\n",
    "\n",
    "        if p_o in p_o_process_list:\n",
    "            continue\n",
    "        else:\n",
    "            p_o_process_list.append(p_o)\n",
    "            print(\"added to list\")\n",
    "\n",
    "            for toa_block in range(1,47,1):  # check this number later\n",
    "\n",
    "                if (toa_block < 10):\n",
    "                    toa_block = str(toa_block).rjust(2,'0')\n",
    "                else:\n",
    "                    toa_block = str(toa_block)\n",
    "\n",
    "                print(p_o)\n",
    "                print(toa_block)\n",
    "                \n",
    "                ## check output on disk\n",
    "#                 out_img_label = p_o+'_'+'B0'+toa_block+\".tif\"  # this image format supports saving neg- values in image\n",
    "#                 out_img_fullpath = os.path.join(image_dir, out_img_label)\n",
    "\n",
    "\n",
    "\n",
    "                out_raw_binary_label = 'roughness_toa_refl_'+p_o+'_'+'B0'+toa_block+\".dat\"  # this image format supports saving neg- values in image\n",
    "                out_raw_binary_fullpath = os.path.join(image_dir, out_raw_binary_label)\n",
    "\n",
    "\n",
    "                if (os.path.isfile(out_raw_binary_fullpath)==True):\n",
    "                    print(out_raw_binary_fullpath)\n",
    "                    print(\"binary exists- continue\")\n",
    "                    continue\n",
    "\n",
    "                ## define and open 9 cameras in order\n",
    "                ## based on order of input to MLP- based on order of cameras in training dataset\n",
    "                ## path,orbit,block,line,sample,lat,lon,Da_r,Ca_r,Ba_r,Aa_r,An_r,An_g,An_b,An_nir,Af_r,Bf_r,Cf_r,Df_r,mean_ATM_roughness\n",
    "\n",
    "                \n",
    "                da_r = \"toa_refl_\"+p_o+\"_B0\"+toa_block+\"_da_red.dat\"\n",
    "                ## check file available\n",
    "                if (os.path.isfile(os.path.join(toa_refl_dir, da_r))==False):\n",
    "                    print(\"block not found- continue\")\n",
    "                    continue\n",
    "                ## read array\n",
    "                da_r_arr = np.fromfile(os.path.join(toa_refl_dir, da_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "                ## check black- continue:\n",
    "                if (np.median(da_r_arr) == -1.0):\n",
    "                    print(\"image black- continue\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                \n",
    "                ca_r = \"toa_refl_\"+p_o+\"_B0\"+toa_block+\"_ca_red.dat\"\n",
    "                ## check file available\n",
    "                if (os.path.isfile(os.path.join(toa_refl_dir, ca_r))==False):\n",
    "                    print(\"block not found- continue\")\n",
    "                    continue\n",
    "                ca_r_arr = np.fromfile(os.path.join(toa_refl_dir, ca_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "                ## check black- continue:\n",
    "                if (np.median(ca_r_arr) == -1.0):\n",
    "                    print(\"image black- continue\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                \n",
    "                ba_r = \"toa_refl_\"+p_o+\"_B0\"+toa_block+\"_ba_red.dat\"\n",
    "                ## check file available\n",
    "                if (os.path.isfile(os.path.join(toa_refl_dir, ba_r))==False):\n",
    "                    print(\"block not found- continue\")\n",
    "                    continue\n",
    "                ba_r_arr = np.fromfile(os.path.join(toa_refl_dir, ba_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "                ## check black- continue:\n",
    "                if (np.median(ba_r_arr) == -1.0):\n",
    "                    print(\"image black- continue\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                \n",
    "                aa_r = \"toa_refl_\"+p_o+\"_B0\"+toa_block+\"_aa_red.dat\"\n",
    "                ## check file available\n",
    "                if (os.path.isfile(os.path.join(toa_refl_dir, aa_r))==False):\n",
    "                    print(\"block not found- continue\")\n",
    "                    continue\n",
    "                aa_r_arr = np.fromfile(os.path.join(toa_refl_dir, aa_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "                ## check black- continue:\n",
    "                if (np.median(aa_r_arr) == -1.0):\n",
    "                    print(\"image black- continue\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                \n",
    "                an_r = \"toa_refl_\"+p_o+\"_B0\"+toa_block+\"_an_red.dat\"\n",
    "                ## check file available\n",
    "                if (os.path.isfile(os.path.join(toa_refl_dir, an_r))==False):\n",
    "                    print(\"block not found- continue\")\n",
    "                    continue\n",
    "                an_r_arr = np.fromfile(os.path.join(toa_refl_dir, an_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "                ## check black- continue:\n",
    "                if (np.median(an_r_arr) == -1.0):\n",
    "                    print(\"image black- continue\")\n",
    "                    continue\n",
    "                \n",
    "   \n",
    "\n",
    "                af_r = \"toa_refl_\"+p_o+\"_B0\"+toa_block+\"_af_red.dat\"\n",
    "                ## check file available\n",
    "                if (os.path.isfile(os.path.join(toa_refl_dir, af_r))==False):\n",
    "                    print(\"block not found- continue\")\n",
    "                    continue\n",
    "                af_r_arr = np.fromfile(os.path.join(toa_refl_dir, af_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "                ## check black- continue:\n",
    "                if (np.median(af_r_arr) == -1.0):\n",
    "                    print(\"image black- continue\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                bf_r = \"toa_refl_\"+p_o+\"_B0\"+toa_block+\"_bf_red.dat\"\n",
    "                ## check file available\n",
    "                if (os.path.isfile(os.path.join(toa_refl_dir, bf_r))==False):\n",
    "                    print(\"block not found- continue\")\n",
    "                    continue\n",
    "                bf_r_arr = np.fromfile(os.path.join(toa_refl_dir, bf_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "                ## check black- continue:\n",
    "                if (np.median(bf_r_arr) == -1.0):\n",
    "                    print(\"image black- continue\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "                cf_r = \"toa_refl_\"+p_o+\"_B0\"+toa_block+\"_cf_red.dat\"\n",
    "                ## check file available\n",
    "                if (os.path.isfile(os.path.join(toa_refl_dir, cf_r))==False):\n",
    "                    print(\"block not found- continue\")\n",
    "                    continue\n",
    "                cf_r_arr = np.fromfile(os.path.join(toa_refl_dir, cf_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "                ## check black- continue:\n",
    "                if (np.median(cf_r_arr) == -1.0):\n",
    "                    print(\"image black- continue\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                df_r = \"toa_refl_\"+p_o+\"_B0\"+toa_block+\"_df_red.dat\"\n",
    "                ## check file available\n",
    "                if (os.path.isfile(os.path.join(toa_refl_dir, df_r))==False):\n",
    "                    print(\"block not found- continue\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                \n",
    "                # open and read toa-refl.dat file here\n",
    "                df_r_arr = np.fromfile(os.path.join(toa_refl_dir, df_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "                ## check black- continue:\n",
    "                if (np.median(df_r_arr) == -1.0):\n",
    "                    print(\"image black- continue\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                print(\"removing black regions\")\n",
    "\n",
    "                ## trim 9 images and get final shape\n",
    "                da_noBlack, da_shape = remove_black_regions(da_r_arr, 'da')\n",
    "                ca_noBlack, ca_shape = remove_black_regions(ca_r_arr, 'ca')\n",
    "                ba_noBlack, ba_shape = remove_black_regions(ba_r_arr, 'ba')\n",
    "                aa_noBlack, aa_shape = remove_black_regions(aa_r_arr, 'aa')\n",
    "                an_noBlack, an_shape = remove_black_regions(an_r_arr, 'an')\n",
    "                af_noBlack, af_shape = remove_black_regions(af_r_arr, 'af')\n",
    "                bf_noBlack, bf_shape = remove_black_regions(bf_r_arr, 'bf')\n",
    "                cf_noBlack, cf_shape = remove_black_regions(cf_r_arr, 'cf')\n",
    "                df_noBlack, df_shape = remove_black_regions(df_r_arr, 'df')\n",
    "\n",
    "                shape_list = [da_shape[1], ca_shape[1], ba_shape[1], \n",
    "                              aa_shape[1], an_shape[1], af_shape[1], \n",
    "                              bf_shape[1], cf_shape[1], df_shape[1]]\n",
    "\n",
    "                min_col = min(shape_list)\n",
    "                print(min_col)\n",
    "\n",
    "                ## trip all 9 images based on min-column\n",
    "                da_noBlack_trim = np.delete(da_noBlack, slice(min_col, da_noBlack.shape[1]), 1)\n",
    "                print(da_noBlack_trim.shape)\n",
    "\n",
    "                ca_noBlack_trim = np.delete(ca_noBlack, slice(min_col, ca_noBlack.shape[1]), 1)\n",
    "                print(ca_noBlack_trim.shape)\n",
    "\n",
    "                ba_noBlack_trim = np.delete(ba_noBlack, slice(min_col, ba_noBlack.shape[1]), 1)\n",
    "                print(ba_noBlack_trim.shape)\n",
    "\n",
    "                aa_noBlack_trim = np.delete(aa_noBlack, slice(min_col, aa_noBlack.shape[1]), 1)\n",
    "                print(aa_noBlack_trim.shape)\n",
    "\n",
    "                an_noBlack_trim = np.delete(an_noBlack, slice(min_col, an_noBlack.shape[1]), 1)\n",
    "                print(an_noBlack_trim.shape)\n",
    "\n",
    "                af_noBlack_trim = np.delete(af_noBlack, slice(min_col, af_noBlack.shape[1]), 1)\n",
    "                print(af_noBlack_trim.shape)\n",
    "\n",
    "                bf_noBlack_trim = np.delete(bf_noBlack, slice(min_col, bf_noBlack.shape[1]), 1)\n",
    "                print(bf_noBlack_trim.shape)\n",
    "\n",
    "                cf_noBlack_trim = np.delete(cf_noBlack, slice(min_col, cf_noBlack.shape[1]), 1)\n",
    "                print(cf_noBlack_trim.shape)\n",
    "\n",
    "                df_noBlack_trim = np.delete(df_noBlack, slice(min_col, df_noBlack.shape[1]), 1)\n",
    "                print(df_noBlack_trim.shape)\n",
    "\n",
    "\n",
    "\n",
    "                ## loop and extract 9 cameras for each row & column, based on order of cameras (important)\n",
    "\n",
    "                x_predict_block = []\n",
    "\n",
    "                for row in range(da_noBlack_trim.shape[0]):\n",
    "                    for col in range(da_noBlack_trim.shape[1]):\n",
    "\n",
    "                        x1 = da_noBlack_trim[row,col]\n",
    "                        x2 = ca_noBlack_trim[row,col]\n",
    "                        x3 = ba_noBlack_trim[row,col]\n",
    "                        x4 = aa_noBlack_trim[row,col]\n",
    "                        x5 = an_noBlack_trim[row,col]\n",
    "                        x6 = af_noBlack_trim[row,col]\n",
    "                        x7 = bf_noBlack_trim[row,col]\n",
    "                        x8 = cf_noBlack_trim[row,col]\n",
    "                        x9 = df_noBlack_trim[row,col]\n",
    "\n",
    "                        nine_features = [x1,x2,x3,x4,x5,x6,x7,x8,x9]\n",
    "                        #print(type(nine_features))\n",
    "                        x_predict_block.append(nine_features)\n",
    "\n",
    "\n",
    "                x_predict_block[0]\n",
    "                \n",
    "                ## prediction for each block\n",
    "                y_predict_block = mlp_model_best.predict(x_predict_block, verbose=1)\n",
    "\n",
    "\n",
    "                \n",
    "                ## re-construct images to 2D\n",
    "                print(y_predict_block.shape)\n",
    "                print(type(y_predict_block))\n",
    "\n",
    "                y_predict_block_2d = y_predict_block.reshape((512,-1))\n",
    "                print(y_predict_block_2d.dtype)\n",
    "                print(y_predict_block_2d.shape)\n",
    "\n",
    "                print(\"min: %s\" %y_predict_block_2d.min())\n",
    "                print(\"max: %s\" %y_predict_block_2d.max())\n",
    "\n",
    "                \n",
    "                ## flatten array and save as binary raw file\n",
    "                y_predict_block_2d.flatten().astype(np.double).tofile(out_raw_binary_fullpath)\n",
    "                print(out_raw_binary_fullpath)\n",
    "                \n",
    "                y_predict_block_2d = None\n",
    "#                 print(len(y_predict_block_2d))\n",
    "\n",
    "\n",
    "#                 ##- create PIL image from predicted roughness array 2D w/ negative values\n",
    "#                 predicted_roughness_img = Image.fromarray(y_predict_block_2d)\n",
    "#                 ##- save roughness PIL image on disc\n",
    "#                 predicted_roughness_img.save(out_img_fullpath)  \n",
    "#                 print(\"predicted image saved to:\")\n",
    "#                 print(out_img_fullpath)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    t2 = datetime.now()\n",
    "    period = t2-t1\n",
    "    print(\"run time: %s\" %period)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##- filter any negative value\n",
    "## not necessary anymore, cuz we trimmed all 9 images\n",
    "\n",
    "# def remove_minuse_one(arr_2d):\n",
    "    \n",
    "#     pixel_list = []\n",
    "#     for row in range(512):\n",
    "#         for col in range(2048):\n",
    "            \n",
    "#             if(arr_2d[row,col] < 0.0):\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 pixel_list.append(arr_2d[row,col])\n",
    "\n",
    "#     return pixel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ba_r_arr[0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##- filter any negative value\n",
    "# toa_file_list = [da_r_arr, ca_r_arr, ba_r_arr, aa_r_arr, an_r_arr, af_r_arr, bf_r_arr, cf_r_arr, df_r_arr]\n",
    "\n",
    "# pixel_values_total_list = []\n",
    "\n",
    "# for row in range(512):\n",
    "#     for col in range(2048):\n",
    "                \n",
    "#         pixels_9cams_list = []\n",
    "        \n",
    "#         if(da_r_arr[row,col] != -1.0):\n",
    "#             pixels_9cams_list.extend(da_r_arr[row,col])\n",
    "\n",
    "            \n",
    "#         if(ca_r_arr[row,col] != -1.0):\n",
    "#             pixels_9cams_list.extend(ca_r_arr[row,col])\n",
    "\n",
    "\n",
    "#         if(ba_r_arr[row,col] != -1.0):\n",
    "#             pixels_9cams_list.extend(ba_r_arr[row,col])\n",
    "            \n",
    "#         print(len(pixels_9cams_list))\n",
    "        \n",
    "#         if(len(pixels_9cams_list)==0):\n",
    "#             continue\n",
    "#         else:\n",
    "#             pixel_values_total_list.append(pixels_9cams_list)\n",
    "#             pixels_9cams_list.clear()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toa_file_list = [da_r_arr, ca_r_arr, ba_r_arr, aa_r_arr, an_r_arr, af_r_arr, bf_r_arr, cf_r_arr, df_r_arr]\n",
    "\n",
    "# for ifile in toa_file_list:\n",
    "# #     print(ifile.min())\n",
    "# #     print(ifile.max())\n",
    "#     print(ifile.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toa_file_list = [da_r_arr, ca_r_arr, ba_r_arr, aa_r_arr, an_r_arr, af_r_arr, bf_r_arr, cf_r_arr, df_r_arr]\n",
    "\n",
    "# da_r_list = remove_minuse_one(da_r_arr)\n",
    "# ca_r_list = remove_minuse_one(ca_r_arr)\n",
    "# ba_r_list = remove_minuse_one(ba_r_arr)\n",
    "# aa_r_list = remove_minuse_one(aa_r_arr)\n",
    "\n",
    "# print(len(da_r_list))\n",
    "# print(len(ca_r_list))\n",
    "# print(len(ba_r_list))\n",
    "# print(len(aa_r_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(aa_r_list).reshape((512,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one single reflectance signature\n",
    "# x_predict_block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##- filter any negative value\n",
    "# pixel_list = []\n",
    "# for row in range(512):\n",
    "#     for col in range(2048):\n",
    "#         if(y_predict_block_2d[row,col] == -1.0):\n",
    "#             continue\n",
    "#         else:\n",
    "#             pixel_list = y_predict_block_2d[row,col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(da_r_arr.shape)\n",
    "# print(da_r_arr.min())\n",
    "# print(da_r_arr.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #- filter any negative value\n",
    "\n",
    "# for row in range(512):\n",
    "#     for col in range(2048):\n",
    "# #         print(da_r_arr[row,col])\n",
    "        \n",
    "#         if(da_r_arr[row,col] < 0.0):\n",
    "#             da_r_arr[row,col] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(da_r_arr.shape)\n",
    "# print(da_r_arr.min())\n",
    "# print(da_r_arr.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete black regions as slice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_black_regions(in_arr):\n",
    "    \n",
    "#     for i in range(2048):\n",
    "#         if(in_arr[0,i] != -1.0):\n",
    "#             col1 = i\n",
    "#             print(col1)\n",
    "#             break\n",
    "\n",
    "#     # print(da_r_arr.shape)\n",
    "#     in_arr2 = np.delete(in_arr, slice(0, col1), 1)\n",
    "\n",
    "#     for j in range(2048):\n",
    "#         if(in_arr2[511,j] == -1.0):\n",
    "#             col2 = j\n",
    "#             print(col2)\n",
    "#             break\n",
    "\n",
    "\n",
    "#     in_arr3 = np.delete(in_arr2, slice(col2, in_arr2.shape[1]), 1)\n",
    "#     print(in_arr3.shape)\n",
    "#     arr_shape = in_arr3.shape\n",
    "\n",
    "#     return in_arr3, arr_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trim 9 images and get final shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da_noBlack, da_shape = remove_black_regions(da_r_arr)\n",
    "# ca_noBlack, ca_shape = remove_black_regions(ca_r_arr)\n",
    "# ba_noBlack, ba_shape = remove_black_regions(ba_r_arr)\n",
    "# aa_noBlack, aa_shape = remove_black_regions(aa_r_arr)\n",
    "# an_noBlack, an_shape = remove_black_regions(an_r_arr)\n",
    "# af_noBlack, af_shape = remove_black_regions(af_r_arr)\n",
    "# bf_noBlack, bf_shape = remove_black_regions(bf_r_arr)\n",
    "# cf_noBlack, cf_shape = remove_black_regions(cf_r_arr)\n",
    "# df_noBlack, df_shape = remove_black_regions(df_r_arr)\n",
    "\n",
    "# shape_list = [da_shape[1], ca_shape[1], ba_shape[1], \n",
    "#               aa_shape[1], an_shape[1], af_shape[1], \n",
    "#               bf_shape[1], cf_shape[1], df_shape[1]]\n",
    "\n",
    "# min_col = min(shape_list)\n",
    "# print(min_col)\n",
    "\n",
    "# ## trip all 9 images based on min-column\n",
    "# da_noBlack_trim = np.delete(da_noBlack, slice(min_col, da_noBlack.shape[1]), 1)\n",
    "# print(da_noBlack_trim.shape)\n",
    "\n",
    "# ca_noBlack_trim = np.delete(ca_noBlack, slice(min_col, ca_noBlack.shape[1]), 1)\n",
    "# print(ca_noBlack_trim.shape)\n",
    "\n",
    "# ba_noBlack_trim = np.delete(ba_noBlack, slice(min_col, ba_noBlack.shape[1]), 1)\n",
    "# print(ba_noBlack_trim.shape)\n",
    "\n",
    "# aa_noBlack_trim = np.delete(aa_noBlack, slice(min_col, aa_noBlack.shape[1]), 1)\n",
    "# print(aa_noBlack_trim.shape)\n",
    "\n",
    "# an_noBlack_trim = np.delete(an_noBlack, slice(min_col, an_noBlack.shape[1]), 1)\n",
    "# print(an_noBlack_trim.shape)\n",
    "\n",
    "# af_noBlack_trim = np.delete(af_noBlack, slice(min_col, af_noBlack.shape[1]), 1)\n",
    "# print(af_noBlack_trim.shape)\n",
    "\n",
    "# bf_noBlack_trim = np.delete(bf_noBlack, slice(min_col, bf_noBlack.shape[1]), 1)\n",
    "# print(bf_noBlack_trim.shape)\n",
    "\n",
    "# cf_noBlack_trim = np.delete(cf_noBlack, slice(min_col, cf_noBlack.shape[1]), 1)\n",
    "# print(cf_noBlack_trim.shape)\n",
    "\n",
    "# df_noBlack_trim = np.delete(df_noBlack, slice(min_col, df_noBlack.shape[1]), 1)\n",
    "# print(df_noBlack_trim.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## check the first input to MLP\n",
    "\n",
    "# irow = 0\n",
    "# icol = 0\n",
    "\n",
    "# print(da_noBlack_trim[irow,icol])\n",
    "# print(ca_noBlack_trim[irow,icol])\n",
    "# print(ba_noBlack_trim[irow,icol])\n",
    "# print(aa_noBlack_trim[irow,icol])\n",
    "# print(an_noBlack_trim[irow,icol])\n",
    "# print(af_noBlack_trim[irow,icol])\n",
    "# print(bf_noBlack_trim[irow,icol])\n",
    "# print(cf_noBlack_trim[irow,icol])\n",
    "# print(df_noBlack_trim[irow,icol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## loop and extract 9 cameras for each row & column, based on order of cameras (important)\n",
    "# ### this is wrong*************\n",
    "\n",
    "# x_predict_block = []\n",
    "\n",
    "# for row in range(da_noBlack_trim.shape[0]):\n",
    "#     for col in range(da_noBlack_trim.shape[1]):\n",
    "        \n",
    "#         x1 = da_noBlack_trim[row,col]\n",
    "#         x2 = ca_noBlack_trim[row,col]\n",
    "#         x3 = ba_noBlack_trim[row,col]\n",
    "#         x4 = aa_noBlack_trim[row,col]\n",
    "#         x5 = an_noBlack_trim[row,col]\n",
    "#         x6 = af_noBlack_trim[row,col]\n",
    "#         x7 = bf_noBlack_trim[row,col]\n",
    "#         x8 = cf_noBlack_trim[row,col]\n",
    "#         x9 = df_noBlack_trim[row,col]\n",
    "        \n",
    "#         nine_features = [x1,x2,x3,x4,x5,x6,x7,x8,x9]\n",
    "# #         print(type(nine_features))\n",
    "#         x_predict_block.append(nine_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_predict_block[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_predict_block[0]\n",
    "\n",
    "# ## prediction for each block\n",
    "# y_predict_block = mlp_model_best.predict(x_predict_block, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_predict_block.shape)\n",
    "# print(type(y_predict_block))\n",
    "\n",
    "# y_predict_block_2d = y_predict_block.reshape((512,-1))\n",
    "# print(y_predict_block_2d.dtype)\n",
    "# print(y_predict_block_2d.shape)\n",
    "\n",
    "# print(y_predict_block_2d.min())\n",
    "# print(y_predict_block_2d.max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot predicted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## open 1 array and check dimentions\n",
    "rough_fname_fp = os.path.join(toa_refl_dir,'roughness_toa_refl_P186_O086895_B008.dat')\n",
    "rough_2d_arr = np.fromfile(rough_fname_fp, dtype=np.double)[0:1048576].reshape((512,-1))\n",
    "rough_2d_arr.shape # (512, 1381)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale data to range(0,1)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler = scaler.fit(rough_2d_arr)\n",
    "y_predict_2d_scaled = scaler.transform(rough_2d_arr)\n",
    "\n",
    "# # Checking reconstruction\n",
    "# X_rec = scaler.inverse_transform(X_scaled)\n",
    "\n",
    "pil_im = Image.fromarray(np.uint8(y_predict_2d_scaled*255))\n",
    "display(pil_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot toa-refl.dat original file for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample block\n",
    "\n",
    "da_r = \"toa_refl_P186_O086895_B008_da_red.dat\"\n",
    "\n",
    "da_r_arr = np.fromfile(os.path.join(toa_refl_dir, da_r), dtype=np.double)[0:1048576].reshape((512,2048))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa_da = scaler.fit(da_r_arr)\n",
    "toa_refl_pos_scaled_da = scaler_toa_da.transform(da_r_arr)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_da*255))\n",
    "display(pil_im2)\n",
    "print(da_r_arr.shape)\n",
    "print(da_r_arr.min())\n",
    "# print(np.meadian(da_r_arr))\n",
    "print(da_r_arr.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa_da_trim = scaler.fit(da_noBlack_trim)\n",
    "toa_refl_pos_scaled_da_sliced = scaler_toa_da_trim.transform(da_noBlack_trim)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_da_sliced*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(ca_r_arr)\n",
    "toa_refl_pos_scaled_ca = scaler.transform(ca_r_arr)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_ca*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(ca_noBlack_trim)\n",
    "toa_refl_pos_scaled_ca = scaler.transform(ca_noBlack_trim)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_ca*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(ba_r_arr)\n",
    "toa_refl_pos_scaled_ba = scaler.transform(ba_r_arr)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_ba*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(ba_noBlack_trim)\n",
    "toa_refl_pos_scaled_ba = scaler.transform(ba_noBlack_trim)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_ba*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(aa_r_arr)\n",
    "toa_refl_pos_scaled_aa = scaler.transform(aa_r_arr)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_aa*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(aa_noBlack_trim)\n",
    "toa_refl_pos_scaled_aa = scaler.transform(aa_noBlack_trim)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_aa*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(an_r_arr)\n",
    "toa_refl_pos_scaled_an = scaler.transform(an_r_arr)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_an*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(an_noBlack_trim)\n",
    "toa_refl_pos_scaled_an = scaler.transform(an_noBlack_trim)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_an*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(af_r_arr)\n",
    "toa_refl_pos_scaled_af = scaler.transform(af_r_arr)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_af*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(af_noBlack_trim)\n",
    "toa_refl_pos_scaled_af = scaler.transform(af_noBlack_trim)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_af*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(bf_r_arr)\n",
    "toa_refl_pos_scaled_bf = scaler.transform(bf_r_arr)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_bf*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(bf_noBlack_trim)\n",
    "toa_refl_pos_scaled_af = scaler.transform(bf_noBlack_trim)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_af*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(cf_r_arr)\n",
    "toa_refl_pos_scaled_cf = scaler.transform(cf_r_arr)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_cf*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(cf_noBlack_trim)\n",
    "toa_refl_pos_scaled_cf = scaler.transform(cf_noBlack_trim)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_cf*255))\n",
    "display(pil_im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_arr.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa_df = scaler.fit(df_r_arr)\n",
    "toa_refl_pos_scaled_df = scaler_toa_df.transform(df_r_arr)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_df*255))\n",
    "display(pil_im2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "scaler_toa = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_toa = scaler.fit(df_noBlack_trim)\n",
    "toa_refl_pos_scaled_df = scaler.transform(df_noBlack_trim)\n",
    "\n",
    "pil_im2 = Image.fromarray(np.uint8(toa_refl_pos_scaled_df*255))\n",
    "display(pil_im2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write the 2D array as tif image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##- create PIL image from roughness array 2D w/ negative values\n",
    "# from PIL import Image\n",
    "\n",
    "# predicted_roughness_img = Image.fromarray(y_predict_2d)\n",
    "\n",
    "\n",
    "# ##- save roughness PIL image on disc\n",
    "# image_dir = \"/Users/ehsanmos/Documents/RnD/MISR_lab/ML_research\"\n",
    "\n",
    "# path_label = \"P186\"\n",
    "# block_label = \"B030\"\n",
    "\n",
    "# out_img_label = path_label+'_'+block_label+\".tif\"  # this image format supports saving neg- values in image\n",
    "\n",
    "# out_img_fullpath = os.path.join(image_dir, out_img_label)\n",
    "\n",
    "# predicted_roughness_img.save(out_img_fullpath)  # note: PIL is able to save neg. pixel values in .tif file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K4WcH5EWP6p",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# ---> QUESTIONS:\n",
    "Q1- how build and train an algorithm (DT, ANN, KNN, MLReg, SVR) correctly?\n",
    "\n",
    "Q2- what are parameters and hyper-parametrs of an algorithm? \n",
    "\n",
    "Q3- how fine-tune hyper-parameters? which parameter is most affective in fine-tuning an algorithm?\n",
    "\n",
    "Q4- how plot the model (regression line/ decision boundary)? and visually check performance score/skill/accuracy of regression model? how plot a model like DT?\n",
    "\n",
    "Q5- how an algorithm learns from data?\n",
    "\n",
    "Q6- if an algorithm can extrapolate on unseen data?\n",
    "\n",
    "Q7- if an algorithm returns a function or shape that is able to extrapolate?\n",
    "\n",
    "Q8- if an algorithm can be influenced by outliers?\n",
    "\n",
    "-------------------------------------------\n",
    "-------------------------------------------\n",
    "\n",
    "Other Questions:\n",
    "\n",
    "\n",
    "Q- what is generalization step? generalization test with a MISR block - statistical metrics?\n",
    "\n",
    "test generalization step to check how our model performs on unseen/new data (in real world)\n",
    "we need to select a metric to score it to be able to compare the perfoamnce of different models\n",
    "\n",
    "Q- what is a good metric to compare perfomance of different models? score? R2? (especially with old method in Anne's paper and also with baseline?)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "ML_research_SeaIceRoughness.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
