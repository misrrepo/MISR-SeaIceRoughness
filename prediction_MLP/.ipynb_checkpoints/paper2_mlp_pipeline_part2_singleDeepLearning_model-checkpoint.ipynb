{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dbc7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- main machine learning libraries used\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras  \n",
    "import sklearn\n",
    "\n",
    "#- other libraries used\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from platform import python_version\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0b69f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.13'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02be279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest run on: 2022-09-06 17:46:25.387300\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "today = datetime.today()\n",
    "\n",
    "print('latest run on:', today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcfe260",
   "metadata": {},
   "source": [
    "## check tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a7442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb3750d",
   "metadata": {},
   "source": [
    "## my functions(.) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9028d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_run(x_train, y_train):\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "    knn_model.fit(x_train, y_train) # fit algorithm to data and train it\n",
    "\n",
    "    # training score\n",
    "    knn_train_score = knn_model.score(x_train, y_train)\n",
    "    print(\"R^2 train: %.2f\" %knn_train_score)\n",
    "\n",
    "    y_train_pred_knn = knn_model.predict(x_train)\n",
    "    print('RMSE train: %.2f' %math.sqrt(mean_squared_error(y_train, y_train_pred_knn))) # square root of MSE.\n",
    "    print('MAE train: %.2f' %mean_absolute_error(y_train, y_train_pred_knn))\n",
    "#     print(MBE(y_train, y_train_pred_knn))\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aba0b3",
   "metadata": {},
   "source": [
    "## input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d30fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/Users/ehsanmos/MLP_dataset/atmmodels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf041aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset directory FOUND!\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(dataset_dir) == False:\n",
    "    print(\"dataset directory NOT found!\")\n",
    "else:\n",
    "    print(\"dataset directory FOUND!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbffb9",
   "metadata": {},
   "source": [
    "## Load input/ training dataset\n",
    "\n",
    "before doing this section, process filter final input dataset with \"check_n_filter_final_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af39e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_ds = \"atmmodel_april_and_july_2016_k_zero_9cams4bands_preprocessed.csv\"\n",
    "in_ds = \"atmmodel_april_and_july_2016_k_zero_9cams4bands_cleanready4NN.csv\"\n",
    "# in_ds = \"atmmodel_april_2016_k_zero_9cams4bands_preprocessed.csv\"\n",
    "# in_ds = \"atmmodel_july_2016_k_zero_9cams4bands_preprocessed.csv\"\n",
    "\n",
    "\n",
    "'''rms for 3 roughness groups \n",
    "'''\n",
    "# in_ds = \"atmmodel_april_2016_k_zero_9cams4bands_original_cleanready4NN_rms_zeroTo10.csv\"\n",
    "# in_ds = \"atmmodel_july_2016_k_zero_9cams4bands_original_cleanready4NN_rms_zeroTo10.csv\"\n",
    "\n",
    "# in_ds = \"atmmodel_april_2016_k_zero_9cams4bands_original_cleanready4NN_rms_10To20.csv\"\n",
    "# in_ds = \"atmmodel_july_2016_k_zero_9cams4bands_original_cleanready4NN_rms_10To20.csv\"\n",
    "# \n",
    "# in_ds = \"atmmodel_april_2016_k_zero_9cams4bands_original_cleanready4NN_rms_20To30.csv\"\n",
    "# in_ds = \"atmmodel_july_2016_k_zero_9cams4bands_original_cleanready4NN_rms_20To30.csv\"\n",
    "\n",
    "\n",
    "'''set this to num. of input features and the rest will be set in if considition\n",
    "'''\n",
    "input_feature_size = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6dc59",
   "metadata": {},
   "source": [
    "## check if input dataset file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ac74e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ehsanmos/MLP_dataset/atmmodels/atmmodel_april_and_july_2016_k_zero_9cams4bands_cleanready4NN.csv\n",
      "input dataset found!\n"
     ]
    }
   ],
   "source": [
    "in_ds_fullpath = os.path.join(dataset_dir, in_ds)\n",
    "print(in_ds_fullpath)\n",
    "\n",
    "if (not os.path.isfile(os.path.join(in_ds_fullpath))):\n",
    "    raise SystemExit()\n",
    "else:\n",
    "    print(\"input dataset found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b6d7b",
   "metadata": {},
   "source": [
    "## Read in dataset and look at dataset columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4582b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv(in_ds_fullpath, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "790ea7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58019, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4164b4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['firstLat', 'firstLon', 'anr', 'ang', 'anb', 'annir', 'aa', 'af', 'ba',\n",
       "       'bf', 'ca', 'cf', 'da', 'df', 'rms'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170b8a4",
   "metadata": {},
   "source": [
    "Note: we will build an input dataset with 9 cameras to train the mlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "654bfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''option: build dataset with 9 or 12 cameras\n",
    "'''\n",
    "\n",
    "if input_feature_size == 9:\n",
    "    input_layer_size = 9\n",
    "    excluce_columns = ['firstLat', 'firstLon', 'ang', 'anb', 'annir']\n",
    "\n",
    "elif input_feature_size == 12 :\n",
    "    input_layer_size = 12\n",
    "    excluce_columns = ['firstLat', 'firstLon']\n",
    "    \n",
    "else:\n",
    "    print('input feature size not valid.')\n",
    "    raise SystemExit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dcea79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are using 13 columns in our training dataset:\n",
      "\n",
      "\n",
      "Index(['anr', 'ang', 'anb', 'annir', 'aa', 'af', 'ba', 'bf', 'ca', 'cf', 'da',\n",
      "       'df', 'rms'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "input_ds_for_training = df_orig.drop(excluce_columns, axis=1)\n",
    "\n",
    "print('we are using %s columns in our training dataset:' %len(input_ds_for_training.columns))\n",
    "print('\\n')\n",
    "print(input_ds_for_training.columns)  # columns should be only 9 cameras + rms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b8050",
   "metadata": {},
   "source": [
    "## shuffle rows of input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f0f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "input_ds_for_training = shuffle(input_ds_for_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706d2b8",
   "metadata": {},
   "source": [
    "## Split dataset to train-test parts for training algorithms\n",
    "- we devided to plit our dataset to 2 parts (2-part split)\n",
    "- Here we use the ‘train_test_split’ to split the data in 80:20 ratio i.e. 80% of the data will be used for training the model while 20% will be used for testing the model that is built out of it.\n",
    "- note: last column should be label == rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d94ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58019, 12)\n",
      "(58019, 1)\n"
     ]
    }
   ],
   "source": [
    "#- split data set to X and Y\n",
    "\n",
    "X = input_ds_for_training.iloc[:, :-1] # to select up to last column of dataset OR [:, 0:3]\n",
    "Y = input_ds_for_training.iloc[:, -1:] # to select last column of DF\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76e41811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size= 30 percent\n",
      "train:\n",
      "(40613, 12)\n",
      "(40613, 1)\n",
      "test:\n",
      "(17406, 12)\n",
      "(17406, 1)\n"
     ]
    }
   ],
   "source": [
    "#- now split dataset to train-test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#- we use this function to split data-- from here because we are usiong SKlearn library, we change all data structures from Pandas DF to numpy\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), Y.to_numpy(), test_size=0.2, random_state=123) # Q- input is DF or numpy array?\n",
    "\n",
    "test_data_size = 0.3\n",
    "print(\"test size= %d percent\" %(test_data_size*100))\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_data_size, random_state=123) # Q- input is DF or numpy array?\n",
    "\n",
    "\n",
    "print(\"train:\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"test:\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b177d3",
   "metadata": {},
   "source": [
    "Qn- how about train-val-test (3 sections)? is this for DL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b7538",
   "metadata": {},
   "source": [
    "## Feature scalling\n",
    "\n",
    "change the scale/range of input features from their original range to a new range. So changed features will have mean=0 and std=1.\n",
    "\n",
    "source: https://www.enjoyalgorithms.com/blog/need-of-feature-scaling-in-machine-learning\n",
    "\n",
    "- We rescale data after we split data to train-test\n",
    "- all features have the same range to reduce bias in data \n",
    "- perform this step before splitting data into train-test split\n",
    "- We normalize data using the training data\n",
    "\n",
    "Qn- why FS is important? why we do FS?\n",
    "\n",
    "Qn- which method? \n",
    "\n",
    "1) standardization/ Z-score/ StandardScaler() == mean=0 & std=1; Standardize features by removing the mean and scaling to unit variance; good for datasets w/ outliers;\n",
    "\n",
    "2) MinMaxScalar() == Transform features by scaling each feature to a given range (usually [0,1])\n",
    "\n",
    "3) normalize() == Scale input vectors individually to unit norm (vector length).\n",
    "\n",
    "source: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ebb65",
   "metadata": {},
   "source": [
    "### 1) Using MinMaxScaler() method to rescale input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "102b941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #- import necessary libraries for Neural Nets\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# #- fit data\n",
    "\n",
    "# scaler_x = MinMaxScaler()\n",
    "# scaler_y = MinMaxScaler()\n",
    "\n",
    "# scaler_x.fit(X_train) # returns Fitted scaler\n",
    "# X_train_scaled = scaler_x.transform(X_train)  # transforms data\n",
    "\n",
    "# scaler_x.fit(X_test)\n",
    "# X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "# scaler_y.fit(y_train)\n",
    "# y_train_scaled = scaler_y.transform(y_train)\n",
    "\n",
    "# scaler_y.fit(y_test)\n",
    "# y_test_scaled = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d345c",
   "metadata": {},
   "source": [
    "### 2) Using StandardScaler() method \n",
    "\n",
    "to rescale input features to mean of 0 and std of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1eae3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler_x.fit(x_train).transform(x_train) # returns daata w/ mean 0 & std 1\n",
    "y_train_scaled = scaler_y.fit(y_train).transform(y_train)\n",
    "x_test_scaled = scaler_x.fit(x_test).transform(x_test)\n",
    "y_test_scaled = scaler_y.fit(y_test).transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20410efa",
   "metadata": {},
   "source": [
    "Check types of input dataset data structure; should be 2D arrays, or Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5083172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4550383093438472e-17\n",
      "0.9999999999999999\n",
      "1.7075559798632404e-16\n",
      "0.9999999999999999\n",
      "-1.4389653817961352e-17\n",
      "1.0\n",
      "-2.4921655761320303e-16\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# print(type(x_train_scaled))\n",
    "# print(type(y_train_scaled))\n",
    "# print(type(x_test_scaled))\n",
    "# print(type(y_test_scaled))\n",
    "\n",
    "print(x_train_scaled.mean())\n",
    "print(x_train_scaled.std())\n",
    "print(y_train_scaled.mean())\n",
    "print(y_train_scaled.std())\n",
    "print(x_test_scaled.mean())\n",
    "print(x_test_scaled.std())\n",
    "print(y_test_scaled.mean())\n",
    "print(y_test_scaled.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2db9f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40613, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b74162",
   "metadata": {},
   "source": [
    "## >>> KNN run >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "062ffebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train: 0.63\n",
      "RMSE train: 3.93\n",
      "MAE train: 2.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_run(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa2e9d",
   "metadata": {},
   "source": [
    "## >>> Neural Network (Regression) >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18088f76",
   "metadata": {},
   "source": [
    "## Build the NN model\n",
    "Q- how find the best architecture? for mlp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b26d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape: int,\n",
    "                hidden_layers_nodes: list,                \n",
    "                hidden_layer_activation: str = 'relu',\n",
    "                num_nodes_at_output: int = 1,\n",
    "                output_layer_activation: str = 'linear'):\n",
    "\n",
    "    # setup input layer\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=input_shape, name=\"input-layer\"))\n",
    "    model_name = ''\n",
    "\n",
    "    # setup hidden layers\n",
    "    for nodes_at_layer in hidden_layers_nodes:\n",
    "        model.add(tf.keras.layers.Dense(nodes_at_layer, activation=hidden_layer_activation))\n",
    "        model_name += 'dense_'+str(nodes_at_layer)+'_'\n",
    "\n",
    "    # setup output layer\n",
    "    model.add(tf.keras.layers.Dense(num_nodes_at_output, activation=output_layer_activation, name=\"output-layer-SIR\"))\n",
    "    model._name = model_name[:-1]\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30c41849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 17:46:28.084252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1dc8f6370>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_node_list = [9, 9] # this architecture is based on our previous search for optimal NN\n",
    "\n",
    "built_model = build_model(input_shape = input_layer_size, \n",
    "                          hidden_layers_nodes = layers_node_list)\n",
    "\n",
    "built_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72227c88",
   "metadata": {},
   "source": [
    "## Tune the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d751192",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_lrt = 0.001\n",
    "\n",
    "opt_alg = tf.keras.optimizers.Adam(\n",
    "                                learning_rate=adam_lrt,   # then everu step * 10 to get to 10\n",
    "                                beta_1=0.9,\n",
    "                                beta_2=0.999,\n",
    "                                epsilon=1e-07,\n",
    "                                amsgrad=False,\n",
    "                                name='Adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ddba6",
   "metadata": {},
   "source": [
    "## compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2dc909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dense_9_dense_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 9)                 117       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 90        \n",
      "                                                                 \n",
      " output-layer-SIR (Dense)    (None, 1)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 217\n",
      "Trainable params: 217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile each model\n",
    "built_model.compile(\n",
    "                    loss='mse', \n",
    "                    optimizer=opt_alg, \n",
    "                    metrics=['mse','mae']\n",
    "            )\n",
    "\n",
    "built_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8e941",
   "metadata": {},
   "source": [
    "## train our model (fit algorithm to train dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d62c75a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "325/325 [==============================] - 1s 1ms/step - loss: 79.8210 - mse: 79.8210 - mae: 6.2842 - val_loss: 47.4235 - val_mse: 47.4235 - val_mae: 4.7227\n",
      "Epoch 2/1000\n",
      "325/325 [==============================] - 0s 929us/step - loss: 54.0237 - mse: 54.0237 - mae: 4.7172 - val_loss: 45.7189 - val_mse: 45.7189 - val_mae: 4.5775\n",
      "Epoch 3/1000\n",
      "325/325 [==============================] - 0s 946us/step - loss: 49.8142 - mse: 49.8142 - mae: 4.6523 - val_loss: 42.6154 - val_mse: 42.6154 - val_mae: 4.6552\n",
      "Epoch 4/1000\n",
      "325/325 [==============================] - 0s 936us/step - loss: 46.6881 - mse: 46.6881 - mae: 4.6751 - val_loss: 41.4808 - val_mse: 41.4808 - val_mae: 4.5888\n",
      "Epoch 5/1000\n",
      "325/325 [==============================] - 0s 934us/step - loss: 44.2139 - mse: 44.2139 - mae: 4.6697 - val_loss: 40.2198 - val_mse: 40.2198 - val_mae: 4.6328\n",
      "Epoch 6/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 42.3992 - mse: 42.3992 - mae: 4.6911 - val_loss: 39.3815 - val_mse: 39.3815 - val_mae: 4.6715\n",
      "Epoch 7/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 40.9815 - mse: 40.9815 - mae: 4.7092 - val_loss: 38.7721 - val_mse: 38.7721 - val_mae: 4.7191\n",
      "Epoch 8/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 39.8355 - mse: 39.8355 - mae: 4.7301 - val_loss: 38.3228 - val_mse: 38.3228 - val_mae: 4.7023\n",
      "Epoch 9/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 38.8929 - mse: 38.8929 - mae: 4.7319 - val_loss: 38.0084 - val_mse: 38.0084 - val_mae: 4.6793\n",
      "Epoch 10/1000\n",
      "325/325 [==============================] - 0s 928us/step - loss: 38.1240 - mse: 38.1240 - mae: 4.7349 - val_loss: 37.5638 - val_mse: 37.5638 - val_mae: 4.6786\n",
      "Epoch 11/1000\n",
      "325/325 [==============================] - 0s 940us/step - loss: 37.6124 - mse: 37.6124 - mae: 4.7187 - val_loss: 37.2535 - val_mse: 37.2535 - val_mae: 4.7060\n",
      "Epoch 12/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 37.3289 - mse: 37.3289 - mae: 4.7159 - val_loss: 37.0807 - val_mse: 37.0807 - val_mae: 4.6593\n",
      "Epoch 13/1000\n",
      "325/325 [==============================] - 0s 933us/step - loss: 37.1394 - mse: 37.1394 - mae: 4.7037 - val_loss: 36.9150 - val_mse: 36.9150 - val_mae: 4.6660\n",
      "Epoch 14/1000\n",
      "325/325 [==============================] - 0s 946us/step - loss: 36.9870 - mse: 36.9870 - mae: 4.6912 - val_loss: 36.7857 - val_mse: 36.7857 - val_mae: 4.6638\n",
      "Epoch 15/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 36.8537 - mse: 36.8537 - mae: 4.6794 - val_loss: 36.6805 - val_mse: 36.6805 - val_mae: 4.6428\n",
      "Epoch 16/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 36.7641 - mse: 36.7641 - mae: 4.6688 - val_loss: 36.6031 - val_mse: 36.6031 - val_mae: 4.6475\n",
      "Epoch 17/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 36.6628 - mse: 36.6628 - mae: 4.6602 - val_loss: 36.5762 - val_mse: 36.5762 - val_mae: 4.6941\n",
      "Epoch 18/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 36.6000 - mse: 36.6000 - mae: 4.6568 - val_loss: 36.4835 - val_mse: 36.4835 - val_mae: 4.6485\n",
      "Epoch 19/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 36.5411 - mse: 36.5411 - mae: 4.6469 - val_loss: 36.4065 - val_mse: 36.4065 - val_mae: 4.6248\n",
      "Epoch 20/1000\n",
      "325/325 [==============================] - 0s 944us/step - loss: 36.4797 - mse: 36.4797 - mae: 4.6400 - val_loss: 36.3721 - val_mse: 36.3721 - val_mae: 4.6533\n",
      "Epoch 21/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 36.4415 - mse: 36.4415 - mae: 4.6382 - val_loss: 36.3251 - val_mse: 36.3251 - val_mae: 4.5869\n",
      "Epoch 22/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 36.3770 - mse: 36.3770 - mae: 4.6262 - val_loss: 36.2643 - val_mse: 36.2643 - val_mae: 4.6169\n",
      "Epoch 23/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 36.3522 - mse: 36.3522 - mae: 4.6301 - val_loss: 36.2300 - val_mse: 36.2300 - val_mae: 4.6135\n",
      "Epoch 24/1000\n",
      "325/325 [==============================] - 0s 950us/step - loss: 36.3008 - mse: 36.3008 - mae: 4.6218 - val_loss: 36.2081 - val_mse: 36.2081 - val_mae: 4.5639\n",
      "Epoch 25/1000\n",
      "325/325 [==============================] - 0s 936us/step - loss: 36.2624 - mse: 36.2624 - mae: 4.6189 - val_loss: 36.2450 - val_mse: 36.2450 - val_mae: 4.6767\n",
      "Epoch 26/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 36.2221 - mse: 36.2221 - mae: 4.6195 - val_loss: 36.1106 - val_mse: 36.1106 - val_mae: 4.5809\n",
      "Epoch 27/1000\n",
      "325/325 [==============================] - 0s 941us/step - loss: 36.2311 - mse: 36.2311 - mae: 4.6177 - val_loss: 36.0728 - val_mse: 36.0728 - val_mae: 4.6009\n",
      "Epoch 28/1000\n",
      "325/325 [==============================] - 0s 941us/step - loss: 36.1765 - mse: 36.1765 - mae: 4.6133 - val_loss: 36.0592 - val_mse: 36.0592 - val_mae: 4.5640\n",
      "Epoch 29/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 36.1327 - mse: 36.1327 - mae: 4.6102 - val_loss: 36.0363 - val_mse: 36.0363 - val_mae: 4.5529\n",
      "Epoch 30/1000\n",
      "325/325 [==============================] - 0s 954us/step - loss: 36.1303 - mse: 36.1303 - mae: 4.6077 - val_loss: 36.0037 - val_mse: 36.0037 - val_mae: 4.5827\n",
      "Epoch 31/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 36.0744 - mse: 36.0744 - mae: 4.6053 - val_loss: 36.0840 - val_mse: 36.0840 - val_mae: 4.5025\n",
      "Epoch 32/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 36.0382 - mse: 36.0382 - mae: 4.6054 - val_loss: 35.9719 - val_mse: 35.9719 - val_mae: 4.5281\n",
      "Epoch 33/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 36.0307 - mse: 36.0307 - mae: 4.6023 - val_loss: 35.9121 - val_mse: 35.9121 - val_mae: 4.5468\n",
      "Epoch 34/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 35.9856 - mse: 35.9856 - mae: 4.5974 - val_loss: 35.9529 - val_mse: 35.9529 - val_mae: 4.6430\n",
      "Epoch 35/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 35.9703 - mse: 35.9703 - mae: 4.5978 - val_loss: 35.8666 - val_mse: 35.8666 - val_mae: 4.6003\n",
      "Epoch 36/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 35.9322 - mse: 35.9322 - mae: 4.5939 - val_loss: 35.8939 - val_mse: 35.8939 - val_mae: 4.5854\n",
      "Epoch 37/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 35.9286 - mse: 35.9286 - mae: 4.5984 - val_loss: 35.8143 - val_mse: 35.8143 - val_mae: 4.5421\n",
      "Epoch 38/1000\n",
      "325/325 [==============================] - 0s 941us/step - loss: 35.9037 - mse: 35.9037 - mae: 4.5916 - val_loss: 35.7823 - val_mse: 35.7823 - val_mae: 4.5683\n",
      "Epoch 39/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 35.8580 - mse: 35.8580 - mae: 4.5867 - val_loss: 35.8226 - val_mse: 35.8226 - val_mae: 4.6130\n",
      "Epoch 40/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 35.8518 - mse: 35.8518 - mae: 4.5859 - val_loss: 35.8695 - val_mse: 35.8695 - val_mae: 4.6312\n",
      "Epoch 41/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 35.8361 - mse: 35.8361 - mae: 4.5887 - val_loss: 35.7373 - val_mse: 35.7373 - val_mae: 4.5643\n",
      "Epoch 42/1000\n",
      "325/325 [==============================] - 0s 932us/step - loss: 35.8017 - mse: 35.8017 - mae: 4.5838 - val_loss: 35.6937 - val_mse: 35.6937 - val_mae: 4.5452\n",
      "Epoch 43/1000\n",
      "325/325 [==============================] - 0s 982us/step - loss: 35.7754 - mse: 35.7754 - mae: 4.5819 - val_loss: 35.6771 - val_mse: 35.6770 - val_mae: 4.5186\n",
      "Epoch 44/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 35.7257 - mse: 35.7257 - mae: 4.5736 - val_loss: 35.6254 - val_mse: 35.6254 - val_mae: 4.5868\n",
      "Epoch 45/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 35.6958 - mse: 35.6958 - mae: 4.5760 - val_loss: 35.6318 - val_mse: 35.6318 - val_mae: 4.4930\n",
      "Epoch 46/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 35.6508 - mse: 35.6508 - mae: 4.5670 - val_loss: 35.6908 - val_mse: 35.6908 - val_mae: 4.6266\n",
      "Epoch 47/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 35.6130 - mse: 35.6130 - mae: 4.5669 - val_loss: 35.5396 - val_mse: 35.5396 - val_mae: 4.5874\n",
      "Epoch 48/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 940us/step - loss: 35.5906 - mse: 35.5906 - mae: 4.5634 - val_loss: 35.5314 - val_mse: 35.5314 - val_mae: 4.5925\n",
      "Epoch 49/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 35.5561 - mse: 35.5561 - mae: 4.5578 - val_loss: 35.5194 - val_mse: 35.5194 - val_mae: 4.6050\n",
      "Epoch 50/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 35.4944 - mse: 35.4944 - mae: 4.5567 - val_loss: 35.4556 - val_mse: 35.4556 - val_mae: 4.5645\n",
      "Epoch 51/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 35.4861 - mse: 35.4861 - mae: 4.5518 - val_loss: 35.3978 - val_mse: 35.3978 - val_mae: 4.5261\n",
      "Epoch 52/1000\n",
      "325/325 [==============================] - 0s 938us/step - loss: 35.4656 - mse: 35.4656 - mae: 4.5473 - val_loss: 35.4341 - val_mse: 35.4341 - val_mae: 4.4989\n",
      "Epoch 53/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 35.4653 - mse: 35.4653 - mae: 4.5544 - val_loss: 35.3332 - val_mse: 35.3332 - val_mae: 4.5001\n",
      "Epoch 54/1000\n",
      "325/325 [==============================] - 0s 954us/step - loss: 35.4161 - mse: 35.4161 - mae: 4.5423 - val_loss: 35.3530 - val_mse: 35.3530 - val_mae: 4.5513\n",
      "Epoch 55/1000\n",
      "325/325 [==============================] - 0s 942us/step - loss: 35.3587 - mse: 35.3587 - mae: 4.5391 - val_loss: 35.3836 - val_mse: 35.3836 - val_mae: 4.4513\n",
      "Epoch 56/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 35.3145 - mse: 35.3145 - mae: 4.5359 - val_loss: 35.2324 - val_mse: 35.2324 - val_mae: 4.5207\n",
      "Epoch 57/1000\n",
      "325/325 [==============================] - 0s 942us/step - loss: 35.2869 - mse: 35.2869 - mae: 4.5385 - val_loss: 35.3136 - val_mse: 35.3136 - val_mae: 4.4414\n",
      "Epoch 58/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 35.2682 - mse: 35.2682 - mae: 4.5284 - val_loss: 35.2348 - val_mse: 35.2348 - val_mae: 4.5406\n",
      "Epoch 59/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 35.2488 - mse: 35.2488 - mae: 4.5308 - val_loss: 35.1762 - val_mse: 35.1762 - val_mae: 4.5017\n",
      "Epoch 60/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 35.2202 - mse: 35.2202 - mae: 4.5230 - val_loss: 35.1345 - val_mse: 35.1345 - val_mae: 4.4844\n",
      "Epoch 61/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 35.1942 - mse: 35.1942 - mae: 4.5225 - val_loss: 35.1384 - val_mse: 35.1384 - val_mae: 4.5009\n",
      "Epoch 62/1000\n",
      "325/325 [==============================] - 0s 987us/step - loss: 35.1374 - mse: 35.1374 - mae: 4.5187 - val_loss: 35.0903 - val_mse: 35.0903 - val_mae: 4.4969\n",
      "Epoch 63/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 35.1455 - mse: 35.1455 - mae: 4.5159 - val_loss: 35.0446 - val_mse: 35.0446 - val_mae: 4.4978\n",
      "Epoch 64/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 35.1311 - mse: 35.1311 - mae: 4.5213 - val_loss: 35.2626 - val_mse: 35.2626 - val_mae: 4.3870\n",
      "Epoch 65/1000\n",
      "325/325 [==============================] - 0s 932us/step - loss: 35.0824 - mse: 35.0824 - mae: 4.5092 - val_loss: 35.0561 - val_mse: 35.0561 - val_mae: 4.4721\n",
      "Epoch 66/1000\n",
      "325/325 [==============================] - 0s 944us/step - loss: 35.0628 - mse: 35.0628 - mae: 4.5087 - val_loss: 35.0487 - val_mse: 35.0487 - val_mae: 4.4352\n",
      "Epoch 67/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 35.0230 - mse: 35.0230 - mae: 4.5077 - val_loss: 34.9655 - val_mse: 34.9655 - val_mae: 4.4952\n",
      "Epoch 68/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 34.9896 - mse: 34.9896 - mae: 4.5051 - val_loss: 35.0033 - val_mse: 35.0033 - val_mae: 4.4400\n",
      "Epoch 69/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 34.9935 - mse: 34.9935 - mae: 4.5060 - val_loss: 34.9193 - val_mse: 34.9193 - val_mae: 4.4605\n",
      "Epoch 70/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 34.9427 - mse: 34.9427 - mae: 4.4989 - val_loss: 34.9184 - val_mse: 34.9184 - val_mae: 4.4458\n",
      "Epoch 71/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 34.9299 - mse: 34.9299 - mae: 4.4965 - val_loss: 34.8868 - val_mse: 34.8868 - val_mae: 4.4831\n",
      "Epoch 72/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 34.9114 - mse: 34.9114 - mae: 4.4952 - val_loss: 34.9278 - val_mse: 34.9278 - val_mae: 4.5573\n",
      "Epoch 73/1000\n",
      "325/325 [==============================] - 0s 947us/step - loss: 34.9088 - mse: 34.9088 - mae: 4.4959 - val_loss: 35.2092 - val_mse: 35.2092 - val_mae: 4.5917\n",
      "Epoch 74/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 34.8967 - mse: 34.8967 - mae: 4.4949 - val_loss: 34.8611 - val_mse: 34.8611 - val_mae: 4.5282\n",
      "Epoch 75/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 34.8566 - mse: 34.8566 - mae: 4.4896 - val_loss: 34.8269 - val_mse: 34.8269 - val_mae: 4.5176\n",
      "Epoch 76/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 34.8190 - mse: 34.8190 - mae: 4.4906 - val_loss: 34.8621 - val_mse: 34.8621 - val_mae: 4.4049\n",
      "Epoch 77/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 34.7833 - mse: 34.7833 - mae: 4.4819 - val_loss: 34.7808 - val_mse: 34.7808 - val_mae: 4.5030\n",
      "Epoch 78/1000\n",
      "325/325 [==============================] - 0s 947us/step - loss: 34.8054 - mse: 34.8054 - mae: 4.4808 - val_loss: 34.7810 - val_mse: 34.7810 - val_mae: 4.5071\n",
      "Epoch 79/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 34.7606 - mse: 34.7606 - mae: 4.4851 - val_loss: 35.1058 - val_mse: 35.1058 - val_mae: 4.3166\n",
      "Epoch 80/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 34.7770 - mse: 34.7770 - mae: 4.4819 - val_loss: 34.7279 - val_mse: 34.7279 - val_mae: 4.4352\n",
      "Epoch 81/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 34.7597 - mse: 34.7597 - mae: 4.4811 - val_loss: 34.7168 - val_mse: 34.7168 - val_mae: 4.4719\n",
      "Epoch 82/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 34.7192 - mse: 34.7192 - mae: 4.4773 - val_loss: 35.0553 - val_mse: 35.0553 - val_mae: 4.3323\n",
      "Epoch 83/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 34.6880 - mse: 34.6880 - mae: 4.4732 - val_loss: 34.6947 - val_mse: 34.6947 - val_mae: 4.4168\n",
      "Epoch 84/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 34.6755 - mse: 34.6755 - mae: 4.4715 - val_loss: 34.6809 - val_mse: 34.6809 - val_mae: 4.4746\n",
      "Epoch 85/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 34.6950 - mse: 34.6950 - mae: 4.4723 - val_loss: 34.6453 - val_mse: 34.6453 - val_mae: 4.4741\n",
      "Epoch 86/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 34.6519 - mse: 34.6519 - mae: 4.4716 - val_loss: 34.5868 - val_mse: 34.5868 - val_mae: 4.4489\n",
      "Epoch 87/1000\n",
      "325/325 [==============================] - 0s 939us/step - loss: 34.6267 - mse: 34.6267 - mae: 4.4682 - val_loss: 34.6057 - val_mse: 34.6057 - val_mae: 4.4454\n",
      "Epoch 88/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 34.5934 - mse: 34.5934 - mae: 4.4635 - val_loss: 34.5730 - val_mse: 34.5730 - val_mae: 4.4771\n",
      "Epoch 89/1000\n",
      "325/325 [==============================] - 0s 935us/step - loss: 34.6014 - mse: 34.6014 - mae: 4.4662 - val_loss: 34.7011 - val_mse: 34.7011 - val_mae: 4.3500\n",
      "Epoch 90/1000\n",
      "325/325 [==============================] - 0s 947us/step - loss: 34.5421 - mse: 34.5421 - mae: 4.4571 - val_loss: 34.5893 - val_mse: 34.5893 - val_mae: 4.5193\n",
      "Epoch 91/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 34.5598 - mse: 34.5598 - mae: 4.4660 - val_loss: 34.5455 - val_mse: 34.5455 - val_mae: 4.4318\n",
      "Epoch 92/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 34.5111 - mse: 34.5111 - mae: 4.4575 - val_loss: 34.6125 - val_mse: 34.6125 - val_mae: 4.4432\n",
      "Epoch 93/1000\n",
      "325/325 [==============================] - 0s 954us/step - loss: 34.4603 - mse: 34.4603 - mae: 4.4524 - val_loss: 34.5336 - val_mse: 34.5336 - val_mae: 4.4786\n",
      "Epoch 94/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 34.4720 - mse: 34.4720 - mae: 4.4577 - val_loss: 34.5310 - val_mse: 34.5310 - val_mae: 4.3823\n",
      "Epoch 95/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 959us/step - loss: 34.4462 - mse: 34.4462 - mae: 4.4494 - val_loss: 34.5742 - val_mse: 34.5742 - val_mae: 4.5258\n",
      "Epoch 96/1000\n",
      "325/325 [==============================] - 0s 936us/step - loss: 34.4266 - mse: 34.4266 - mae: 4.4517 - val_loss: 34.4835 - val_mse: 34.4835 - val_mae: 4.3970\n",
      "Epoch 97/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 34.3653 - mse: 34.3653 - mae: 4.4499 - val_loss: 34.4806 - val_mse: 34.4806 - val_mae: 4.3739\n",
      "Epoch 98/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 34.3655 - mse: 34.3655 - mae: 4.4457 - val_loss: 34.4034 - val_mse: 34.4034 - val_mae: 4.4039\n",
      "Epoch 99/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 34.3357 - mse: 34.3357 - mae: 4.4405 - val_loss: 34.3507 - val_mse: 34.3507 - val_mae: 4.4593\n",
      "Epoch 100/1000\n",
      "325/325 [==============================] - 0s 938us/step - loss: 34.3468 - mse: 34.3468 - mae: 4.4492 - val_loss: 34.3651 - val_mse: 34.3651 - val_mae: 4.4091\n",
      "Epoch 101/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 34.3205 - mse: 34.3205 - mae: 4.4400 - val_loss: 34.3758 - val_mse: 34.3758 - val_mae: 4.4589\n",
      "Epoch 102/1000\n",
      "325/325 [==============================] - 0s 944us/step - loss: 34.2852 - mse: 34.2852 - mae: 4.4429 - val_loss: 34.7528 - val_mse: 34.7528 - val_mae: 4.2631\n",
      "Epoch 103/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 34.2779 - mse: 34.2779 - mae: 4.4310 - val_loss: 34.6299 - val_mse: 34.6299 - val_mae: 4.5621\n",
      "Epoch 104/1000\n",
      "325/325 [==============================] - 0s 941us/step - loss: 34.2271 - mse: 34.2271 - mae: 4.4368 - val_loss: 34.2667 - val_mse: 34.2667 - val_mae: 4.4373\n",
      "Epoch 105/1000\n",
      "325/325 [==============================] - 0s 939us/step - loss: 34.2300 - mse: 34.2300 - mae: 4.4361 - val_loss: 34.2468 - val_mse: 34.2468 - val_mae: 4.3667\n",
      "Epoch 106/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 34.1890 - mse: 34.1890 - mae: 4.4276 - val_loss: 34.2062 - val_mse: 34.2062 - val_mae: 4.4225\n",
      "Epoch 107/1000\n",
      "325/325 [==============================] - 0s 943us/step - loss: 34.1945 - mse: 34.1945 - mae: 4.4279 - val_loss: 34.2436 - val_mse: 34.2436 - val_mae: 4.3728\n",
      "Epoch 108/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 34.1445 - mse: 34.1445 - mae: 4.4264 - val_loss: 34.2293 - val_mse: 34.2293 - val_mae: 4.4544\n",
      "Epoch 109/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 34.2490 - mse: 34.2490 - mae: 4.4339 - val_loss: 34.1735 - val_mse: 34.1735 - val_mae: 4.3848\n",
      "Epoch 110/1000\n",
      "325/325 [==============================] - 0s 941us/step - loss: 34.1392 - mse: 34.1392 - mae: 4.4247 - val_loss: 34.2378 - val_mse: 34.2378 - val_mae: 4.3359\n",
      "Epoch 111/1000\n",
      "325/325 [==============================] - 0s 936us/step - loss: 34.1097 - mse: 34.1097 - mae: 4.4229 - val_loss: 34.1445 - val_mse: 34.1445 - val_mae: 4.3683\n",
      "Epoch 112/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 34.0804 - mse: 34.0804 - mae: 4.4183 - val_loss: 34.1088 - val_mse: 34.1088 - val_mae: 4.3947\n",
      "Epoch 113/1000\n",
      "325/325 [==============================] - 0s 940us/step - loss: 34.0876 - mse: 34.0876 - mae: 4.4220 - val_loss: 34.1366 - val_mse: 34.1366 - val_mae: 4.3552\n",
      "Epoch 114/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 34.0664 - mse: 34.0664 - mae: 4.4178 - val_loss: 34.2130 - val_mse: 34.2130 - val_mae: 4.2932\n",
      "Epoch 115/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 34.0236 - mse: 34.0236 - mae: 4.4116 - val_loss: 34.0466 - val_mse: 34.0466 - val_mae: 4.3882\n",
      "Epoch 116/1000\n",
      "325/325 [==============================] - 0s 942us/step - loss: 33.9978 - mse: 33.9978 - mae: 4.4103 - val_loss: 34.1237 - val_mse: 34.1237 - val_mae: 4.4736\n",
      "Epoch 117/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 34.0063 - mse: 34.0063 - mae: 4.4130 - val_loss: 34.0871 - val_mse: 34.0871 - val_mae: 4.4413\n",
      "Epoch 118/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 33.9682 - mse: 33.9682 - mae: 4.4082 - val_loss: 34.0548 - val_mse: 34.0548 - val_mae: 4.3391\n",
      "Epoch 119/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 33.9411 - mse: 33.9411 - mae: 4.4082 - val_loss: 34.2476 - val_mse: 34.2476 - val_mae: 4.2761\n",
      "Epoch 120/1000\n",
      "325/325 [==============================] - 0s 946us/step - loss: 33.9679 - mse: 33.9679 - mae: 4.4095 - val_loss: 33.9594 - val_mse: 33.9594 - val_mae: 4.3889\n",
      "Epoch 121/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 33.9353 - mse: 33.9353 - mae: 4.3999 - val_loss: 34.1769 - val_mse: 34.1769 - val_mae: 4.5059\n",
      "Epoch 122/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 33.9242 - mse: 33.9242 - mae: 4.4064 - val_loss: 33.9421 - val_mse: 33.9421 - val_mae: 4.3775\n",
      "Epoch 123/1000\n",
      "325/325 [==============================] - 0s 934us/step - loss: 33.9092 - mse: 33.9092 - mae: 4.4032 - val_loss: 33.9163 - val_mse: 33.9163 - val_mae: 4.3638\n",
      "Epoch 124/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 33.8906 - mse: 33.8906 - mae: 4.4027 - val_loss: 33.9719 - val_mse: 33.9719 - val_mae: 4.3261\n",
      "Epoch 125/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 33.9179 - mse: 33.9179 - mae: 4.4018 - val_loss: 34.0278 - val_mse: 34.0278 - val_mae: 4.3397\n",
      "Epoch 126/1000\n",
      "325/325 [==============================] - 0s 939us/step - loss: 33.8734 - mse: 33.8734 - mae: 4.4026 - val_loss: 33.8837 - val_mse: 33.8837 - val_mae: 4.3780\n",
      "Epoch 127/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 33.8900 - mse: 33.8900 - mae: 4.3976 - val_loss: 34.1965 - val_mse: 34.1965 - val_mae: 4.5042\n",
      "Epoch 128/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 33.8472 - mse: 33.8472 - mae: 4.3923 - val_loss: 33.9035 - val_mse: 33.9035 - val_mae: 4.3602\n",
      "Epoch 129/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 33.8344 - mse: 33.8344 - mae: 4.3997 - val_loss: 33.9134 - val_mse: 33.9134 - val_mae: 4.3030\n",
      "Epoch 130/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 33.7999 - mse: 33.7999 - mae: 4.3887 - val_loss: 33.9536 - val_mse: 33.9536 - val_mae: 4.4579\n",
      "Epoch 131/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 33.8518 - mse: 33.8518 - mae: 4.3980 - val_loss: 33.9160 - val_mse: 33.9160 - val_mae: 4.4655\n",
      "Epoch 132/1000\n",
      "325/325 [==============================] - 0s 937us/step - loss: 33.8216 - mse: 33.8216 - mae: 4.3955 - val_loss: 33.9064 - val_mse: 33.9064 - val_mae: 4.2866\n",
      "Epoch 133/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 33.7392 - mse: 33.7392 - mae: 4.3873 - val_loss: 33.8257 - val_mse: 33.8257 - val_mae: 4.3831\n",
      "Epoch 134/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 33.7715 - mse: 33.7715 - mae: 4.3874 - val_loss: 33.7983 - val_mse: 33.7983 - val_mae: 4.3699\n",
      "Epoch 135/1000\n",
      "325/325 [==============================] - 0s 954us/step - loss: 33.7405 - mse: 33.7405 - mae: 4.3878 - val_loss: 33.8113 - val_mse: 33.8113 - val_mae: 4.3696\n",
      "Epoch 136/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 33.7413 - mse: 33.7413 - mae: 4.3868 - val_loss: 33.8150 - val_mse: 33.8150 - val_mae: 4.3019\n",
      "Epoch 137/1000\n",
      "325/325 [==============================] - 0s 947us/step - loss: 33.7079 - mse: 33.7079 - mae: 4.3829 - val_loss: 33.7869 - val_mse: 33.7869 - val_mae: 4.3478\n",
      "Epoch 138/1000\n",
      "325/325 [==============================] - 0s 947us/step - loss: 33.7250 - mse: 33.7250 - mae: 4.3825 - val_loss: 33.8717 - val_mse: 33.8717 - val_mae: 4.2806\n",
      "Epoch 139/1000\n",
      "325/325 [==============================] - 0s 954us/step - loss: 33.7423 - mse: 33.7423 - mae: 4.3861 - val_loss: 34.4555 - val_mse: 34.4555 - val_mae: 4.5390\n",
      "Epoch 140/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 33.6961 - mse: 33.6961 - mae: 4.3799 - val_loss: 33.7211 - val_mse: 33.7211 - val_mae: 4.3434\n",
      "Epoch 141/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 33.6816 - mse: 33.6816 - mae: 4.3833 - val_loss: 33.9402 - val_mse: 33.9402 - val_mae: 4.4310\n",
      "Epoch 142/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 965us/step - loss: 33.6687 - mse: 33.6687 - mae: 4.3789 - val_loss: 33.9362 - val_mse: 33.9362 - val_mae: 4.2846\n",
      "Epoch 143/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 33.6579 - mse: 33.6579 - mae: 4.3790 - val_loss: 33.7149 - val_mse: 33.7149 - val_mae: 4.4206\n",
      "Epoch 144/1000\n",
      "325/325 [==============================] - 0s 995us/step - loss: 33.6680 - mse: 33.6680 - mae: 4.3764 - val_loss: 33.6849 - val_mse: 33.6849 - val_mae: 4.3304\n",
      "Epoch 145/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 33.6251 - mse: 33.6251 - mae: 4.3768 - val_loss: 33.8599 - val_mse: 33.8599 - val_mae: 4.4710\n",
      "Epoch 146/1000\n",
      "325/325 [==============================] - 0s 980us/step - loss: 33.6399 - mse: 33.6399 - mae: 4.3769 - val_loss: 33.8793 - val_mse: 33.8793 - val_mae: 4.5089\n",
      "Epoch 147/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 33.6405 - mse: 33.6405 - mae: 4.3785 - val_loss: 33.7552 - val_mse: 33.7552 - val_mae: 4.2631\n",
      "Epoch 148/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 33.6667 - mse: 33.6667 - mae: 4.3767 - val_loss: 33.6693 - val_mse: 33.6693 - val_mae: 4.3074\n",
      "Epoch 149/1000\n",
      "325/325 [==============================] - 0s 998us/step - loss: 33.5929 - mse: 33.5929 - mae: 4.3743 - val_loss: 33.6459 - val_mse: 33.6459 - val_mae: 4.3878\n",
      "Epoch 150/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 33.5594 - mse: 33.5594 - mae: 4.3720 - val_loss: 33.6254 - val_mse: 33.6254 - val_mae: 4.3383\n",
      "Epoch 151/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 33.6172 - mse: 33.6172 - mae: 4.3726 - val_loss: 33.7755 - val_mse: 33.7755 - val_mae: 4.4257\n",
      "Epoch 152/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 33.5831 - mse: 33.5831 - mae: 4.3716 - val_loss: 33.6627 - val_mse: 33.6627 - val_mae: 4.2855\n",
      "Epoch 153/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 33.5430 - mse: 33.5431 - mae: 4.3674 - val_loss: 33.6053 - val_mse: 33.6053 - val_mae: 4.3578\n",
      "Epoch 154/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 33.5337 - mse: 33.5337 - mae: 4.3670 - val_loss: 33.6172 - val_mse: 33.6172 - val_mae: 4.4195\n",
      "Epoch 155/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 33.5341 - mse: 33.5341 - mae: 4.3690 - val_loss: 33.6436 - val_mse: 33.6436 - val_mae: 4.2953\n",
      "Epoch 156/1000\n",
      "325/325 [==============================] - 0s 937us/step - loss: 33.4967 - mse: 33.4967 - mae: 4.3670 - val_loss: 33.5916 - val_mse: 33.5916 - val_mae: 4.3327\n",
      "Epoch 157/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 33.5246 - mse: 33.5246 - mae: 4.3641 - val_loss: 33.6035 - val_mse: 33.6035 - val_mae: 4.4155\n",
      "Epoch 158/1000\n",
      "325/325 [==============================] - 0s 951us/step - loss: 33.4899 - mse: 33.4899 - mae: 4.3668 - val_loss: 33.5991 - val_mse: 33.5991 - val_mae: 4.2873\n",
      "Epoch 159/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 33.4829 - mse: 33.4829 - mae: 4.3632 - val_loss: 33.5442 - val_mse: 33.5442 - val_mae: 4.3463\n",
      "Epoch 160/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 33.5154 - mse: 33.5154 - mae: 4.3641 - val_loss: 33.5500 - val_mse: 33.5500 - val_mae: 4.3741\n",
      "Epoch 161/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 33.4892 - mse: 33.4892 - mae: 4.3644 - val_loss: 33.7112 - val_mse: 33.7112 - val_mae: 4.4260\n",
      "Epoch 162/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 33.5137 - mse: 33.5137 - mae: 4.3660 - val_loss: 33.5840 - val_mse: 33.5840 - val_mae: 4.2878\n",
      "Epoch 163/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 33.4430 - mse: 33.4430 - mae: 4.3571 - val_loss: 33.5661 - val_mse: 33.5661 - val_mae: 4.3547\n",
      "Epoch 164/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 33.4604 - mse: 33.4604 - mae: 4.3624 - val_loss: 33.5818 - val_mse: 33.5818 - val_mae: 4.3803\n",
      "Epoch 165/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 33.4042 - mse: 33.4042 - mae: 4.3542 - val_loss: 33.5642 - val_mse: 33.5642 - val_mae: 4.4362\n",
      "Epoch 166/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 33.4402 - mse: 33.4402 - mae: 4.3582 - val_loss: 33.5155 - val_mse: 33.5155 - val_mae: 4.4059\n",
      "Epoch 167/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 33.4009 - mse: 33.4009 - mae: 4.3602 - val_loss: 33.4815 - val_mse: 33.4815 - val_mae: 4.3018\n",
      "Epoch 168/1000\n",
      "325/325 [==============================] - 0s 954us/step - loss: 33.3985 - mse: 33.3985 - mae: 4.3532 - val_loss: 33.5572 - val_mse: 33.5572 - val_mae: 4.3386\n",
      "Epoch 169/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 33.3721 - mse: 33.3721 - mae: 4.3556 - val_loss: 33.5009 - val_mse: 33.5009 - val_mae: 4.3822\n",
      "Epoch 170/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 33.4136 - mse: 33.4136 - mae: 4.3522 - val_loss: 33.5045 - val_mse: 33.5045 - val_mae: 4.3722\n",
      "Epoch 171/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 33.4166 - mse: 33.4166 - mae: 4.3591 - val_loss: 33.5213 - val_mse: 33.5213 - val_mae: 4.3303\n",
      "Epoch 172/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 33.3945 - mse: 33.3945 - mae: 4.3545 - val_loss: 33.4395 - val_mse: 33.4395 - val_mae: 4.3259\n",
      "Epoch 173/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 33.3807 - mse: 33.3807 - mae: 4.3521 - val_loss: 33.5359 - val_mse: 33.5359 - val_mae: 4.3934\n",
      "Epoch 174/1000\n",
      "325/325 [==============================] - 0s 946us/step - loss: 33.3310 - mse: 33.3310 - mae: 4.3535 - val_loss: 33.4550 - val_mse: 33.4550 - val_mae: 4.3268\n",
      "Epoch 175/1000\n",
      "325/325 [==============================] - 0s 936us/step - loss: 33.3286 - mse: 33.3286 - mae: 4.3491 - val_loss: 33.4777 - val_mse: 33.4777 - val_mae: 4.2878\n",
      "Epoch 176/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 33.3386 - mse: 33.3386 - mae: 4.3491 - val_loss: 33.7426 - val_mse: 33.7426 - val_mae: 4.3028\n",
      "Epoch 177/1000\n",
      "325/325 [==============================] - 0s 947us/step - loss: 33.3236 - mse: 33.3236 - mae: 4.3520 - val_loss: 33.4175 - val_mse: 33.4175 - val_mae: 4.3330\n",
      "Epoch 178/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 33.3280 - mse: 33.3280 - mae: 4.3477 - val_loss: 33.5054 - val_mse: 33.5054 - val_mae: 4.3715\n",
      "Epoch 179/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 33.3178 - mse: 33.3178 - mae: 4.3501 - val_loss: 33.4237 - val_mse: 33.4237 - val_mae: 4.3859\n",
      "Epoch 180/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 33.3355 - mse: 33.3355 - mae: 4.3490 - val_loss: 33.5648 - val_mse: 33.5648 - val_mae: 4.3422\n",
      "Epoch 181/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 33.3394 - mse: 33.3395 - mae: 4.3473 - val_loss: 33.4199 - val_mse: 33.4199 - val_mae: 4.3628\n",
      "Epoch 182/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 33.3097 - mse: 33.3097 - mae: 4.3496 - val_loss: 33.3961 - val_mse: 33.3961 - val_mae: 4.2760\n",
      "Epoch 183/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 33.3080 - mse: 33.3080 - mae: 4.3489 - val_loss: 33.3905 - val_mse: 33.3905 - val_mae: 4.3654\n",
      "Epoch 184/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 33.2840 - mse: 33.2840 - mae: 4.3431 - val_loss: 33.4763 - val_mse: 33.4763 - val_mae: 4.3852\n",
      "Epoch 185/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 33.2823 - mse: 33.2823 - mae: 4.3457 - val_loss: 33.4859 - val_mse: 33.4859 - val_mae: 4.3619\n",
      "Epoch 186/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 33.2327 - mse: 33.2327 - mae: 4.3457 - val_loss: 33.4745 - val_mse: 33.4745 - val_mae: 4.3547\n",
      "Epoch 187/1000\n",
      "325/325 [==============================] - 0s 941us/step - loss: 33.2849 - mse: 33.2849 - mae: 4.3464 - val_loss: 33.3975 - val_mse: 33.3975 - val_mae: 4.2586\n",
      "Epoch 188/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 33.2673 - mse: 33.2673 - mae: 4.3461 - val_loss: 33.3635 - val_mse: 33.3635 - val_mae: 4.2848\n",
      "Epoch 189/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 970us/step - loss: 33.2468 - mse: 33.2468 - mae: 4.3400 - val_loss: 33.3288 - val_mse: 33.3288 - val_mae: 4.3627\n",
      "Epoch 190/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 33.2200 - mse: 33.2200 - mae: 4.3453 - val_loss: 33.3502 - val_mse: 33.3502 - val_mae: 4.3038\n",
      "Epoch 191/1000\n",
      "325/325 [==============================] - 0s 951us/step - loss: 33.2258 - mse: 33.2258 - mae: 4.3362 - val_loss: 33.3802 - val_mse: 33.3802 - val_mae: 4.3421\n",
      "Epoch 192/1000\n",
      "325/325 [==============================] - 0s 944us/step - loss: 33.1928 - mse: 33.1928 - mae: 4.3392 - val_loss: 33.3372 - val_mse: 33.3372 - val_mae: 4.3108\n",
      "Epoch 193/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 33.1835 - mse: 33.1835 - mae: 4.3369 - val_loss: 33.3759 - val_mse: 33.3759 - val_mae: 4.2677\n",
      "Epoch 194/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 33.2290 - mse: 33.2290 - mae: 4.3383 - val_loss: 33.2962 - val_mse: 33.2962 - val_mae: 4.3606\n",
      "Epoch 195/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 33.1990 - mse: 33.1990 - mae: 4.3372 - val_loss: 33.2974 - val_mse: 33.2974 - val_mae: 4.3763\n",
      "Epoch 196/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 33.2261 - mse: 33.2261 - mae: 4.3438 - val_loss: 33.3421 - val_mse: 33.3421 - val_mae: 4.2522\n",
      "Epoch 197/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 33.2298 - mse: 33.2298 - mae: 4.3376 - val_loss: 33.4144 - val_mse: 33.4144 - val_mae: 4.4265\n",
      "Epoch 198/1000\n",
      "325/325 [==============================] - 0s 944us/step - loss: 33.2202 - mse: 33.2203 - mae: 4.3450 - val_loss: 33.2502 - val_mse: 33.2502 - val_mae: 4.3052\n",
      "Epoch 199/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 33.1925 - mse: 33.1925 - mae: 4.3346 - val_loss: 33.5247 - val_mse: 33.5247 - val_mae: 4.1889\n",
      "Epoch 200/1000\n",
      "325/325 [==============================] - 0s 943us/step - loss: 33.1638 - mse: 33.1638 - mae: 4.3338 - val_loss: 33.4513 - val_mse: 33.4513 - val_mae: 4.4184\n",
      "Epoch 201/1000\n",
      "325/325 [==============================] - 0s 933us/step - loss: 33.1539 - mse: 33.1539 - mae: 4.3331 - val_loss: 33.2740 - val_mse: 33.2740 - val_mae: 4.3318\n",
      "Epoch 202/1000\n",
      "325/325 [==============================] - 0s 941us/step - loss: 33.1190 - mse: 33.1190 - mae: 4.3325 - val_loss: 33.2790 - val_mse: 33.2790 - val_mae: 4.2567\n",
      "Epoch 203/1000\n",
      "325/325 [==============================] - 0s 941us/step - loss: 33.1387 - mse: 33.1387 - mae: 4.3338 - val_loss: 33.2944 - val_mse: 33.2944 - val_mae: 4.3427\n",
      "Epoch 204/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 33.1149 - mse: 33.1149 - mae: 4.3350 - val_loss: 33.2505 - val_mse: 33.2505 - val_mae: 4.2905\n",
      "Epoch 205/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 33.0971 - mse: 33.0971 - mae: 4.3292 - val_loss: 33.4267 - val_mse: 33.4267 - val_mae: 4.3677\n",
      "Epoch 206/1000\n",
      "325/325 [==============================] - 0s 995us/step - loss: 33.1113 - mse: 33.1113 - mae: 4.3330 - val_loss: 33.3773 - val_mse: 33.3773 - val_mae: 4.2736\n",
      "Epoch 207/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 33.1303 - mse: 33.1303 - mae: 4.3294 - val_loss: 33.2033 - val_mse: 33.2033 - val_mae: 4.3015\n",
      "Epoch 208/1000\n",
      "325/325 [==============================] - 0s 947us/step - loss: 33.1197 - mse: 33.1197 - mae: 4.3316 - val_loss: 33.2187 - val_mse: 33.2187 - val_mae: 4.3020\n",
      "Epoch 209/1000\n",
      "325/325 [==============================] - 0s 940us/step - loss: 33.1039 - mse: 33.1039 - mae: 4.3285 - val_loss: 33.4783 - val_mse: 33.4783 - val_mae: 4.4910\n",
      "Epoch 210/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 33.0847 - mse: 33.0847 - mae: 4.3343 - val_loss: 33.1641 - val_mse: 33.1641 - val_mae: 4.3111\n",
      "Epoch 211/1000\n",
      "325/325 [==============================] - 0s 951us/step - loss: 33.0929 - mse: 33.0929 - mae: 4.3316 - val_loss: 33.1805 - val_mse: 33.1805 - val_mae: 4.3325\n",
      "Epoch 212/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 33.0602 - mse: 33.0602 - mae: 4.3262 - val_loss: 33.3622 - val_mse: 33.3622 - val_mae: 4.2892\n",
      "Epoch 213/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 33.0593 - mse: 33.0593 - mae: 4.3280 - val_loss: 33.1885 - val_mse: 33.1885 - val_mae: 4.2684\n",
      "Epoch 214/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 33.1533 - mse: 33.1533 - mae: 4.3343 - val_loss: 33.2833 - val_mse: 33.2833 - val_mae: 4.3952\n",
      "Epoch 215/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 33.0507 - mse: 33.0507 - mae: 4.3257 - val_loss: 33.2669 - val_mse: 33.2669 - val_mae: 4.2453\n",
      "Epoch 216/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 33.0498 - mse: 33.0498 - mae: 4.3281 - val_loss: 33.2410 - val_mse: 33.2410 - val_mae: 4.3526\n",
      "Epoch 217/1000\n",
      "325/325 [==============================] - 0s 935us/step - loss: 33.0295 - mse: 33.0295 - mae: 4.3277 - val_loss: 33.4363 - val_mse: 33.4363 - val_mae: 4.2579\n",
      "Epoch 218/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 33.0306 - mse: 33.0306 - mae: 4.3237 - val_loss: 33.1785 - val_mse: 33.1785 - val_mae: 4.2842\n",
      "Epoch 219/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 33.0121 - mse: 33.0121 - mae: 4.3230 - val_loss: 33.1582 - val_mse: 33.1582 - val_mae: 4.2763\n",
      "Epoch 220/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 32.9597 - mse: 32.9597 - mae: 4.3225 - val_loss: 33.4205 - val_mse: 33.4205 - val_mae: 4.2574\n",
      "Epoch 221/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 33.0249 - mse: 33.0249 - mae: 4.3280 - val_loss: 33.6294 - val_mse: 33.6294 - val_mae: 4.4634\n",
      "Epoch 222/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 33.0211 - mse: 33.0211 - mae: 4.3192 - val_loss: 33.0981 - val_mse: 33.0981 - val_mae: 4.3482\n",
      "Epoch 223/1000\n",
      "325/325 [==============================] - 0s 933us/step - loss: 33.0186 - mse: 33.0186 - mae: 4.3236 - val_loss: 33.1333 - val_mse: 33.1334 - val_mae: 4.2991\n",
      "Epoch 224/1000\n",
      "325/325 [==============================] - 0s 933us/step - loss: 33.0070 - mse: 33.0070 - mae: 4.3209 - val_loss: 33.0708 - val_mse: 33.0708 - val_mae: 4.3256\n",
      "Epoch 225/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 32.9343 - mse: 32.9343 - mae: 4.3201 - val_loss: 33.1394 - val_mse: 33.1394 - val_mae: 4.3149\n",
      "Epoch 226/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 32.9358 - mse: 32.9358 - mae: 4.3205 - val_loss: 33.4051 - val_mse: 33.4051 - val_mae: 4.2030\n",
      "Epoch 227/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 32.9513 - mse: 32.9513 - mae: 4.3141 - val_loss: 33.1127 - val_mse: 33.1127 - val_mae: 4.2669\n",
      "Epoch 228/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 32.9788 - mse: 32.9788 - mae: 4.3219 - val_loss: 33.2234 - val_mse: 33.2234 - val_mae: 4.4357\n",
      "Epoch 229/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 32.9291 - mse: 32.9291 - mae: 4.3150 - val_loss: 33.2165 - val_mse: 33.2165 - val_mae: 4.3877\n",
      "Epoch 230/1000\n",
      "325/325 [==============================] - 0s 941us/step - loss: 32.9097 - mse: 32.9097 - mae: 4.3150 - val_loss: 33.1009 - val_mse: 33.1009 - val_mae: 4.3946\n",
      "Epoch 231/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 32.9077 - mse: 32.9077 - mae: 4.3137 - val_loss: 33.0751 - val_mse: 33.0751 - val_mae: 4.2685\n",
      "Epoch 232/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 32.9166 - mse: 32.9166 - mae: 4.3141 - val_loss: 33.0940 - val_mse: 33.0940 - val_mae: 4.3313\n",
      "Epoch 233/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 32.8695 - mse: 32.8695 - mae: 4.3139 - val_loss: 33.0149 - val_mse: 33.0149 - val_mae: 4.3427\n",
      "Epoch 234/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 32.9296 - mse: 32.9296 - mae: 4.3180 - val_loss: 33.1723 - val_mse: 33.1723 - val_mae: 4.3432\n",
      "Epoch 235/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 32.8687 - mse: 32.8687 - mae: 4.3145 - val_loss: 33.0521 - val_mse: 33.0521 - val_mae: 4.3698\n",
      "Epoch 236/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 996us/step - loss: 32.8726 - mse: 32.8726 - mae: 4.3124 - val_loss: 33.0679 - val_mse: 33.0679 - val_mae: 4.3185\n",
      "Epoch 237/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 32.8837 - mse: 32.8837 - mae: 4.3123 - val_loss: 33.0503 - val_mse: 33.0503 - val_mae: 4.2906\n",
      "Epoch 238/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 32.8641 - mse: 32.8641 - mae: 4.3127 - val_loss: 32.9506 - val_mse: 32.9506 - val_mae: 4.2660\n",
      "Epoch 239/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 32.8079 - mse: 32.8079 - mae: 4.3040 - val_loss: 33.0145 - val_mse: 33.0145 - val_mae: 4.4011\n",
      "Epoch 240/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 32.8412 - mse: 32.8412 - mae: 4.3110 - val_loss: 33.1072 - val_mse: 33.1072 - val_mae: 4.4330\n",
      "Epoch 241/1000\n",
      "325/325 [==============================] - 0s 942us/step - loss: 32.8207 - mse: 32.8207 - mae: 4.3067 - val_loss: 32.9165 - val_mse: 32.9165 - val_mae: 4.2752\n",
      "Epoch 242/1000\n",
      "325/325 [==============================] - 0s 946us/step - loss: 32.8341 - mse: 32.8341 - mae: 4.3074 - val_loss: 33.1588 - val_mse: 33.1588 - val_mae: 4.4507\n",
      "Epoch 243/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 32.8209 - mse: 32.8209 - mae: 4.3042 - val_loss: 33.0070 - val_mse: 33.0070 - val_mae: 4.2763\n",
      "Epoch 244/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 32.8096 - mse: 32.8096 - mae: 4.3083 - val_loss: 32.9421 - val_mse: 32.9421 - val_mae: 4.2670\n",
      "Epoch 245/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 32.7407 - mse: 32.7407 - mae: 4.2982 - val_loss: 33.1808 - val_mse: 33.1808 - val_mae: 4.4036\n",
      "Epoch 246/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 32.7535 - mse: 32.7535 - mae: 4.3020 - val_loss: 32.8802 - val_mse: 32.8802 - val_mae: 4.3361\n",
      "Epoch 247/1000\n",
      "325/325 [==============================] - 0s 928us/step - loss: 32.7539 - mse: 32.7539 - mae: 4.3036 - val_loss: 32.9579 - val_mse: 32.9579 - val_mae: 4.2145\n",
      "Epoch 248/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 32.7490 - mse: 32.7490 - mae: 4.2983 - val_loss: 32.8869 - val_mse: 32.8869 - val_mae: 4.2860\n",
      "Epoch 249/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 32.7539 - mse: 32.7539 - mae: 4.3002 - val_loss: 33.0041 - val_mse: 33.0041 - val_mae: 4.2554\n",
      "Epoch 250/1000\n",
      "325/325 [==============================] - 0s 947us/step - loss: 32.7172 - mse: 32.7172 - mae: 4.3035 - val_loss: 32.9182 - val_mse: 32.9182 - val_mae: 4.3284\n",
      "Epoch 251/1000\n",
      "325/325 [==============================] - 0s 951us/step - loss: 32.7759 - mse: 32.7759 - mae: 4.3029 - val_loss: 32.8524 - val_mse: 32.8524 - val_mae: 4.3201\n",
      "Epoch 252/1000\n",
      "325/325 [==============================] - 0s 938us/step - loss: 32.7259 - mse: 32.7259 - mae: 4.2982 - val_loss: 33.1905 - val_mse: 33.1905 - val_mae: 4.1884\n",
      "Epoch 253/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 32.6688 - mse: 32.6688 - mae: 4.2946 - val_loss: 32.8114 - val_mse: 32.8114 - val_mae: 4.3254\n",
      "Epoch 254/1000\n",
      "325/325 [==============================] - 0s 941us/step - loss: 32.6755 - mse: 32.6755 - mae: 4.2964 - val_loss: 33.1703 - val_mse: 33.1703 - val_mae: 4.2189\n",
      "Epoch 255/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 32.7055 - mse: 32.7055 - mae: 4.2989 - val_loss: 33.0193 - val_mse: 33.0193 - val_mae: 4.1722\n",
      "Epoch 256/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 32.6524 - mse: 32.6524 - mae: 4.2878 - val_loss: 32.8574 - val_mse: 32.8574 - val_mae: 4.2980\n",
      "Epoch 257/1000\n",
      "325/325 [==============================] - 0s 950us/step - loss: 32.6354 - mse: 32.6354 - mae: 4.2910 - val_loss: 32.7786 - val_mse: 32.7786 - val_mae: 4.2503\n",
      "Epoch 258/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 32.6132 - mse: 32.6132 - mae: 4.2852 - val_loss: 32.7284 - val_mse: 32.7284 - val_mae: 4.2872\n",
      "Epoch 259/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 32.5876 - mse: 32.5876 - mae: 4.2868 - val_loss: 32.7405 - val_mse: 32.7405 - val_mae: 4.3235\n",
      "Epoch 260/1000\n",
      "325/325 [==============================] - 0s 944us/step - loss: 32.5347 - mse: 32.5347 - mae: 4.2828 - val_loss: 32.7104 - val_mse: 32.7104 - val_mae: 4.2398\n",
      "Epoch 261/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 32.5624 - mse: 32.5624 - mae: 4.2814 - val_loss: 32.6783 - val_mse: 32.6783 - val_mae: 4.2314\n",
      "Epoch 262/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 32.5508 - mse: 32.5508 - mae: 4.2836 - val_loss: 32.8467 - val_mse: 32.8467 - val_mae: 4.3594\n",
      "Epoch 263/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 32.5562 - mse: 32.5562 - mae: 4.2816 - val_loss: 32.7681 - val_mse: 32.7681 - val_mae: 4.3092\n",
      "Epoch 264/1000\n",
      "325/325 [==============================] - 0s 946us/step - loss: 32.4995 - mse: 32.4995 - mae: 4.2789 - val_loss: 32.6952 - val_mse: 32.6952 - val_mae: 4.2677\n",
      "Epoch 265/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 32.5094 - mse: 32.5094 - mae: 4.2787 - val_loss: 32.8398 - val_mse: 32.8398 - val_mae: 4.3260\n",
      "Epoch 266/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 32.5019 - mse: 32.5019 - mae: 4.2773 - val_loss: 32.8682 - val_mse: 32.8682 - val_mae: 4.1683\n",
      "Epoch 267/1000\n",
      "325/325 [==============================] - 0s 942us/step - loss: 32.4687 - mse: 32.4687 - mae: 4.2744 - val_loss: 32.8235 - val_mse: 32.8235 - val_mae: 4.1654\n",
      "Epoch 268/1000\n",
      "325/325 [==============================] - 0s 943us/step - loss: 32.4749 - mse: 32.4749 - mae: 4.2762 - val_loss: 32.6399 - val_mse: 32.6399 - val_mae: 4.2345\n",
      "Epoch 269/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 32.4672 - mse: 32.4672 - mae: 4.2801 - val_loss: 32.6525 - val_mse: 32.6525 - val_mae: 4.1953\n",
      "Epoch 270/1000\n",
      "325/325 [==============================] - 0s 942us/step - loss: 32.5096 - mse: 32.5096 - mae: 4.2766 - val_loss: 32.6751 - val_mse: 32.6751 - val_mae: 4.2005\n",
      "Epoch 271/1000\n",
      "325/325 [==============================] - 0s 943us/step - loss: 32.4126 - mse: 32.4126 - mae: 4.2770 - val_loss: 32.6036 - val_mse: 32.6036 - val_mae: 4.2271\n",
      "Epoch 272/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 32.4107 - mse: 32.4107 - mae: 4.2639 - val_loss: 32.6815 - val_mse: 32.6815 - val_mae: 4.1866\n",
      "Epoch 273/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 32.4075 - mse: 32.4075 - mae: 4.2730 - val_loss: 32.7866 - val_mse: 32.7866 - val_mae: 4.1723\n",
      "Epoch 274/1000\n",
      "325/325 [==============================] - 0s 951us/step - loss: 32.4023 - mse: 32.4023 - mae: 4.2657 - val_loss: 32.6777 - val_mse: 32.6777 - val_mae: 4.1759\n",
      "Epoch 275/1000\n",
      "325/325 [==============================] - 0s 946us/step - loss: 32.3999 - mse: 32.3999 - mae: 4.2678 - val_loss: 32.7243 - val_mse: 32.7243 - val_mae: 4.3837\n",
      "Epoch 276/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 32.3493 - mse: 32.3493 - mae: 4.2653 - val_loss: 32.5125 - val_mse: 32.5125 - val_mae: 4.2893\n",
      "Epoch 277/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 32.3421 - mse: 32.3421 - mae: 4.2658 - val_loss: 33.3533 - val_mse: 33.3533 - val_mae: 4.4864\n",
      "Epoch 278/1000\n",
      "325/325 [==============================] - 0s 954us/step - loss: 32.4132 - mse: 32.4132 - mae: 4.2707 - val_loss: 33.0207 - val_mse: 33.0207 - val_mae: 4.4478\n",
      "Epoch 279/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 32.3827 - mse: 32.3827 - mae: 4.2687 - val_loss: 32.5434 - val_mse: 32.5434 - val_mae: 4.2943\n",
      "Epoch 280/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 32.3372 - mse: 32.3372 - mae: 4.2688 - val_loss: 32.5931 - val_mse: 32.5931 - val_mae: 4.3007\n",
      "Epoch 281/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 32.3428 - mse: 32.3428 - mae: 4.2624 - val_loss: 32.5253 - val_mse: 32.5253 - val_mae: 4.2465\n",
      "Epoch 282/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 32.2800 - mse: 32.2800 - mae: 4.2594 - val_loss: 33.1840 - val_mse: 33.1840 - val_mae: 4.5236\n",
      "Epoch 283/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 963us/step - loss: 32.2917 - mse: 32.2917 - mae: 4.2599 - val_loss: 32.5186 - val_mse: 32.5186 - val_mae: 4.2369\n",
      "Epoch 284/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 32.3334 - mse: 32.3334 - mae: 4.2653 - val_loss: 32.5101 - val_mse: 32.5101 - val_mae: 4.2159\n",
      "Epoch 285/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 32.2621 - mse: 32.2621 - mae: 4.2574 - val_loss: 32.4374 - val_mse: 32.4374 - val_mae: 4.3010\n",
      "Epoch 286/1000\n",
      "325/325 [==============================] - 0s 999us/step - loss: 32.2677 - mse: 32.2677 - mae: 4.2586 - val_loss: 32.4997 - val_mse: 32.4997 - val_mae: 4.1929\n",
      "Epoch 287/1000\n",
      "325/325 [==============================] - 0s 999us/step - loss: 32.2672 - mse: 32.2672 - mae: 4.2550 - val_loss: 32.5095 - val_mse: 32.5095 - val_mae: 4.2439\n",
      "Epoch 288/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 32.2737 - mse: 32.2737 - mae: 4.2557 - val_loss: 32.9985 - val_mse: 32.9985 - val_mae: 4.4536\n",
      "Epoch 289/1000\n",
      "325/325 [==============================] - 0s 995us/step - loss: 32.2991 - mse: 32.2991 - mae: 4.2601 - val_loss: 32.4156 - val_mse: 32.4156 - val_mae: 4.2316\n",
      "Epoch 290/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 32.2061 - mse: 32.2061 - mae: 4.2516 - val_loss: 32.4168 - val_mse: 32.4168 - val_mae: 4.2551\n",
      "Epoch 291/1000\n",
      "325/325 [==============================] - 0s 983us/step - loss: 32.2228 - mse: 32.2228 - mae: 4.2550 - val_loss: 32.5538 - val_mse: 32.5538 - val_mae: 4.3474\n",
      "Epoch 292/1000\n",
      "325/325 [==============================] - 0s 995us/step - loss: 32.2176 - mse: 32.2176 - mae: 4.2535 - val_loss: 32.4954 - val_mse: 32.4954 - val_mae: 4.2521\n",
      "Epoch 293/1000\n",
      "325/325 [==============================] - 0s 993us/step - loss: 32.2090 - mse: 32.2090 - mae: 4.2489 - val_loss: 32.6304 - val_mse: 32.6304 - val_mae: 4.3937\n",
      "Epoch 294/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 32.1829 - mse: 32.1829 - mae: 4.2493 - val_loss: 32.4123 - val_mse: 32.4123 - val_mae: 4.2625\n",
      "Epoch 295/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 32.2041 - mse: 32.2041 - mae: 4.2515 - val_loss: 32.3656 - val_mse: 32.3656 - val_mae: 4.2714\n",
      "Epoch 296/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 32.2177 - mse: 32.2177 - mae: 4.2509 - val_loss: 32.5949 - val_mse: 32.5949 - val_mae: 4.1560\n",
      "Epoch 297/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 32.1749 - mse: 32.1749 - mae: 4.2473 - val_loss: 32.3639 - val_mse: 32.3639 - val_mae: 4.2792\n",
      "Epoch 298/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 32.1258 - mse: 32.1258 - mae: 4.2469 - val_loss: 32.5702 - val_mse: 32.5702 - val_mae: 4.3105\n",
      "Epoch 299/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 32.1429 - mse: 32.1429 - mae: 4.2480 - val_loss: 32.6634 - val_mse: 32.6634 - val_mae: 4.1068\n",
      "Epoch 300/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 32.1275 - mse: 32.1275 - mae: 4.2396 - val_loss: 32.4765 - val_mse: 32.4765 - val_mae: 4.2138\n",
      "Epoch 301/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 32.0950 - mse: 32.0950 - mae: 4.2460 - val_loss: 32.6054 - val_mse: 32.6054 - val_mae: 4.3023\n",
      "Epoch 302/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 32.1498 - mse: 32.1498 - mae: 4.2402 - val_loss: 32.3751 - val_mse: 32.3751 - val_mae: 4.2977\n",
      "Epoch 303/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 32.1158 - mse: 32.1158 - mae: 4.2452 - val_loss: 32.3942 - val_mse: 32.3942 - val_mae: 4.2626\n",
      "Epoch 304/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 32.1190 - mse: 32.1190 - mae: 4.2435 - val_loss: 32.3751 - val_mse: 32.3751 - val_mae: 4.3083\n",
      "Epoch 305/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 32.1175 - mse: 32.1175 - mae: 4.2389 - val_loss: 32.2781 - val_mse: 32.2781 - val_mae: 4.2506\n",
      "Epoch 306/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 32.0506 - mse: 32.0506 - mae: 4.2405 - val_loss: 32.3605 - val_mse: 32.3605 - val_mae: 4.2896\n",
      "Epoch 307/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 32.0539 - mse: 32.0539 - mae: 4.2368 - val_loss: 32.7060 - val_mse: 32.7060 - val_mae: 4.1342\n",
      "Epoch 308/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 32.0848 - mse: 32.0848 - mae: 4.2364 - val_loss: 32.5924 - val_mse: 32.5924 - val_mae: 4.3107\n",
      "Epoch 309/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 32.0533 - mse: 32.0533 - mae: 4.2380 - val_loss: 32.3318 - val_mse: 32.3318 - val_mae: 4.2799\n",
      "Epoch 310/1000\n",
      "325/325 [==============================] - 0s 999us/step - loss: 32.0918 - mse: 32.0918 - mae: 4.2365 - val_loss: 32.6182 - val_mse: 32.6182 - val_mae: 4.4273\n",
      "Epoch 311/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 32.0798 - mse: 32.0798 - mae: 4.2382 - val_loss: 32.3399 - val_mse: 32.3399 - val_mae: 4.2673\n",
      "Epoch 312/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 32.0006 - mse: 32.0006 - mae: 4.2328 - val_loss: 32.3034 - val_mse: 32.3034 - val_mae: 4.2999\n",
      "Epoch 313/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 32.0051 - mse: 32.0051 - mae: 4.2322 - val_loss: 32.2595 - val_mse: 32.2595 - val_mae: 4.2832\n",
      "Epoch 314/1000\n",
      "325/325 [==============================] - 0s 991us/step - loss: 32.0426 - mse: 32.0426 - mae: 4.2364 - val_loss: 32.3173 - val_mse: 32.3173 - val_mae: 4.3000\n",
      "Epoch 315/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 32.0274 - mse: 32.0274 - mae: 4.2351 - val_loss: 32.2303 - val_mse: 32.2303 - val_mae: 4.2296\n",
      "Epoch 316/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.9978 - mse: 31.9978 - mae: 4.2311 - val_loss: 32.2556 - val_mse: 32.2556 - val_mae: 4.1949\n",
      "Epoch 317/1000\n",
      "325/325 [==============================] - 0s 987us/step - loss: 32.0190 - mse: 32.0190 - mae: 4.2331 - val_loss: 32.3162 - val_mse: 32.3162 - val_mae: 4.2091\n",
      "Epoch 318/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.9959 - mse: 31.9959 - mae: 4.2318 - val_loss: 32.5019 - val_mse: 32.5019 - val_mae: 4.3758\n",
      "Epoch 319/1000\n",
      "325/325 [==============================] - 0s 980us/step - loss: 32.0164 - mse: 32.0164 - mae: 4.2318 - val_loss: 32.4186 - val_mse: 32.4186 - val_mae: 4.3424\n",
      "Epoch 320/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.9952 - mse: 31.9952 - mae: 4.2313 - val_loss: 32.1795 - val_mse: 32.1795 - val_mae: 4.2172\n",
      "Epoch 321/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 32.0001 - mse: 32.0001 - mae: 4.2351 - val_loss: 33.0736 - val_mse: 33.0736 - val_mae: 4.0798\n",
      "Epoch 322/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.9554 - mse: 31.9554 - mae: 4.2280 - val_loss: 32.3172 - val_mse: 32.3172 - val_mae: 4.1497\n",
      "Epoch 323/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.9370 - mse: 31.9370 - mae: 4.2221 - val_loss: 32.4725 - val_mse: 32.4725 - val_mae: 4.3872\n",
      "Epoch 324/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.9354 - mse: 31.9354 - mae: 4.2287 - val_loss: 32.2310 - val_mse: 32.2310 - val_mae: 4.2090\n",
      "Epoch 325/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.9934 - mse: 31.9934 - mae: 4.2326 - val_loss: 32.2721 - val_mse: 32.2721 - val_mae: 4.2103\n",
      "Epoch 326/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 31.9532 - mse: 31.9532 - mae: 4.2225 - val_loss: 32.3975 - val_mse: 32.3975 - val_mae: 4.1406\n",
      "Epoch 327/1000\n",
      "325/325 [==============================] - 0s 998us/step - loss: 31.8967 - mse: 31.8967 - mae: 4.2220 - val_loss: 32.1592 - val_mse: 32.1592 - val_mae: 4.2605\n",
      "Epoch 328/1000\n",
      "325/325 [==============================] - 0s 987us/step - loss: 31.9047 - mse: 31.9047 - mae: 4.2243 - val_loss: 32.4572 - val_mse: 32.4572 - val_mae: 4.3434\n",
      "Epoch 329/1000\n",
      "325/325 [==============================] - 0s 989us/step - loss: 31.9222 - mse: 31.9222 - mae: 4.2245 - val_loss: 32.2662 - val_mse: 32.2662 - val_mae: 4.1552\n",
      "Epoch 330/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 987us/step - loss: 31.9432 - mse: 31.9432 - mae: 4.2238 - val_loss: 32.2461 - val_mse: 32.2461 - val_mae: 4.1522\n",
      "Epoch 331/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.9085 - mse: 31.9085 - mae: 4.2230 - val_loss: 32.2273 - val_mse: 32.2273 - val_mae: 4.1461\n",
      "Epoch 332/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.8515 - mse: 31.8515 - mae: 4.2152 - val_loss: 32.1426 - val_mse: 32.1426 - val_mae: 4.2200\n",
      "Epoch 333/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.8728 - mse: 31.8728 - mae: 4.2231 - val_loss: 32.3951 - val_mse: 32.3951 - val_mae: 4.1207\n",
      "Epoch 334/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.9326 - mse: 31.9326 - mae: 4.2230 - val_loss: 32.2618 - val_mse: 32.2618 - val_mae: 4.1682\n",
      "Epoch 335/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.8573 - mse: 31.8573 - mae: 4.2202 - val_loss: 32.1089 - val_mse: 32.1089 - val_mae: 4.1802\n",
      "Epoch 336/1000\n",
      "325/325 [==============================] - 0s 985us/step - loss: 31.8507 - mse: 31.8507 - mae: 4.2180 - val_loss: 32.1431 - val_mse: 32.1431 - val_mae: 4.2563\n",
      "Epoch 337/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.7939 - mse: 31.7940 - mae: 4.2143 - val_loss: 32.0832 - val_mse: 32.0832 - val_mae: 4.2060\n",
      "Epoch 338/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.8471 - mse: 31.8471 - mae: 4.2173 - val_loss: 32.2547 - val_mse: 32.2547 - val_mae: 4.3075\n",
      "Epoch 339/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.7903 - mse: 31.7903 - mae: 4.2161 - val_loss: 32.1679 - val_mse: 32.1679 - val_mae: 4.1409\n",
      "Epoch 340/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.8381 - mse: 31.8381 - mae: 4.2169 - val_loss: 32.0740 - val_mse: 32.0740 - val_mae: 4.1673\n",
      "Epoch 341/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.7801 - mse: 31.7801 - mae: 4.2131 - val_loss: 32.0547 - val_mse: 32.0547 - val_mae: 4.2420\n",
      "Epoch 342/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.8130 - mse: 31.8130 - mae: 4.2187 - val_loss: 32.0629 - val_mse: 32.0629 - val_mae: 4.1838\n",
      "Epoch 343/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.7477 - mse: 31.7477 - mae: 4.2087 - val_loss: 32.1486 - val_mse: 32.1486 - val_mae: 4.2677\n",
      "Epoch 344/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.8020 - mse: 31.8020 - mae: 4.2098 - val_loss: 32.1442 - val_mse: 32.1442 - val_mae: 4.2707\n",
      "Epoch 345/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 31.7824 - mse: 31.7824 - mae: 4.2115 - val_loss: 32.1113 - val_mse: 32.1113 - val_mae: 4.1854\n",
      "Epoch 346/1000\n",
      "325/325 [==============================] - 0s 999us/step - loss: 31.7534 - mse: 31.7534 - mae: 4.2119 - val_loss: 32.1490 - val_mse: 32.1490 - val_mae: 4.1412\n",
      "Epoch 347/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.7624 - mse: 31.7624 - mae: 4.2082 - val_loss: 32.0370 - val_mse: 32.0370 - val_mae: 4.2107\n",
      "Epoch 348/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.7000 - mse: 31.7000 - mae: 4.2032 - val_loss: 32.1188 - val_mse: 32.1188 - val_mae: 4.2501\n",
      "Epoch 349/1000\n",
      "325/325 [==============================] - 0s 989us/step - loss: 31.8659 - mse: 31.8659 - mae: 4.2168 - val_loss: 32.0808 - val_mse: 32.0808 - val_mae: 4.1621\n",
      "Epoch 350/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.7275 - mse: 31.7275 - mae: 4.2076 - val_loss: 32.2085 - val_mse: 32.2085 - val_mae: 4.1570\n",
      "Epoch 351/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.7697 - mse: 31.7697 - mae: 4.2142 - val_loss: 32.0935 - val_mse: 32.0935 - val_mae: 4.2823\n",
      "Epoch 352/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.7681 - mse: 31.7681 - mae: 4.2088 - val_loss: 32.2422 - val_mse: 32.2422 - val_mae: 4.2521\n",
      "Epoch 353/1000\n",
      "325/325 [==============================] - 0s 994us/step - loss: 31.7316 - mse: 31.7316 - mae: 4.2078 - val_loss: 32.0902 - val_mse: 32.0902 - val_mae: 4.1716\n",
      "Epoch 354/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.7552 - mse: 31.7552 - mae: 4.2062 - val_loss: 32.1485 - val_mse: 32.1485 - val_mae: 4.1329\n",
      "Epoch 355/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.7589 - mse: 31.7590 - mae: 4.2087 - val_loss: 32.5571 - val_mse: 32.5572 - val_mae: 4.3886\n",
      "Epoch 356/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.7434 - mse: 31.7434 - mae: 4.2075 - val_loss: 31.9830 - val_mse: 31.9830 - val_mae: 4.2824\n",
      "Epoch 357/1000\n",
      "325/325 [==============================] - 0s 982us/step - loss: 31.6874 - mse: 31.6874 - mae: 4.2052 - val_loss: 32.0100 - val_mse: 32.0100 - val_mae: 4.2596\n",
      "Epoch 358/1000\n",
      "325/325 [==============================] - 0s 980us/step - loss: 31.6637 - mse: 31.6637 - mae: 4.2004 - val_loss: 32.1314 - val_mse: 32.1314 - val_mae: 4.1613\n",
      "Epoch 359/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.7415 - mse: 31.7415 - mae: 4.2122 - val_loss: 32.2391 - val_mse: 32.2391 - val_mae: 4.1337\n",
      "Epoch 360/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.7104 - mse: 31.7104 - mae: 4.2036 - val_loss: 32.0003 - val_mse: 32.0003 - val_mae: 4.2306\n",
      "Epoch 361/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 31.7106 - mse: 31.7106 - mae: 4.2045 - val_loss: 32.0953 - val_mse: 32.0953 - val_mae: 4.3036\n",
      "Epoch 362/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.6922 - mse: 31.6922 - mae: 4.2059 - val_loss: 32.1061 - val_mse: 32.1061 - val_mae: 4.1251\n",
      "Epoch 363/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.6721 - mse: 31.6721 - mae: 4.2023 - val_loss: 32.0121 - val_mse: 32.0121 - val_mae: 4.2266\n",
      "Epoch 364/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.6884 - mse: 31.6884 - mae: 4.1985 - val_loss: 31.9686 - val_mse: 31.9686 - val_mae: 4.2074\n",
      "Epoch 365/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.6672 - mse: 31.6672 - mae: 4.2073 - val_loss: 31.9502 - val_mse: 31.9502 - val_mae: 4.1750\n",
      "Epoch 366/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.6976 - mse: 31.6976 - mae: 4.2004 - val_loss: 32.1896 - val_mse: 32.1896 - val_mae: 4.3093\n",
      "Epoch 367/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.7441 - mse: 31.7441 - mae: 4.2056 - val_loss: 32.0450 - val_mse: 32.0450 - val_mae: 4.2969\n",
      "Epoch 368/1000\n",
      "325/325 [==============================] - 0s 980us/step - loss: 31.6493 - mse: 31.6493 - mae: 4.2005 - val_loss: 31.9762 - val_mse: 31.9762 - val_mae: 4.2964\n",
      "Epoch 369/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.7199 - mse: 31.7199 - mae: 4.2043 - val_loss: 32.3321 - val_mse: 32.3321 - val_mae: 4.3693\n",
      "Epoch 370/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.6955 - mse: 31.6955 - mae: 4.2049 - val_loss: 31.9977 - val_mse: 31.9977 - val_mae: 4.2245\n",
      "Epoch 371/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.6706 - mse: 31.6706 - mae: 4.2045 - val_loss: 32.1203 - val_mse: 32.1203 - val_mae: 4.1403\n",
      "Epoch 372/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.6802 - mse: 31.6802 - mae: 4.2025 - val_loss: 32.1434 - val_mse: 32.1434 - val_mae: 4.1019\n",
      "Epoch 373/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 31.6236 - mse: 31.6236 - mae: 4.2000 - val_loss: 31.8907 - val_mse: 31.8907 - val_mae: 4.2136\n",
      "Epoch 374/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.6089 - mse: 31.6089 - mae: 4.2013 - val_loss: 31.9234 - val_mse: 31.9234 - val_mae: 4.1630\n",
      "Epoch 375/1000\n",
      "325/325 [==============================] - 0s 942us/step - loss: 31.6458 - mse: 31.6458 - mae: 4.1992 - val_loss: 32.1599 - val_mse: 32.1599 - val_mae: 4.3213\n",
      "Epoch 376/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.6213 - mse: 31.6213 - mae: 4.1974 - val_loss: 32.0377 - val_mse: 32.0377 - val_mae: 4.1573\n",
      "Epoch 377/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 976us/step - loss: 31.7226 - mse: 31.7226 - mae: 4.1994 - val_loss: 32.1248 - val_mse: 32.1248 - val_mae: 4.2734\n",
      "Epoch 378/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.6640 - mse: 31.6640 - mae: 4.2007 - val_loss: 32.0696 - val_mse: 32.0696 - val_mae: 4.3299\n",
      "Epoch 379/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.6184 - mse: 31.6184 - mae: 4.2013 - val_loss: 31.9863 - val_mse: 31.9863 - val_mae: 4.1783\n",
      "Epoch 380/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.6309 - mse: 31.6309 - mae: 4.2000 - val_loss: 32.3888 - val_mse: 32.3888 - val_mae: 4.0877\n",
      "Epoch 381/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.6076 - mse: 31.6076 - mae: 4.1932 - val_loss: 32.0718 - val_mse: 32.0718 - val_mae: 4.2536\n",
      "Epoch 382/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.5970 - mse: 31.5970 - mae: 4.1987 - val_loss: 31.9910 - val_mse: 31.9910 - val_mae: 4.2025\n",
      "Epoch 383/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 31.6442 - mse: 31.6442 - mae: 4.2002 - val_loss: 32.0151 - val_mse: 32.0151 - val_mae: 4.1355\n",
      "Epoch 384/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.6179 - mse: 31.6179 - mae: 4.1958 - val_loss: 32.1531 - val_mse: 32.1531 - val_mae: 4.1339\n",
      "Epoch 385/1000\n",
      "325/325 [==============================] - 0s 980us/step - loss: 31.5714 - mse: 31.5714 - mae: 4.1922 - val_loss: 31.9194 - val_mse: 31.9194 - val_mae: 4.2544\n",
      "Epoch 386/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.5617 - mse: 31.5617 - mae: 4.1959 - val_loss: 32.6580 - val_mse: 32.6580 - val_mae: 4.0536\n",
      "Epoch 387/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.6427 - mse: 31.6427 - mae: 4.1974 - val_loss: 32.0173 - val_mse: 32.0173 - val_mae: 4.1615\n",
      "Epoch 388/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.6117 - mse: 31.6117 - mae: 4.1922 - val_loss: 31.8416 - val_mse: 31.8416 - val_mae: 4.2254\n",
      "Epoch 389/1000\n",
      "325/325 [==============================] - 0s 993us/step - loss: 31.5471 - mse: 31.5471 - mae: 4.1917 - val_loss: 32.1550 - val_mse: 32.1550 - val_mae: 4.1062\n",
      "Epoch 390/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.6124 - mse: 31.6124 - mae: 4.2014 - val_loss: 32.2819 - val_mse: 32.2819 - val_mae: 4.3370\n",
      "Epoch 391/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.6372 - mse: 31.6372 - mae: 4.1939 - val_loss: 31.9239 - val_mse: 31.9239 - val_mae: 4.2839\n",
      "Epoch 392/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 31.6059 - mse: 31.6059 - mae: 4.1946 - val_loss: 31.9492 - val_mse: 31.9492 - val_mae: 4.1518\n",
      "Epoch 393/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.5882 - mse: 31.5882 - mae: 4.1945 - val_loss: 32.1369 - val_mse: 32.1369 - val_mae: 4.2612\n",
      "Epoch 394/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.5895 - mse: 31.5895 - mae: 4.1929 - val_loss: 31.9411 - val_mse: 31.9411 - val_mae: 4.2990\n",
      "Epoch 395/1000\n",
      "325/325 [==============================] - 0s 982us/step - loss: 31.5709 - mse: 31.5709 - mae: 4.1911 - val_loss: 31.8386 - val_mse: 31.8386 - val_mae: 4.1734\n",
      "Epoch 396/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.5838 - mse: 31.5838 - mae: 4.1955 - val_loss: 31.9157 - val_mse: 31.9157 - val_mae: 4.1943\n",
      "Epoch 397/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5162 - mse: 31.5162 - mae: 4.1886 - val_loss: 31.9183 - val_mse: 31.9183 - val_mae: 4.1661\n",
      "Epoch 398/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.6058 - mse: 31.6058 - mae: 4.1940 - val_loss: 32.3935 - val_mse: 32.3935 - val_mae: 4.3863\n",
      "Epoch 399/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.5498 - mse: 31.5498 - mae: 4.1906 - val_loss: 31.8878 - val_mse: 31.8878 - val_mae: 4.1357\n",
      "Epoch 400/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.5933 - mse: 31.5933 - mae: 4.1947 - val_loss: 31.9284 - val_mse: 31.9284 - val_mae: 4.3079\n",
      "Epoch 401/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 31.5553 - mse: 31.5553 - mae: 4.1924 - val_loss: 32.1954 - val_mse: 32.1954 - val_mae: 4.3280\n",
      "Epoch 402/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.6080 - mse: 31.6080 - mae: 4.1939 - val_loss: 31.8615 - val_mse: 31.8615 - val_mae: 4.2082\n",
      "Epoch 403/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.5358 - mse: 31.5358 - mae: 4.1898 - val_loss: 31.8882 - val_mse: 31.8882 - val_mae: 4.1942\n",
      "Epoch 404/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.5397 - mse: 31.5397 - mae: 4.1927 - val_loss: 32.1880 - val_mse: 32.1880 - val_mae: 4.0988\n",
      "Epoch 405/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.5377 - mse: 31.5377 - mae: 4.1896 - val_loss: 31.8994 - val_mse: 31.8994 - val_mae: 4.2079\n",
      "Epoch 406/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 31.5848 - mse: 31.5848 - mae: 4.1914 - val_loss: 31.9311 - val_mse: 31.9311 - val_mae: 4.1930\n",
      "Epoch 407/1000\n",
      "325/325 [==============================] - 0s 987us/step - loss: 31.5032 - mse: 31.5032 - mae: 4.1893 - val_loss: 31.9336 - val_mse: 31.9336 - val_mae: 4.2267\n",
      "Epoch 408/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.5378 - mse: 31.5378 - mae: 4.1880 - val_loss: 32.0401 - val_mse: 32.0401 - val_mae: 4.1399\n",
      "Epoch 409/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.5967 - mse: 31.5967 - mae: 4.1949 - val_loss: 31.9643 - val_mse: 31.9643 - val_mae: 4.2874\n",
      "Epoch 410/1000\n",
      "325/325 [==============================] - 0s 992us/step - loss: 31.5551 - mse: 31.5551 - mae: 4.1880 - val_loss: 31.9825 - val_mse: 31.9825 - val_mae: 4.1731\n",
      "Epoch 411/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.6286 - mse: 31.6286 - mae: 4.1961 - val_loss: 31.9448 - val_mse: 31.9448 - val_mae: 4.1250\n",
      "Epoch 412/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.6001 - mse: 31.6001 - mae: 4.1962 - val_loss: 32.3543 - val_mse: 32.3543 - val_mae: 4.3290\n",
      "Epoch 413/1000\n",
      "325/325 [==============================] - 0s 980us/step - loss: 31.5565 - mse: 31.5565 - mae: 4.1909 - val_loss: 32.1820 - val_mse: 32.1820 - val_mae: 4.2425\n",
      "Epoch 414/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 31.5467 - mse: 31.5467 - mae: 4.1908 - val_loss: 31.9418 - val_mse: 31.9418 - val_mae: 4.1628\n",
      "Epoch 415/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.5375 - mse: 31.5375 - mae: 4.1879 - val_loss: 32.0823 - val_mse: 32.0823 - val_mae: 4.2810\n",
      "Epoch 416/1000\n",
      "325/325 [==============================] - 0s 991us/step - loss: 31.5028 - mse: 31.5028 - mae: 4.1880 - val_loss: 31.8827 - val_mse: 31.8827 - val_mae: 4.2703\n",
      "Epoch 417/1000\n",
      "325/325 [==============================] - 0s 987us/step - loss: 31.4943 - mse: 31.4943 - mae: 4.1868 - val_loss: 32.2552 - val_mse: 32.2552 - val_mae: 4.1821\n",
      "Epoch 418/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.5346 - mse: 31.5346 - mae: 4.1907 - val_loss: 32.0896 - val_mse: 32.0896 - val_mae: 4.2982\n",
      "Epoch 419/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.5493 - mse: 31.5493 - mae: 4.1960 - val_loss: 31.8629 - val_mse: 31.8629 - val_mae: 4.1311\n",
      "Epoch 420/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 31.5126 - mse: 31.5126 - mae: 4.1909 - val_loss: 32.2032 - val_mse: 32.2032 - val_mae: 4.0828\n",
      "Epoch 421/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.5341 - mse: 31.5341 - mae: 4.1867 - val_loss: 31.7964 - val_mse: 31.7964 - val_mae: 4.1653\n",
      "Epoch 422/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.6161 - mse: 31.6161 - mae: 4.1972 - val_loss: 31.8577 - val_mse: 31.8577 - val_mae: 4.1546\n",
      "Epoch 423/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 31.5547 - mse: 31.5547 - mae: 4.1890 - val_loss: 31.8931 - val_mse: 31.8931 - val_mae: 4.1274\n",
      "Epoch 424/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 996us/step - loss: 31.5540 - mse: 31.5540 - mae: 4.1909 - val_loss: 31.8322 - val_mse: 31.8322 - val_mae: 4.1898\n",
      "Epoch 425/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 31.5122 - mse: 31.5122 - mae: 4.1875 - val_loss: 32.3200 - val_mse: 32.3200 - val_mae: 4.3166\n",
      "Epoch 426/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5306 - mse: 31.5306 - mae: 4.1897 - val_loss: 32.0540 - val_mse: 32.0540 - val_mae: 4.2452\n",
      "Epoch 427/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5092 - mse: 31.5092 - mae: 4.1863 - val_loss: 31.8587 - val_mse: 31.8587 - val_mae: 4.1972\n",
      "Epoch 428/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5205 - mse: 31.5205 - mae: 4.1875 - val_loss: 31.9439 - val_mse: 31.9439 - val_mae: 4.2637\n",
      "Epoch 429/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5429 - mse: 31.5429 - mae: 4.1944 - val_loss: 31.8732 - val_mse: 31.8732 - val_mae: 4.2069\n",
      "Epoch 430/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5456 - mse: 31.5456 - mae: 4.1910 - val_loss: 31.9135 - val_mse: 31.9135 - val_mae: 4.1904\n",
      "Epoch 431/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 31.5117 - mse: 31.5117 - mae: 4.1876 - val_loss: 31.7537 - val_mse: 31.7537 - val_mae: 4.2025\n",
      "Epoch 432/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5112 - mse: 31.5112 - mae: 4.1856 - val_loss: 31.8373 - val_mse: 31.8373 - val_mae: 4.2745\n",
      "Epoch 433/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5331 - mse: 31.5331 - mae: 4.1887 - val_loss: 32.2297 - val_mse: 32.2297 - val_mae: 4.3356\n",
      "Epoch 434/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5723 - mse: 31.5723 - mae: 4.1896 - val_loss: 32.0138 - val_mse: 32.0138 - val_mae: 4.3368\n",
      "Epoch 435/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4700 - mse: 31.4700 - mae: 4.1880 - val_loss: 31.8582 - val_mse: 31.8582 - val_mae: 4.1419\n",
      "Epoch 436/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4812 - mse: 31.4812 - mae: 4.1867 - val_loss: 31.8642 - val_mse: 31.8642 - val_mae: 4.1499\n",
      "Epoch 437/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4668 - mse: 31.4668 - mae: 4.1811 - val_loss: 31.9164 - val_mse: 31.9164 - val_mae: 4.2024\n",
      "Epoch 438/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.4805 - mse: 31.4805 - mae: 4.1900 - val_loss: 31.8963 - val_mse: 31.8963 - val_mae: 4.2363\n",
      "Epoch 439/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.5549 - mse: 31.5549 - mae: 4.1908 - val_loss: 31.7809 - val_mse: 31.7809 - val_mae: 4.2178\n",
      "Epoch 440/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4657 - mse: 31.4657 - mae: 4.1839 - val_loss: 31.7632 - val_mse: 31.7632 - val_mae: 4.2035\n",
      "Epoch 441/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4840 - mse: 31.4840 - mae: 4.1856 - val_loss: 31.8377 - val_mse: 31.8377 - val_mae: 4.2926\n",
      "Epoch 442/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4831 - mse: 31.4831 - mae: 4.1817 - val_loss: 31.7702 - val_mse: 31.7702 - val_mae: 4.2395\n",
      "Epoch 443/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5271 - mse: 31.5271 - mae: 4.1899 - val_loss: 31.8498 - val_mse: 31.8498 - val_mae: 4.2726\n",
      "Epoch 444/1000\n",
      "325/325 [==============================] - 0s 988us/step - loss: 31.5006 - mse: 31.5006 - mae: 4.1881 - val_loss: 32.0638 - val_mse: 32.0638 - val_mae: 4.1189\n",
      "Epoch 445/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5509 - mse: 31.5509 - mae: 4.1907 - val_loss: 32.1186 - val_mse: 32.1186 - val_mae: 4.1052\n",
      "Epoch 446/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4759 - mse: 31.4759 - mae: 4.1829 - val_loss: 31.9774 - val_mse: 31.9774 - val_mae: 4.3264\n",
      "Epoch 447/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.5193 - mse: 31.5193 - mae: 4.1901 - val_loss: 31.8825 - val_mse: 31.8825 - val_mae: 4.1468\n",
      "Epoch 448/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4821 - mse: 31.4821 - mae: 4.1838 - val_loss: 32.0747 - val_mse: 32.0747 - val_mae: 4.2946\n",
      "Epoch 449/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5020 - mse: 31.5020 - mae: 4.1879 - val_loss: 31.7889 - val_mse: 31.7889 - val_mae: 4.2506\n",
      "Epoch 450/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4761 - mse: 31.4761 - mae: 4.1866 - val_loss: 31.7987 - val_mse: 31.7987 - val_mae: 4.1491\n",
      "Epoch 451/1000\n",
      "325/325 [==============================] - 0s 991us/step - loss: 31.4914 - mse: 31.4914 - mae: 4.1854 - val_loss: 31.8165 - val_mse: 31.8165 - val_mae: 4.2520\n",
      "Epoch 452/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 31.4532 - mse: 31.4532 - mae: 4.1829 - val_loss: 31.7993 - val_mse: 31.7993 - val_mae: 4.1812\n",
      "Epoch 453/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.4894 - mse: 31.4894 - mae: 4.1856 - val_loss: 31.8180 - val_mse: 31.8180 - val_mae: 4.1364\n",
      "Epoch 454/1000\n",
      "325/325 [==============================] - 0s 994us/step - loss: 31.5172 - mse: 31.5172 - mae: 4.1883 - val_loss: 31.8149 - val_mse: 31.8149 - val_mae: 4.1366\n",
      "Epoch 455/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.5087 - mse: 31.5087 - mae: 4.1847 - val_loss: 32.0439 - val_mse: 32.0439 - val_mae: 4.2642\n",
      "Epoch 456/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4439 - mse: 31.4439 - mae: 4.1837 - val_loss: 32.1184 - val_mse: 32.1184 - val_mae: 4.1388\n",
      "Epoch 457/1000\n",
      "325/325 [==============================] - 0s 993us/step - loss: 31.4848 - mse: 31.4849 - mae: 4.1840 - val_loss: 31.9301 - val_mse: 31.9301 - val_mae: 4.2927\n",
      "Epoch 458/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.4851 - mse: 31.4851 - mae: 4.1854 - val_loss: 31.8039 - val_mse: 31.8039 - val_mae: 4.2317\n",
      "Epoch 459/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.5220 - mse: 31.5220 - mae: 4.1860 - val_loss: 31.8417 - val_mse: 31.8417 - val_mae: 4.2996\n",
      "Epoch 460/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.4662 - mse: 31.4662 - mae: 4.1840 - val_loss: 32.1127 - val_mse: 32.1127 - val_mae: 4.1786\n",
      "Epoch 461/1000\n",
      "325/325 [==============================] - 0s 989us/step - loss: 31.4631 - mse: 31.4631 - mae: 4.1859 - val_loss: 31.7700 - val_mse: 31.7700 - val_mae: 4.2554\n",
      "Epoch 462/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4352 - mse: 31.4352 - mae: 4.1824 - val_loss: 32.0159 - val_mse: 32.0159 - val_mae: 4.1060\n",
      "Epoch 463/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.5004 - mse: 31.5004 - mae: 4.1850 - val_loss: 31.8257 - val_mse: 31.8257 - val_mae: 4.2825\n",
      "Epoch 464/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 31.4304 - mse: 31.4304 - mae: 4.1829 - val_loss: 31.7887 - val_mse: 31.7887 - val_mae: 4.1654\n",
      "Epoch 465/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.4450 - mse: 31.4450 - mae: 4.1825 - val_loss: 31.7405 - val_mse: 31.7405 - val_mae: 4.1973\n",
      "Epoch 466/1000\n",
      "325/325 [==============================] - 0s 991us/step - loss: 31.5555 - mse: 31.5555 - mae: 4.1881 - val_loss: 31.8802 - val_mse: 31.8802 - val_mae: 4.1328\n",
      "Epoch 467/1000\n",
      "325/325 [==============================] - 0s 998us/step - loss: 31.4544 - mse: 31.4544 - mae: 4.1847 - val_loss: 31.8741 - val_mse: 31.8741 - val_mae: 4.1455\n",
      "Epoch 468/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4410 - mse: 31.4410 - mae: 4.1828 - val_loss: 31.8056 - val_mse: 31.8056 - val_mae: 4.2697\n",
      "Epoch 469/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4997 - mse: 31.4997 - mae: 4.1839 - val_loss: 31.8232 - val_mse: 31.8232 - val_mae: 4.2899\n",
      "Epoch 470/1000\n",
      "325/325 [==============================] - 0s 993us/step - loss: 31.4515 - mse: 31.4515 - mae: 4.1827 - val_loss: 31.7905 - val_mse: 31.7905 - val_mae: 4.1851\n",
      "Epoch 471/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4108 - mse: 31.4108 - mae: 4.1823 - val_loss: 31.9040 - val_mse: 31.9040 - val_mae: 4.2073\n",
      "Epoch 472/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4639 - mse: 31.4639 - mae: 4.1857 - val_loss: 31.8063 - val_mse: 31.8063 - val_mae: 4.1538\n",
      "Epoch 473/1000\n",
      "325/325 [==============================] - 0s 998us/step - loss: 31.4279 - mse: 31.4279 - mae: 4.1790 - val_loss: 31.9352 - val_mse: 31.9352 - val_mae: 4.3528\n",
      "Epoch 474/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.5169 - mse: 31.5169 - mae: 4.1887 - val_loss: 31.8238 - val_mse: 31.8238 - val_mae: 4.2896\n",
      "Epoch 475/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.4377 - mse: 31.4377 - mae: 4.1849 - val_loss: 31.9365 - val_mse: 31.9365 - val_mae: 4.1636\n",
      "Epoch 476/1000\n",
      "325/325 [==============================] - 0s 990us/step - loss: 31.4078 - mse: 31.4078 - mae: 4.1816 - val_loss: 31.8624 - val_mse: 31.8624 - val_mae: 4.1495\n",
      "Epoch 477/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.4646 - mse: 31.4646 - mae: 4.1846 - val_loss: 31.8326 - val_mse: 31.8326 - val_mae: 4.1688\n",
      "Epoch 478/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4365 - mse: 31.4365 - mae: 4.1813 - val_loss: 31.8053 - val_mse: 31.8053 - val_mae: 4.2205\n",
      "Epoch 479/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.4474 - mse: 31.4474 - mae: 4.1819 - val_loss: 32.1722 - val_mse: 32.1722 - val_mae: 4.4090\n",
      "Epoch 480/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.4099 - mse: 31.4099 - mae: 4.1805 - val_loss: 31.7254 - val_mse: 31.7254 - val_mae: 4.2144\n",
      "Epoch 481/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4653 - mse: 31.4653 - mae: 4.1865 - val_loss: 31.7769 - val_mse: 31.7769 - val_mae: 4.1713\n",
      "Epoch 482/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.4914 - mse: 31.4914 - mae: 4.1835 - val_loss: 32.5015 - val_mse: 32.5015 - val_mae: 4.3981\n",
      "Epoch 483/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.4471 - mse: 31.4471 - mae: 4.1799 - val_loss: 31.8379 - val_mse: 31.8379 - val_mae: 4.3014\n",
      "Epoch 484/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.4803 - mse: 31.4803 - mae: 4.1820 - val_loss: 31.8491 - val_mse: 31.8491 - val_mae: 4.2490\n",
      "Epoch 485/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.4345 - mse: 31.4345 - mae: 4.1823 - val_loss: 31.8462 - val_mse: 31.8462 - val_mae: 4.3033\n",
      "Epoch 486/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.4467 - mse: 31.4467 - mae: 4.1840 - val_loss: 31.8184 - val_mse: 31.8184 - val_mae: 4.1547\n",
      "Epoch 487/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 31.4298 - mse: 31.4298 - mae: 4.1800 - val_loss: 31.9875 - val_mse: 31.9875 - val_mae: 4.3513\n",
      "Epoch 488/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4528 - mse: 31.4528 - mae: 4.1834 - val_loss: 31.8407 - val_mse: 31.8407 - val_mae: 4.2713\n",
      "Epoch 489/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.4400 - mse: 31.4400 - mae: 4.1845 - val_loss: 31.8252 - val_mse: 31.8252 - val_mae: 4.1332\n",
      "Epoch 490/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.3969 - mse: 31.3969 - mae: 4.1818 - val_loss: 31.9924 - val_mse: 31.9924 - val_mae: 4.2840\n",
      "Epoch 491/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.4830 - mse: 31.4830 - mae: 4.1839 - val_loss: 32.0766 - val_mse: 32.0766 - val_mae: 4.0648\n",
      "Epoch 492/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.4603 - mse: 31.4603 - mae: 4.1813 - val_loss: 31.8180 - val_mse: 31.8180 - val_mae: 4.1360\n",
      "Epoch 493/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 31.4491 - mse: 31.4491 - mae: 4.1837 - val_loss: 31.8683 - val_mse: 31.8683 - val_mae: 4.3115\n",
      "Epoch 494/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.4665 - mse: 31.4665 - mae: 4.1871 - val_loss: 31.7694 - val_mse: 31.7694 - val_mae: 4.1328\n",
      "Epoch 495/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.4080 - mse: 31.4080 - mae: 4.1817 - val_loss: 31.7956 - val_mse: 31.7956 - val_mae: 4.1427\n",
      "Epoch 496/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4042 - mse: 31.4042 - mae: 4.1863 - val_loss: 32.1595 - val_mse: 32.1595 - val_mae: 4.0648\n",
      "Epoch 497/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4954 - mse: 31.4954 - mae: 4.1804 - val_loss: 31.8795 - val_mse: 31.8795 - val_mae: 4.2883\n",
      "Epoch 498/1000\n",
      "325/325 [==============================] - 0s 998us/step - loss: 31.4166 - mse: 31.4166 - mae: 4.1846 - val_loss: 31.9578 - val_mse: 31.9578 - val_mae: 4.3662\n",
      "Epoch 499/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.4657 - mse: 31.4657 - mae: 4.1862 - val_loss: 31.8869 - val_mse: 31.8869 - val_mae: 4.1092\n",
      "Epoch 500/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3626 - mse: 31.3626 - mae: 4.1731 - val_loss: 32.1076 - val_mse: 32.1076 - val_mae: 4.3363\n",
      "Epoch 501/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4451 - mse: 31.4451 - mae: 4.1827 - val_loss: 31.7102 - val_mse: 31.7102 - val_mae: 4.1925\n",
      "Epoch 502/1000\n",
      "325/325 [==============================] - 0s 1000us/step - loss: 31.4671 - mse: 31.4671 - mae: 4.1825 - val_loss: 31.7192 - val_mse: 31.7192 - val_mae: 4.2642\n",
      "Epoch 503/1000\n",
      "325/325 [==============================] - 0s 995us/step - loss: 31.3916 - mse: 31.3916 - mae: 4.1795 - val_loss: 32.0266 - val_mse: 32.0266 - val_mae: 4.3470\n",
      "Epoch 504/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4241 - mse: 31.4241 - mae: 4.1841 - val_loss: 31.8688 - val_mse: 31.8688 - val_mae: 4.1255\n",
      "Epoch 505/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3645 - mse: 31.3645 - mae: 4.1762 - val_loss: 31.9534 - val_mse: 31.9534 - val_mae: 4.2686\n",
      "Epoch 506/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4247 - mse: 31.4247 - mae: 4.1838 - val_loss: 31.9181 - val_mse: 31.9181 - val_mae: 4.2813\n",
      "Epoch 507/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4469 - mse: 31.4469 - mae: 4.1863 - val_loss: 31.8825 - val_mse: 31.8825 - val_mae: 4.3312\n",
      "Epoch 508/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3997 - mse: 31.3997 - mae: 4.1784 - val_loss: 32.0651 - val_mse: 32.0651 - val_mae: 4.3207\n",
      "Epoch 509/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4408 - mse: 31.4408 - mae: 4.1849 - val_loss: 32.0256 - val_mse: 32.0256 - val_mae: 4.2882\n",
      "Epoch 510/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4442 - mse: 31.4442 - mae: 4.1854 - val_loss: 31.6766 - val_mse: 31.6766 - val_mae: 4.2089\n",
      "Epoch 511/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4024 - mse: 31.4024 - mae: 4.1774 - val_loss: 31.8787 - val_mse: 31.8787 - val_mae: 4.3096\n",
      "Epoch 512/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3601 - mse: 31.3601 - mae: 4.1789 - val_loss: 32.0369 - val_mse: 32.0369 - val_mae: 4.2228\n",
      "Epoch 513/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4044 - mse: 31.4044 - mae: 4.1813 - val_loss: 31.7618 - val_mse: 31.7618 - val_mae: 4.2001\n",
      "Epoch 514/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4016 - mse: 31.4016 - mae: 4.1819 - val_loss: 32.3480 - val_mse: 32.3480 - val_mae: 4.0611\n",
      "Epoch 515/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3951 - mse: 31.3951 - mae: 4.1812 - val_loss: 31.8723 - val_mse: 31.8723 - val_mae: 4.1067\n",
      "Epoch 516/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4215 - mse: 31.4215 - mae: 4.1775 - val_loss: 31.8654 - val_mse: 31.8654 - val_mae: 4.3025\n",
      "Epoch 517/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4336 - mse: 31.4336 - mae: 4.1806 - val_loss: 31.7556 - val_mse: 31.7556 - val_mae: 4.2299\n",
      "Epoch 518/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4232 - mse: 31.4232 - mae: 4.1805 - val_loss: 32.1586 - val_mse: 32.1586 - val_mae: 4.0691\n",
      "Epoch 519/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3731 - mse: 31.3731 - mae: 4.1766 - val_loss: 31.8907 - val_mse: 31.8907 - val_mae: 4.1195\n",
      "Epoch 520/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.3795 - mse: 31.3795 - mae: 4.1785 - val_loss: 32.5191 - val_mse: 32.5191 - val_mae: 4.4485\n",
      "Epoch 521/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.4461 - mse: 31.4461 - mae: 4.1858 - val_loss: 31.7669 - val_mse: 31.7669 - val_mae: 4.1567\n",
      "Epoch 522/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4321 - mse: 31.4321 - mae: 4.1808 - val_loss: 31.7283 - val_mse: 31.7283 - val_mae: 4.2017\n",
      "Epoch 523/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.4158 - mse: 31.4158 - mae: 4.1798 - val_loss: 31.9374 - val_mse: 31.9374 - val_mae: 4.1582\n",
      "Epoch 524/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.4056 - mse: 31.4056 - mae: 4.1799 - val_loss: 31.7213 - val_mse: 31.7213 - val_mae: 4.2352\n",
      "Epoch 525/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 31.3650 - mse: 31.3650 - mae: 4.1763 - val_loss: 31.6513 - val_mse: 31.6513 - val_mae: 4.1857\n",
      "Epoch 526/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.3636 - mse: 31.3636 - mae: 4.1753 - val_loss: 31.9249 - val_mse: 31.9249 - val_mae: 4.3223\n",
      "Epoch 527/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.3895 - mse: 31.3895 - mae: 4.1814 - val_loss: 32.0109 - val_mse: 32.0109 - val_mae: 4.0922\n",
      "Epoch 528/1000\n",
      "325/325 [==============================] - 0s 995us/step - loss: 31.4203 - mse: 31.4203 - mae: 4.1774 - val_loss: 31.8229 - val_mse: 31.8229 - val_mae: 4.1333\n",
      "Epoch 529/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 31.3792 - mse: 31.3792 - mae: 4.1785 - val_loss: 31.8701 - val_mse: 31.8701 - val_mae: 4.2914\n",
      "Epoch 530/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.3477 - mse: 31.3477 - mae: 4.1763 - val_loss: 32.0803 - val_mse: 32.0803 - val_mae: 4.3258\n",
      "Epoch 531/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.4248 - mse: 31.4248 - mae: 4.1830 - val_loss: 31.7130 - val_mse: 31.7130 - val_mae: 4.1754\n",
      "Epoch 532/1000\n",
      "325/325 [==============================] - 0s 995us/step - loss: 31.4680 - mse: 31.4680 - mae: 4.1848 - val_loss: 31.7067 - val_mse: 31.7067 - val_mae: 4.1919\n",
      "Epoch 533/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3894 - mse: 31.3894 - mae: 4.1776 - val_loss: 31.7337 - val_mse: 31.7337 - val_mae: 4.1962\n",
      "Epoch 534/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 31.3901 - mse: 31.3901 - mae: 4.1753 - val_loss: 31.7387 - val_mse: 31.7387 - val_mae: 4.2922\n",
      "Epoch 535/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.4180 - mse: 31.4180 - mae: 4.1842 - val_loss: 31.7408 - val_mse: 31.7408 - val_mae: 4.1563\n",
      "Epoch 536/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.3905 - mse: 31.3905 - mae: 4.1824 - val_loss: 31.7893 - val_mse: 31.7893 - val_mae: 4.1567\n",
      "Epoch 537/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.4142 - mse: 31.4142 - mae: 4.1800 - val_loss: 32.1013 - val_mse: 32.1013 - val_mae: 4.1404\n",
      "Epoch 538/1000\n",
      "325/325 [==============================] - 0s 989us/step - loss: 31.4044 - mse: 31.4044 - mae: 4.1785 - val_loss: 31.7970 - val_mse: 31.7970 - val_mae: 4.2791\n",
      "Epoch 539/1000\n",
      "325/325 [==============================] - 0s 982us/step - loss: 31.3132 - mse: 31.3132 - mae: 4.1752 - val_loss: 31.8008 - val_mse: 31.8008 - val_mae: 4.2238\n",
      "Epoch 540/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.4185 - mse: 31.4185 - mae: 4.1836 - val_loss: 31.9328 - val_mse: 31.9328 - val_mae: 4.3295\n",
      "Epoch 541/1000\n",
      "325/325 [==============================] - 0s 998us/step - loss: 31.4597 - mse: 31.4597 - mae: 4.1807 - val_loss: 31.6974 - val_mse: 31.6974 - val_mae: 4.2116\n",
      "Epoch 542/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.3804 - mse: 31.3804 - mae: 4.1738 - val_loss: 31.9676 - val_mse: 31.9676 - val_mae: 4.3293\n",
      "Epoch 543/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 31.3749 - mse: 31.3749 - mae: 4.1782 - val_loss: 31.8321 - val_mse: 31.8321 - val_mae: 4.2877\n",
      "Epoch 544/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 31.3445 - mse: 31.3445 - mae: 4.1790 - val_loss: 31.6871 - val_mse: 31.6871 - val_mae: 4.1850\n",
      "Epoch 545/1000\n",
      "325/325 [==============================] - 0s 990us/step - loss: 31.3545 - mse: 31.3545 - mae: 4.1734 - val_loss: 31.9492 - val_mse: 31.9492 - val_mae: 4.3173\n",
      "Epoch 546/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.3661 - mse: 31.3661 - mae: 4.1773 - val_loss: 31.9777 - val_mse: 31.9777 - val_mae: 4.3493\n",
      "Epoch 547/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 31.3002 - mse: 31.3002 - mae: 4.1758 - val_loss: 31.8729 - val_mse: 31.8729 - val_mae: 4.1563\n",
      "Epoch 548/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.3120 - mse: 31.3120 - mae: 4.1738 - val_loss: 31.6695 - val_mse: 31.6695 - val_mae: 4.1728\n",
      "Epoch 549/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 31.3365 - mse: 31.3365 - mae: 4.1783 - val_loss: 31.7217 - val_mse: 31.7217 - val_mae: 4.1658\n",
      "Epoch 550/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 31.4567 - mse: 31.4567 - mae: 4.1821 - val_loss: 31.8439 - val_mse: 31.8439 - val_mae: 4.1228\n",
      "Epoch 551/1000\n",
      "325/325 [==============================] - 0s 943us/step - loss: 31.3518 - mse: 31.3518 - mae: 4.1770 - val_loss: 31.6794 - val_mse: 31.6794 - val_mae: 4.1614\n",
      "Epoch 552/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.3493 - mse: 31.3493 - mae: 4.1761 - val_loss: 32.3966 - val_mse: 32.3966 - val_mae: 4.4111\n",
      "Epoch 553/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 31.4261 - mse: 31.4261 - mae: 4.1859 - val_loss: 31.8514 - val_mse: 31.8514 - val_mae: 4.2052\n",
      "Epoch 554/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 31.3349 - mse: 31.3349 - mae: 4.1740 - val_loss: 31.7136 - val_mse: 31.7136 - val_mae: 4.1811\n",
      "Epoch 555/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 31.3915 - mse: 31.3915 - mae: 4.1796 - val_loss: 31.6438 - val_mse: 31.6438 - val_mae: 4.2108\n",
      "Epoch 556/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.3193 - mse: 31.3193 - mae: 4.1747 - val_loss: 31.6553 - val_mse: 31.6554 - val_mae: 4.1760\n",
      "Epoch 557/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 31.3568 - mse: 31.3568 - mae: 4.1785 - val_loss: 31.9842 - val_mse: 31.9842 - val_mae: 4.0917\n",
      "Epoch 558/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 31.3466 - mse: 31.3466 - mae: 4.1770 - val_loss: 31.7549 - val_mse: 31.7549 - val_mae: 4.1917\n",
      "Epoch 559/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 31.3858 - mse: 31.3858 - mae: 4.1821 - val_loss: 32.1250 - val_mse: 32.1250 - val_mae: 4.1080\n",
      "Epoch 560/1000\n",
      "325/325 [==============================] - 0s 950us/step - loss: 31.4183 - mse: 31.4183 - mae: 4.1777 - val_loss: 31.6926 - val_mse: 31.6926 - val_mae: 4.1773\n",
      "Epoch 561/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 31.3876 - mse: 31.3876 - mae: 4.1824 - val_loss: 31.7193 - val_mse: 31.7193 - val_mae: 4.2194\n",
      "Epoch 562/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.3555 - mse: 31.3555 - mae: 4.1767 - val_loss: 31.7658 - val_mse: 31.7658 - val_mae: 4.2267\n",
      "Epoch 563/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 31.3947 - mse: 31.3947 - mae: 4.1805 - val_loss: 32.0710 - val_mse: 32.0710 - val_mae: 4.3714\n",
      "Epoch 564/1000\n",
      "325/325 [==============================] - 0s 985us/step - loss: 31.3495 - mse: 31.3495 - mae: 4.1754 - val_loss: 32.0980 - val_mse: 32.0980 - val_mae: 4.3101\n",
      "Epoch 565/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 979us/step - loss: 31.4103 - mse: 31.4103 - mae: 4.1845 - val_loss: 31.7803 - val_mse: 31.7803 - val_mae: 4.1321\n",
      "Epoch 566/1000\n",
      "325/325 [==============================] - 0s 951us/step - loss: 31.3436 - mse: 31.3436 - mae: 4.1722 - val_loss: 32.0174 - val_mse: 32.0174 - val_mae: 4.2968\n",
      "Epoch 567/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 31.3823 - mse: 31.3823 - mae: 4.1811 - val_loss: 31.7243 - val_mse: 31.7243 - val_mae: 4.2223\n",
      "Epoch 568/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.3307 - mse: 31.3307 - mae: 4.1749 - val_loss: 31.7543 - val_mse: 31.7543 - val_mae: 4.1586\n",
      "Epoch 569/1000\n",
      "325/325 [==============================] - 0s 951us/step - loss: 31.3282 - mse: 31.3282 - mae: 4.1783 - val_loss: 31.8483 - val_mse: 31.8483 - val_mae: 4.2160\n",
      "Epoch 570/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.2647 - mse: 31.2647 - mae: 4.1707 - val_loss: 31.6450 - val_mse: 31.6450 - val_mae: 4.1739\n",
      "Epoch 571/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 31.3483 - mse: 31.3483 - mae: 4.1732 - val_loss: 31.7962 - val_mse: 31.7962 - val_mae: 4.2340\n",
      "Epoch 572/1000\n",
      "325/325 [==============================] - 0s 951us/step - loss: 31.3700 - mse: 31.3700 - mae: 4.1783 - val_loss: 31.6494 - val_mse: 31.6494 - val_mae: 4.2081\n",
      "Epoch 573/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 31.3308 - mse: 31.3308 - mae: 4.1742 - val_loss: 31.7006 - val_mse: 31.7006 - val_mae: 4.2912\n",
      "Epoch 574/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.4016 - mse: 31.4016 - mae: 4.1809 - val_loss: 31.7261 - val_mse: 31.7261 - val_mae: 4.1226\n",
      "Epoch 575/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.3761 - mse: 31.3761 - mae: 4.1796 - val_loss: 32.1760 - val_mse: 32.1760 - val_mae: 4.3552\n",
      "Epoch 576/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.3206 - mse: 31.3206 - mae: 4.1746 - val_loss: 31.6791 - val_mse: 31.6791 - val_mae: 4.1476\n",
      "Epoch 577/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 31.3474 - mse: 31.3474 - mae: 4.1778 - val_loss: 31.6288 - val_mse: 31.6288 - val_mae: 4.1745\n",
      "Epoch 578/1000\n",
      "325/325 [==============================] - 0s 954us/step - loss: 31.3333 - mse: 31.3333 - mae: 4.1732 - val_loss: 31.6973 - val_mse: 31.6973 - val_mae: 4.1814\n",
      "Epoch 579/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 31.3236 - mse: 31.3236 - mae: 4.1745 - val_loss: 32.1761 - val_mse: 32.1761 - val_mae: 4.3278\n",
      "Epoch 580/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 31.3642 - mse: 31.3642 - mae: 4.1811 - val_loss: 31.7419 - val_mse: 31.7419 - val_mae: 4.1294\n",
      "Epoch 581/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.3376 - mse: 31.3376 - mae: 4.1757 - val_loss: 31.8969 - val_mse: 31.8969 - val_mae: 4.1520\n",
      "Epoch 582/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.2863 - mse: 31.2863 - mae: 4.1690 - val_loss: 31.9833 - val_mse: 31.9833 - val_mae: 4.1009\n",
      "Epoch 583/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 31.2924 - mse: 31.2924 - mae: 4.1695 - val_loss: 31.8598 - val_mse: 31.8598 - val_mae: 4.3161\n",
      "Epoch 584/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.3791 - mse: 31.3791 - mae: 4.1810 - val_loss: 31.8226 - val_mse: 31.8226 - val_mae: 4.2892\n",
      "Epoch 585/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.3486 - mse: 31.3486 - mae: 4.1752 - val_loss: 31.6754 - val_mse: 31.6754 - val_mae: 4.1485\n",
      "Epoch 586/1000\n",
      "325/325 [==============================] - 0s 954us/step - loss: 31.3587 - mse: 31.3587 - mae: 4.1776 - val_loss: 31.7527 - val_mse: 31.7527 - val_mae: 4.2419\n",
      "Epoch 587/1000\n",
      "325/325 [==============================] - 0s 992us/step - loss: 31.3744 - mse: 31.3744 - mae: 4.1799 - val_loss: 31.9021 - val_mse: 31.9021 - val_mae: 4.2821\n",
      "Epoch 588/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3423 - mse: 31.3423 - mae: 4.1724 - val_loss: 31.7603 - val_mse: 31.7603 - val_mae: 4.1423\n",
      "Epoch 589/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.3259 - mse: 31.3259 - mae: 4.1768 - val_loss: 31.7161 - val_mse: 31.7161 - val_mae: 4.2443\n",
      "Epoch 590/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3675 - mse: 31.3675 - mae: 4.1760 - val_loss: 31.6424 - val_mse: 31.6424 - val_mae: 4.2188\n",
      "Epoch 591/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 31.3514 - mse: 31.3514 - mae: 4.1754 - val_loss: 31.7070 - val_mse: 31.7070 - val_mae: 4.2701\n",
      "Epoch 592/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 31.2849 - mse: 31.2849 - mae: 4.1704 - val_loss: 31.8196 - val_mse: 31.8196 - val_mae: 4.3111\n",
      "Epoch 593/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3489 - mse: 31.3489 - mae: 4.1776 - val_loss: 31.6997 - val_mse: 31.6997 - val_mae: 4.2325\n",
      "Epoch 594/1000\n",
      "325/325 [==============================] - 0s 994us/step - loss: 31.3082 - mse: 31.3082 - mae: 4.1746 - val_loss: 31.6651 - val_mse: 31.6651 - val_mae: 4.1942\n",
      "Epoch 595/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 31.3221 - mse: 31.3221 - mae: 4.1740 - val_loss: 31.6383 - val_mse: 31.6383 - val_mae: 4.2545\n",
      "Epoch 596/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 31.2892 - mse: 31.2892 - mae: 4.1737 - val_loss: 32.0210 - val_mse: 32.0210 - val_mae: 4.0775\n",
      "Epoch 597/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 31.3096 - mse: 31.3096 - mae: 4.1757 - val_loss: 31.6208 - val_mse: 31.6208 - val_mae: 4.2042\n",
      "Epoch 598/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.3629 - mse: 31.3629 - mae: 4.1787 - val_loss: 31.7041 - val_mse: 31.7041 - val_mae: 4.1335\n",
      "Epoch 599/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 31.3872 - mse: 31.3872 - mae: 4.1777 - val_loss: 31.6695 - val_mse: 31.6695 - val_mae: 4.2062\n",
      "Epoch 600/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.3261 - mse: 31.3261 - mae: 4.1733 - val_loss: 31.7674 - val_mse: 31.7674 - val_mae: 4.1535\n",
      "Epoch 601/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 31.3285 - mse: 31.3285 - mae: 4.1726 - val_loss: 31.8218 - val_mse: 31.8218 - val_mae: 4.3065\n",
      "Epoch 602/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3404 - mse: 31.3404 - mae: 4.1746 - val_loss: 31.7451 - val_mse: 31.7451 - val_mae: 4.2987\n",
      "Epoch 603/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.2811 - mse: 31.2811 - mae: 4.1727 - val_loss: 31.6579 - val_mse: 31.6579 - val_mae: 4.1390\n",
      "Epoch 604/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.3257 - mse: 31.3257 - mae: 4.1722 - val_loss: 31.8198 - val_mse: 31.8198 - val_mae: 4.1418\n",
      "Epoch 605/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.2979 - mse: 31.2979 - mae: 4.1693 - val_loss: 31.8364 - val_mse: 31.8364 - val_mae: 4.3327\n",
      "Epoch 606/1000\n",
      "325/325 [==============================] - 0s 993us/step - loss: 31.2813 - mse: 31.2813 - mae: 4.1706 - val_loss: 31.7078 - val_mse: 31.7078 - val_mae: 4.1639\n",
      "Epoch 607/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 31.2802 - mse: 31.2802 - mae: 4.1707 - val_loss: 31.9896 - val_mse: 31.9896 - val_mae: 4.1249\n",
      "Epoch 608/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 31.2662 - mse: 31.2662 - mae: 4.1709 - val_loss: 32.0172 - val_mse: 32.0172 - val_mae: 4.3864\n",
      "Epoch 609/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.3269 - mse: 31.3269 - mae: 4.1748 - val_loss: 32.0060 - val_mse: 32.0060 - val_mae: 4.3159\n",
      "Epoch 610/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.3606 - mse: 31.3606 - mae: 4.1763 - val_loss: 31.6649 - val_mse: 31.6649 - val_mae: 4.2201\n",
      "Epoch 611/1000\n",
      "325/325 [==============================] - 0s 987us/step - loss: 31.3070 - mse: 31.3070 - mae: 4.1719 - val_loss: 31.6579 - val_mse: 31.6579 - val_mae: 4.2192\n",
      "Epoch 612/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 950us/step - loss: 31.3000 - mse: 31.3000 - mae: 4.1701 - val_loss: 31.7767 - val_mse: 31.7767 - val_mae: 4.1896\n",
      "Epoch 613/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.2614 - mse: 31.2614 - mae: 4.1688 - val_loss: 31.7499 - val_mse: 31.7499 - val_mae: 4.2542\n",
      "Epoch 614/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.2900 - mse: 31.2900 - mae: 4.1717 - val_loss: 31.6856 - val_mse: 31.6856 - val_mae: 4.1264\n",
      "Epoch 615/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.2596 - mse: 31.2596 - mae: 4.1634 - val_loss: 31.6786 - val_mse: 31.6786 - val_mae: 4.2289\n",
      "Epoch 616/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3408 - mse: 31.3408 - mae: 4.1756 - val_loss: 31.6395 - val_mse: 31.6395 - val_mae: 4.1418\n",
      "Epoch 617/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.3260 - mse: 31.3260 - mae: 4.1725 - val_loss: 31.6631 - val_mse: 31.6631 - val_mae: 4.2187\n",
      "Epoch 618/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.3391 - mse: 31.3391 - mae: 4.1716 - val_loss: 31.7099 - val_mse: 31.7099 - val_mae: 4.1844\n",
      "Epoch 619/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 31.3190 - mse: 31.3190 - mae: 4.1755 - val_loss: 31.6845 - val_mse: 31.6845 - val_mae: 4.2004\n",
      "Epoch 620/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.2826 - mse: 31.2826 - mae: 4.1674 - val_loss: 31.7086 - val_mse: 31.7086 - val_mae: 4.1488\n",
      "Epoch 621/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3234 - mse: 31.3234 - mae: 4.1724 - val_loss: 31.7672 - val_mse: 31.7672 - val_mae: 4.0979\n",
      "Epoch 622/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.2417 - mse: 31.2417 - mae: 4.1663 - val_loss: 31.8930 - val_mse: 31.8930 - val_mae: 4.2855\n",
      "Epoch 623/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.2535 - mse: 31.2535 - mae: 4.1691 - val_loss: 31.8831 - val_mse: 31.8831 - val_mae: 4.2822\n",
      "Epoch 624/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 31.3608 - mse: 31.3608 - mae: 4.1748 - val_loss: 31.8473 - val_mse: 31.8473 - val_mae: 4.1383\n",
      "Epoch 625/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.2637 - mse: 31.2637 - mae: 4.1688 - val_loss: 31.6450 - val_mse: 31.6450 - val_mae: 4.1938\n",
      "Epoch 626/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2928 - mse: 31.2928 - mae: 4.1730 - val_loss: 31.8809 - val_mse: 31.8809 - val_mae: 4.0786\n",
      "Epoch 627/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2781 - mse: 31.2781 - mae: 4.1693 - val_loss: 31.8049 - val_mse: 31.8049 - val_mae: 4.2400\n",
      "Epoch 628/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2747 - mse: 31.2747 - mae: 4.1725 - val_loss: 31.6731 - val_mse: 31.6731 - val_mae: 4.1482\n",
      "Epoch 629/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2299 - mse: 31.2299 - mae: 4.1618 - val_loss: 31.7085 - val_mse: 31.7085 - val_mae: 4.3000\n",
      "Epoch 630/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3499 - mse: 31.3499 - mae: 4.1737 - val_loss: 31.7515 - val_mse: 31.7515 - val_mae: 4.2405\n",
      "Epoch 631/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3094 - mse: 31.3094 - mae: 4.1753 - val_loss: 31.5775 - val_mse: 31.5775 - val_mae: 4.1939\n",
      "Epoch 632/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 31.2736 - mse: 31.2736 - mae: 4.1723 - val_loss: 31.7320 - val_mse: 31.7320 - val_mae: 4.1722\n",
      "Epoch 633/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.3640 - mse: 31.3640 - mae: 4.1778 - val_loss: 31.8405 - val_mse: 31.8405 - val_mae: 4.1829\n",
      "Epoch 634/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2434 - mse: 31.2434 - mae: 4.1635 - val_loss: 31.7085 - val_mse: 31.7085 - val_mae: 4.2991\n",
      "Epoch 635/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2941 - mse: 31.2941 - mae: 4.1717 - val_loss: 31.7106 - val_mse: 31.7106 - val_mae: 4.1428\n",
      "Epoch 636/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2755 - mse: 31.2755 - mae: 4.1678 - val_loss: 31.5476 - val_mse: 31.5476 - val_mae: 4.2116\n",
      "Epoch 637/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.3707 - mse: 31.3707 - mae: 4.1748 - val_loss: 32.8671 - val_mse: 32.8671 - val_mae: 4.5605\n",
      "Epoch 638/1000\n",
      "325/325 [==============================] - 0s 994us/step - loss: 31.3723 - mse: 31.3723 - mae: 4.1825 - val_loss: 31.6836 - val_mse: 31.6836 - val_mae: 4.1414\n",
      "Epoch 639/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.2560 - mse: 31.2560 - mae: 4.1736 - val_loss: 31.6535 - val_mse: 31.6535 - val_mae: 4.1433\n",
      "Epoch 640/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.2786 - mse: 31.2786 - mae: 4.1691 - val_loss: 31.6971 - val_mse: 31.6971 - val_mae: 4.1576\n",
      "Epoch 641/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 31.3279 - mse: 31.3279 - mae: 4.1703 - val_loss: 31.5475 - val_mse: 31.5475 - val_mae: 4.2129\n",
      "Epoch 642/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 31.3451 - mse: 31.3451 - mae: 4.1731 - val_loss: 31.8495 - val_mse: 31.8495 - val_mae: 4.1424\n",
      "Epoch 643/1000\n",
      "325/325 [==============================] - 0s 949us/step - loss: 31.2382 - mse: 31.2382 - mae: 4.1661 - val_loss: 32.1028 - val_mse: 32.1028 - val_mae: 4.3191\n",
      "Epoch 644/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.2491 - mse: 31.2491 - mae: 4.1729 - val_loss: 31.7322 - val_mse: 31.7322 - val_mae: 4.1120\n",
      "Epoch 645/1000\n",
      "325/325 [==============================] - 0s 983us/step - loss: 31.2728 - mse: 31.2728 - mae: 4.1655 - val_loss: 31.6563 - val_mse: 31.6563 - val_mae: 4.2313\n",
      "Epoch 646/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.2325 - mse: 31.2325 - mae: 4.1643 - val_loss: 31.6519 - val_mse: 31.6519 - val_mae: 4.1610\n",
      "Epoch 647/1000\n",
      "325/325 [==============================] - 0s 942us/step - loss: 31.3217 - mse: 31.3217 - mae: 4.1745 - val_loss: 31.9402 - val_mse: 31.9402 - val_mae: 4.3345\n",
      "Epoch 648/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 31.2684 - mse: 31.2684 - mae: 4.1660 - val_loss: 31.7511 - val_mse: 31.7511 - val_mae: 4.1451\n",
      "Epoch 649/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.2715 - mse: 31.2715 - mae: 4.1688 - val_loss: 31.5778 - val_mse: 31.5778 - val_mae: 4.1793\n",
      "Epoch 650/1000\n",
      "325/325 [==============================] - 0s 987us/step - loss: 31.2117 - mse: 31.2117 - mae: 4.1660 - val_loss: 31.7225 - val_mse: 31.7225 - val_mae: 4.0947\n",
      "Epoch 651/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.2980 - mse: 31.2980 - mae: 4.1697 - val_loss: 32.3499 - val_mse: 32.3499 - val_mae: 4.4393\n",
      "Epoch 652/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.2507 - mse: 31.2507 - mae: 4.1655 - val_loss: 31.5288 - val_mse: 31.5288 - val_mae: 4.2223\n",
      "Epoch 653/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.2760 - mse: 31.2760 - mae: 4.1717 - val_loss: 31.6480 - val_mse: 31.6480 - val_mae: 4.2148\n",
      "Epoch 654/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.2770 - mse: 31.2770 - mae: 4.1688 - val_loss: 31.6272 - val_mse: 31.6272 - val_mae: 4.2788\n",
      "Epoch 655/1000\n",
      "325/325 [==============================] - 0s 947us/step - loss: 31.2078 - mse: 31.2078 - mae: 4.1655 - val_loss: 31.6795 - val_mse: 31.6795 - val_mae: 4.1317\n",
      "Epoch 656/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.3211 - mse: 31.3211 - mae: 4.1707 - val_loss: 31.5720 - val_mse: 31.5720 - val_mae: 4.2487\n",
      "Epoch 657/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.3426 - mse: 31.3426 - mae: 4.1746 - val_loss: 31.5498 - val_mse: 31.5498 - val_mae: 4.1959\n",
      "Epoch 658/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.2391 - mse: 31.2391 - mae: 4.1633 - val_loss: 31.6365 - val_mse: 31.6365 - val_mae: 4.2732\n",
      "Epoch 659/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 959us/step - loss: 31.2269 - mse: 31.2269 - mae: 4.1683 - val_loss: 31.5746 - val_mse: 31.5746 - val_mae: 4.1958\n",
      "Epoch 660/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.3067 - mse: 31.3067 - mae: 4.1678 - val_loss: 31.5094 - val_mse: 31.5094 - val_mae: 4.1639\n",
      "Epoch 661/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.3137 - mse: 31.3137 - mae: 4.1672 - val_loss: 31.7137 - val_mse: 31.7137 - val_mae: 4.2523\n",
      "Epoch 662/1000\n",
      "325/325 [==============================] - 0s 948us/step - loss: 31.2823 - mse: 31.2823 - mae: 4.1764 - val_loss: 31.6788 - val_mse: 31.6788 - val_mae: 4.1393\n",
      "Epoch 663/1000\n",
      "325/325 [==============================] - 0s 950us/step - loss: 31.2529 - mse: 31.2529 - mae: 4.1663 - val_loss: 31.6136 - val_mse: 31.6136 - val_mae: 4.1778\n",
      "Epoch 664/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.2198 - mse: 31.2198 - mae: 4.1665 - val_loss: 31.6416 - val_mse: 31.6416 - val_mae: 4.1762\n",
      "Epoch 665/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.2395 - mse: 31.2395 - mae: 4.1692 - val_loss: 31.7588 - val_mse: 31.7588 - val_mae: 4.0917\n",
      "Epoch 666/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 31.2947 - mse: 31.2947 - mae: 4.1664 - val_loss: 31.8344 - val_mse: 31.8344 - val_mae: 4.1229\n",
      "Epoch 667/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.2179 - mse: 31.2179 - mae: 4.1645 - val_loss: 31.6247 - val_mse: 31.6247 - val_mae: 4.2752\n",
      "Epoch 668/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.3187 - mse: 31.3187 - mae: 4.1736 - val_loss: 31.8125 - val_mse: 31.8125 - val_mae: 4.2216\n",
      "Epoch 669/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.2281 - mse: 31.2281 - mae: 4.1654 - val_loss: 31.5928 - val_mse: 31.5928 - val_mae: 4.1383\n",
      "Epoch 670/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.2601 - mse: 31.2601 - mae: 4.1685 - val_loss: 31.6514 - val_mse: 31.6514 - val_mae: 4.2450\n",
      "Epoch 671/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.2302 - mse: 31.2302 - mae: 4.1629 - val_loss: 31.6670 - val_mse: 31.6670 - val_mae: 4.1522\n",
      "Epoch 672/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.2183 - mse: 31.2183 - mae: 4.1645 - val_loss: 31.7905 - val_mse: 31.7905 - val_mae: 4.1348\n",
      "Epoch 673/1000\n",
      "325/325 [==============================] - 0s 957us/step - loss: 31.2683 - mse: 31.2683 - mae: 4.1677 - val_loss: 31.7971 - val_mse: 31.7971 - val_mae: 4.2051\n",
      "Epoch 674/1000\n",
      "325/325 [==============================] - 0s 983us/step - loss: 31.2570 - mse: 31.2570 - mae: 4.1662 - val_loss: 31.5276 - val_mse: 31.5276 - val_mae: 4.1773\n",
      "Epoch 675/1000\n",
      "325/325 [==============================] - 0s 993us/step - loss: 31.3045 - mse: 31.3045 - mae: 4.1732 - val_loss: 31.5040 - val_mse: 31.5040 - val_mae: 4.1753\n",
      "Epoch 676/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2293 - mse: 31.2293 - mae: 4.1637 - val_loss: 31.6938 - val_mse: 31.6938 - val_mae: 4.1110\n",
      "Epoch 677/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2303 - mse: 31.2303 - mae: 4.1641 - val_loss: 31.5964 - val_mse: 31.5964 - val_mae: 4.1881\n",
      "Epoch 678/1000\n",
      "325/325 [==============================] - 0s 998us/step - loss: 31.2353 - mse: 31.2353 - mae: 4.1690 - val_loss: 31.7475 - val_mse: 31.7475 - val_mae: 4.1113\n",
      "Epoch 679/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.2451 - mse: 31.2451 - mae: 4.1639 - val_loss: 31.5882 - val_mse: 31.5882 - val_mae: 4.2450\n",
      "Epoch 680/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.1706 - mse: 31.1706 - mae: 4.1625 - val_loss: 31.6060 - val_mse: 31.6060 - val_mae: 4.2464\n",
      "Epoch 681/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2468 - mse: 31.2468 - mae: 4.1646 - val_loss: 31.6283 - val_mse: 31.6283 - val_mae: 4.1761\n",
      "Epoch 682/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2366 - mse: 31.2366 - mae: 4.1632 - val_loss: 31.6501 - val_mse: 31.6501 - val_mae: 4.2983\n",
      "Epoch 683/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2671 - mse: 31.2671 - mae: 4.1661 - val_loss: 31.8499 - val_mse: 31.8499 - val_mae: 4.3152\n",
      "Epoch 684/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.2721 - mse: 31.2721 - mae: 4.1707 - val_loss: 31.7798 - val_mse: 31.7798 - val_mae: 4.2636\n",
      "Epoch 685/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.1789 - mse: 31.1789 - mae: 4.1643 - val_loss: 31.8111 - val_mse: 31.8111 - val_mae: 4.3032\n",
      "Epoch 686/1000\n",
      "325/325 [==============================] - 0s 983us/step - loss: 31.2212 - mse: 31.2212 - mae: 4.1658 - val_loss: 31.5490 - val_mse: 31.5490 - val_mae: 4.2179\n",
      "Epoch 687/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1984 - mse: 31.1984 - mae: 4.1652 - val_loss: 31.4902 - val_mse: 31.4902 - val_mae: 4.1896\n",
      "Epoch 688/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2116 - mse: 31.2116 - mae: 4.1655 - val_loss: 31.6112 - val_mse: 31.6112 - val_mae: 4.1677\n",
      "Epoch 689/1000\n",
      "325/325 [==============================] - 0s 980us/step - loss: 31.2635 - mse: 31.2635 - mae: 4.1637 - val_loss: 31.6665 - val_mse: 31.6665 - val_mae: 4.2796\n",
      "Epoch 690/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2391 - mse: 31.2391 - mae: 4.1667 - val_loss: 31.7497 - val_mse: 31.7497 - val_mae: 4.1650\n",
      "Epoch 691/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.2417 - mse: 31.2417 - mae: 4.1657 - val_loss: 31.5981 - val_mse: 31.5981 - val_mae: 4.2277\n",
      "Epoch 692/1000\n",
      "325/325 [==============================] - 0s 987us/step - loss: 31.2323 - mse: 31.2323 - mae: 4.1633 - val_loss: 31.5682 - val_mse: 31.5682 - val_mae: 4.1712\n",
      "Epoch 693/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.2394 - mse: 31.2394 - mae: 4.1647 - val_loss: 31.5132 - val_mse: 31.5132 - val_mae: 4.1862\n",
      "Epoch 694/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2241 - mse: 31.2241 - mae: 4.1664 - val_loss: 31.6408 - val_mse: 31.6408 - val_mae: 4.1705\n",
      "Epoch 695/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2177 - mse: 31.2177 - mae: 4.1637 - val_loss: 32.0400 - val_mse: 32.0400 - val_mae: 4.3771\n",
      "Epoch 696/1000\n",
      "325/325 [==============================] - 0s 991us/step - loss: 31.2616 - mse: 31.2616 - mae: 4.1677 - val_loss: 31.5273 - val_mse: 31.5273 - val_mae: 4.1789\n",
      "Epoch 697/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.2370 - mse: 31.2370 - mae: 4.1637 - val_loss: 31.5930 - val_mse: 31.5930 - val_mae: 4.2584\n",
      "Epoch 698/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.2471 - mse: 31.2471 - mae: 4.1685 - val_loss: 31.5954 - val_mse: 31.5954 - val_mae: 4.1193\n",
      "Epoch 699/1000\n",
      "325/325 [==============================] - 0s 987us/step - loss: 31.2280 - mse: 31.2280 - mae: 4.1638 - val_loss: 31.7514 - val_mse: 31.7514 - val_mae: 4.2643\n",
      "Epoch 700/1000\n",
      "325/325 [==============================] - 0s 951us/step - loss: 31.2575 - mse: 31.2575 - mae: 4.1685 - val_loss: 31.8409 - val_mse: 31.8409 - val_mae: 4.0647\n",
      "Epoch 701/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.1865 - mse: 31.1865 - mae: 4.1624 - val_loss: 31.6414 - val_mse: 31.6414 - val_mae: 4.2533\n",
      "Epoch 702/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.1703 - mse: 31.1703 - mae: 4.1581 - val_loss: 31.4931 - val_mse: 31.4931 - val_mae: 4.2158\n",
      "Epoch 703/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.2532 - mse: 31.2532 - mae: 4.1688 - val_loss: 31.5454 - val_mse: 31.5454 - val_mae: 4.1592\n",
      "Epoch 704/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.2066 - mse: 31.2066 - mae: 4.1646 - val_loss: 31.7048 - val_mse: 31.7048 - val_mae: 4.1112\n",
      "Epoch 705/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.2354 - mse: 31.2354 - mae: 4.1655 - val_loss: 31.8242 - val_mse: 31.8242 - val_mae: 4.2907\n",
      "Epoch 706/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 973us/step - loss: 31.2834 - mse: 31.2834 - mae: 4.1724 - val_loss: 31.5492 - val_mse: 31.5492 - val_mae: 4.2272\n",
      "Epoch 707/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.2133 - mse: 31.2133 - mae: 4.1649 - val_loss: 31.6545 - val_mse: 31.6545 - val_mae: 4.2189\n",
      "Epoch 708/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.2556 - mse: 31.2556 - mae: 4.1667 - val_loss: 31.6015 - val_mse: 31.6015 - val_mae: 4.2065\n",
      "Epoch 709/1000\n",
      "325/325 [==============================] - 0s 989us/step - loss: 31.1637 - mse: 31.1637 - mae: 4.1609 - val_loss: 31.5603 - val_mse: 31.5603 - val_mae: 4.1594\n",
      "Epoch 710/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 31.3611 - mse: 31.3611 - mae: 4.1690 - val_loss: 31.7318 - val_mse: 31.7318 - val_mae: 4.3283\n",
      "Epoch 711/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.2222 - mse: 31.2222 - mae: 4.1625 - val_loss: 31.6795 - val_mse: 31.6795 - val_mae: 4.1886\n",
      "Epoch 712/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 31.2260 - mse: 31.2260 - mae: 4.1627 - val_loss: 31.6368 - val_mse: 31.6368 - val_mae: 4.2493\n",
      "Epoch 713/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.1782 - mse: 31.1782 - mae: 4.1620 - val_loss: 31.5092 - val_mse: 31.5092 - val_mae: 4.1321\n",
      "Epoch 714/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 31.1736 - mse: 31.1736 - mae: 4.1588 - val_loss: 31.4935 - val_mse: 31.4935 - val_mae: 4.1869\n",
      "Epoch 715/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.2530 - mse: 31.2530 - mae: 4.1679 - val_loss: 31.8434 - val_mse: 31.8434 - val_mae: 4.0875\n",
      "Epoch 716/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2375 - mse: 31.2375 - mae: 4.1633 - val_loss: 31.5966 - val_mse: 31.5966 - val_mae: 4.2298\n",
      "Epoch 717/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.2758 - mse: 31.2758 - mae: 4.1672 - val_loss: 31.6879 - val_mse: 31.6879 - val_mae: 4.1303\n",
      "Epoch 718/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.2420 - mse: 31.2420 - mae: 4.1651 - val_loss: 31.7571 - val_mse: 31.7571 - val_mae: 4.1494\n",
      "Epoch 719/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.1904 - mse: 31.1904 - mae: 4.1612 - val_loss: 31.5571 - val_mse: 31.5571 - val_mae: 4.2258\n",
      "Epoch 720/1000\n",
      "325/325 [==============================] - 0s 995us/step - loss: 31.2297 - mse: 31.2297 - mae: 4.1627 - val_loss: 31.9878 - val_mse: 31.9878 - val_mae: 4.3535\n",
      "Epoch 721/1000\n",
      "325/325 [==============================] - 0s 995us/step - loss: 31.2555 - mse: 31.2555 - mae: 4.1655 - val_loss: 31.5260 - val_mse: 31.5260 - val_mae: 4.1733\n",
      "Epoch 722/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 31.2314 - mse: 31.2314 - mae: 4.1650 - val_loss: 31.6287 - val_mse: 31.6287 - val_mae: 4.1228\n",
      "Epoch 723/1000\n",
      "325/325 [==============================] - 0s 989us/step - loss: 31.2200 - mse: 31.2200 - mae: 4.1630 - val_loss: 31.8633 - val_mse: 31.8633 - val_mae: 4.2809\n",
      "Epoch 724/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.2648 - mse: 31.2648 - mae: 4.1712 - val_loss: 31.6290 - val_mse: 31.6290 - val_mae: 4.0986\n",
      "Epoch 725/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.2056 - mse: 31.2056 - mae: 4.1623 - val_loss: 31.9193 - val_mse: 31.9193 - val_mae: 4.3296\n",
      "Epoch 726/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2433 - mse: 31.2433 - mae: 4.1671 - val_loss: 31.5006 - val_mse: 31.5006 - val_mae: 4.1687\n",
      "Epoch 727/1000\n",
      "325/325 [==============================] - 0s 991us/step - loss: 31.1931 - mse: 31.1931 - mae: 4.1620 - val_loss: 31.7541 - val_mse: 31.7541 - val_mae: 4.0984\n",
      "Epoch 728/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.1904 - mse: 31.1904 - mae: 4.1649 - val_loss: 31.5282 - val_mse: 31.5282 - val_mae: 4.1711\n",
      "Epoch 729/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.1921 - mse: 31.1921 - mae: 4.1623 - val_loss: 32.0867 - val_mse: 32.0867 - val_mae: 4.3285\n",
      "Epoch 730/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1898 - mse: 31.1898 - mae: 4.1630 - val_loss: 31.5229 - val_mse: 31.5229 - val_mae: 4.1908\n",
      "Epoch 731/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.2145 - mse: 31.2145 - mae: 4.1615 - val_loss: 31.7543 - val_mse: 31.7543 - val_mae: 4.2930\n",
      "Epoch 732/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1792 - mse: 31.1792 - mae: 4.1579 - val_loss: 31.5600 - val_mse: 31.5600 - val_mae: 4.2656\n",
      "Epoch 733/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2741 - mse: 31.2741 - mae: 4.1695 - val_loss: 31.9223 - val_mse: 31.9223 - val_mae: 4.3151\n",
      "Epoch 734/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.2562 - mse: 31.2562 - mae: 4.1682 - val_loss: 31.8354 - val_mse: 31.8354 - val_mae: 4.0667\n",
      "Epoch 735/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2238 - mse: 31.2238 - mae: 4.1640 - val_loss: 31.6323 - val_mse: 31.6323 - val_mae: 4.2909\n",
      "Epoch 736/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2301 - mse: 31.2301 - mae: 4.1661 - val_loss: 31.6807 - val_mse: 31.6807 - val_mae: 4.1067\n",
      "Epoch 737/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1790 - mse: 31.1790 - mae: 4.1607 - val_loss: 31.6726 - val_mse: 31.6726 - val_mae: 4.0920\n",
      "Epoch 738/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1427 - mse: 31.1427 - mae: 4.1598 - val_loss: 31.5909 - val_mse: 31.5909 - val_mae: 4.1276\n",
      "Epoch 739/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1911 - mse: 31.1911 - mae: 4.1584 - val_loss: 31.5951 - val_mse: 31.5951 - val_mae: 4.2189\n",
      "Epoch 740/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1821 - mse: 31.1821 - mae: 4.1646 - val_loss: 31.6525 - val_mse: 31.6525 - val_mae: 4.2510\n",
      "Epoch 741/1000\n",
      "325/325 [==============================] - 0s 982us/step - loss: 31.2574 - mse: 31.2574 - mae: 4.1648 - val_loss: 31.5758 - val_mse: 31.5758 - val_mae: 4.1521\n",
      "Epoch 742/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2235 - mse: 31.2235 - mae: 4.1630 - val_loss: 31.6158 - val_mse: 31.6158 - val_mae: 4.1931\n",
      "Epoch 743/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2169 - mse: 31.2169 - mae: 4.1618 - val_loss: 31.5098 - val_mse: 31.5098 - val_mae: 4.2498\n",
      "Epoch 744/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2508 - mse: 31.2508 - mae: 4.1659 - val_loss: 31.6196 - val_mse: 31.6196 - val_mae: 4.2161\n",
      "Epoch 745/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1676 - mse: 31.1676 - mae: 4.1589 - val_loss: 31.7449 - val_mse: 31.7449 - val_mae: 4.2299\n",
      "Epoch 746/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1946 - mse: 31.1946 - mae: 4.1610 - val_loss: 31.5060 - val_mse: 31.5060 - val_mae: 4.2125\n",
      "Epoch 747/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2101 - mse: 31.2101 - mae: 4.1668 - val_loss: 31.6821 - val_mse: 31.6821 - val_mae: 4.1404\n",
      "Epoch 748/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2382 - mse: 31.2382 - mae: 4.1631 - val_loss: 31.5342 - val_mse: 31.5342 - val_mae: 4.1963\n",
      "Epoch 749/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2055 - mse: 31.2055 - mae: 4.1643 - val_loss: 31.4672 - val_mse: 31.4672 - val_mae: 4.1898\n",
      "Epoch 750/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.1515 - mse: 31.1515 - mae: 4.1572 - val_loss: 31.5587 - val_mse: 31.5587 - val_mae: 4.1312\n",
      "Epoch 751/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1921 - mse: 31.1921 - mae: 4.1615 - val_loss: 31.6700 - val_mse: 31.6700 - val_mae: 4.2507\n",
      "Epoch 752/1000\n",
      "325/325 [==============================] - 0s 1000us/step - loss: 31.1389 - mse: 31.1389 - mae: 4.1595 - val_loss: 31.5891 - val_mse: 31.5891 - val_mae: 4.1057\n",
      "Epoch 753/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 969us/step - loss: 31.1429 - mse: 31.1429 - mae: 4.1588 - val_loss: 31.6194 - val_mse: 31.6194 - val_mae: 4.2802\n",
      "Epoch 754/1000\n",
      "325/325 [==============================] - 0s 993us/step - loss: 31.2352 - mse: 31.2352 - mae: 4.1677 - val_loss: 31.6032 - val_mse: 31.6032 - val_mae: 4.1754\n",
      "Epoch 755/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.2547 - mse: 31.2547 - mae: 4.1640 - val_loss: 31.5938 - val_mse: 31.5938 - val_mae: 4.1535\n",
      "Epoch 756/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1632 - mse: 31.1632 - mae: 4.1613 - val_loss: 31.6383 - val_mse: 31.6383 - val_mae: 4.1987\n",
      "Epoch 757/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.2225 - mse: 31.2225 - mae: 4.1638 - val_loss: 31.6338 - val_mse: 31.6338 - val_mae: 4.1011\n",
      "Epoch 758/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.1800 - mse: 31.1800 - mae: 4.1602 - val_loss: 31.5528 - val_mse: 31.5528 - val_mae: 4.1405\n",
      "Epoch 759/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1919 - mse: 31.1919 - mae: 4.1586 - val_loss: 32.3265 - val_mse: 32.3265 - val_mae: 4.4464\n",
      "Epoch 760/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.2108 - mse: 31.2108 - mae: 4.1667 - val_loss: 31.7611 - val_mse: 31.7611 - val_mae: 4.1385\n",
      "Epoch 761/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.1764 - mse: 31.1764 - mae: 4.1595 - val_loss: 31.5501 - val_mse: 31.5501 - val_mae: 4.2693\n",
      "Epoch 762/1000\n",
      "325/325 [==============================] - 0s 990us/step - loss: 31.1254 - mse: 31.1254 - mae: 4.1543 - val_loss: 31.5540 - val_mse: 31.5540 - val_mae: 4.2038\n",
      "Epoch 763/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.2083 - mse: 31.2083 - mae: 4.1640 - val_loss: 31.5951 - val_mse: 31.5951 - val_mae: 4.1217\n",
      "Epoch 764/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.1319 - mse: 31.1319 - mae: 4.1562 - val_loss: 31.5247 - val_mse: 31.5247 - val_mae: 4.1770\n",
      "Epoch 765/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.1650 - mse: 31.1650 - mae: 4.1600 - val_loss: 31.7267 - val_mse: 31.7267 - val_mae: 4.3074\n",
      "Epoch 766/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.1681 - mse: 31.1681 - mae: 4.1599 - val_loss: 31.6392 - val_mse: 31.6392 - val_mae: 4.0808\n",
      "Epoch 767/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 31.1953 - mse: 31.1953 - mae: 4.1647 - val_loss: 31.6762 - val_mse: 31.6762 - val_mae: 4.1038\n",
      "Epoch 768/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 31.2583 - mse: 31.2583 - mae: 4.1625 - val_loss: 31.7251 - val_mse: 31.7251 - val_mae: 4.0951\n",
      "Epoch 769/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 31.2875 - mse: 31.2875 - mae: 4.1691 - val_loss: 31.5487 - val_mse: 31.5487 - val_mae: 4.1342\n",
      "Epoch 770/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.1847 - mse: 31.1847 - mae: 4.1586 - val_loss: 31.7124 - val_mse: 31.7124 - val_mae: 4.2505\n",
      "Epoch 771/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.1587 - mse: 31.1587 - mae: 4.1585 - val_loss: 31.4753 - val_mse: 31.4753 - val_mae: 4.1540\n",
      "Epoch 772/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.1985 - mse: 31.1985 - mae: 4.1656 - val_loss: 31.6190 - val_mse: 31.6190 - val_mae: 4.1680\n",
      "Epoch 773/1000\n",
      "325/325 [==============================] - 0s 985us/step - loss: 31.1863 - mse: 31.1863 - mae: 4.1572 - val_loss: 31.4528 - val_mse: 31.4528 - val_mae: 4.2005\n",
      "Epoch 774/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.2068 - mse: 31.2068 - mae: 4.1669 - val_loss: 32.2101 - val_mse: 32.2101 - val_mae: 4.0721\n",
      "Epoch 775/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 31.1546 - mse: 31.1546 - mae: 4.1601 - val_loss: 31.6110 - val_mse: 31.6110 - val_mae: 4.2318\n",
      "Epoch 776/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.2079 - mse: 31.2079 - mae: 4.1589 - val_loss: 31.4351 - val_mse: 31.4351 - val_mae: 4.1956\n",
      "Epoch 777/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.2324 - mse: 31.2324 - mae: 4.1646 - val_loss: 31.5345 - val_mse: 31.5345 - val_mae: 4.2200\n",
      "Epoch 778/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.2374 - mse: 31.2374 - mae: 4.1708 - val_loss: 32.1665 - val_mse: 32.1665 - val_mae: 4.3385\n",
      "Epoch 779/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.2296 - mse: 31.2296 - mae: 4.1592 - val_loss: 31.7846 - val_mse: 31.7846 - val_mae: 4.3442\n",
      "Epoch 780/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.1746 - mse: 31.1746 - mae: 4.1604 - val_loss: 31.8575 - val_mse: 31.8575 - val_mae: 4.3342\n",
      "Epoch 781/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.1334 - mse: 31.1334 - mae: 4.1584 - val_loss: 31.8755 - val_mse: 31.8755 - val_mae: 4.0856\n",
      "Epoch 782/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.2226 - mse: 31.2226 - mae: 4.1632 - val_loss: 31.8235 - val_mse: 31.8235 - val_mae: 4.1765\n",
      "Epoch 783/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.1349 - mse: 31.1349 - mae: 4.1612 - val_loss: 31.6610 - val_mse: 31.6610 - val_mae: 4.0956\n",
      "Epoch 784/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 31.2531 - mse: 31.2531 - mae: 4.1666 - val_loss: 31.7089 - val_mse: 31.7089 - val_mae: 4.3207\n",
      "Epoch 785/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.1809 - mse: 31.1809 - mae: 4.1626 - val_loss: 31.6538 - val_mse: 31.6538 - val_mae: 4.1070\n",
      "Epoch 786/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.2249 - mse: 31.2249 - mae: 4.1661 - val_loss: 31.5959 - val_mse: 31.5959 - val_mae: 4.2101\n",
      "Epoch 787/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.2081 - mse: 31.2081 - mae: 4.1666 - val_loss: 31.6172 - val_mse: 31.6172 - val_mae: 4.2185\n",
      "Epoch 788/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.1588 - mse: 31.1588 - mae: 4.1634 - val_loss: 31.5650 - val_mse: 31.5650 - val_mae: 4.1390\n",
      "Epoch 789/1000\n",
      "325/325 [==============================] - 0s 945us/step - loss: 31.1177 - mse: 31.1177 - mae: 4.1576 - val_loss: 31.7911 - val_mse: 31.7911 - val_mae: 4.2560\n",
      "Epoch 790/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.1982 - mse: 31.1982 - mae: 4.1643 - val_loss: 31.6512 - val_mse: 31.6512 - val_mae: 4.1078\n",
      "Epoch 791/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.1073 - mse: 31.1073 - mae: 4.1522 - val_loss: 31.4982 - val_mse: 31.4982 - val_mae: 4.2164\n",
      "Epoch 792/1000\n",
      "325/325 [==============================] - 0s 990us/step - loss: 31.1346 - mse: 31.1346 - mae: 4.1592 - val_loss: 31.5954 - val_mse: 31.5954 - val_mae: 4.1439\n",
      "Epoch 793/1000\n",
      "325/325 [==============================] - 0s 951us/step - loss: 31.1810 - mse: 31.1810 - mae: 4.1607 - val_loss: 31.6007 - val_mse: 31.6007 - val_mae: 4.1321\n",
      "Epoch 794/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 31.2223 - mse: 31.2223 - mae: 4.1663 - val_loss: 31.5389 - val_mse: 31.5389 - val_mae: 4.1305\n",
      "Epoch 795/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.1666 - mse: 31.1667 - mae: 4.1584 - val_loss: 31.5039 - val_mse: 31.5039 - val_mae: 4.1871\n",
      "Epoch 796/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.1827 - mse: 31.1827 - mae: 4.1606 - val_loss: 31.5075 - val_mse: 31.5075 - val_mae: 4.1938\n",
      "Epoch 797/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.2590 - mse: 31.2590 - mae: 4.1677 - val_loss: 31.7980 - val_mse: 31.7980 - val_mae: 4.3078\n",
      "Epoch 798/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.2296 - mse: 31.2296 - mae: 4.1660 - val_loss: 31.7128 - val_mse: 31.7128 - val_mae: 4.1202\n",
      "Epoch 799/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 31.1618 - mse: 31.1618 - mae: 4.1571 - val_loss: 31.4655 - val_mse: 31.4655 - val_mae: 4.2130\n",
      "Epoch 800/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 959us/step - loss: 31.2538 - mse: 31.2538 - mae: 4.1690 - val_loss: 31.7800 - val_mse: 31.7800 - val_mae: 4.2193\n",
      "Epoch 801/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.1251 - mse: 31.1251 - mae: 4.1545 - val_loss: 31.6130 - val_mse: 31.6130 - val_mae: 4.2039\n",
      "Epoch 802/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.1649 - mse: 31.1649 - mae: 4.1603 - val_loss: 31.4578 - val_mse: 31.4578 - val_mae: 4.1897\n",
      "Epoch 803/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 31.0909 - mse: 31.0909 - mae: 4.1538 - val_loss: 31.9170 - val_mse: 31.9170 - val_mae: 4.0926\n",
      "Epoch 804/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 31.1588 - mse: 31.1588 - mae: 4.1621 - val_loss: 31.4721 - val_mse: 31.4721 - val_mae: 4.2099\n",
      "Epoch 805/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.2081 - mse: 31.2081 - mae: 4.1659 - val_loss: 31.4659 - val_mse: 31.4659 - val_mae: 4.2045\n",
      "Epoch 806/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 31.2270 - mse: 31.2270 - mae: 4.1642 - val_loss: 31.6705 - val_mse: 31.6705 - val_mae: 4.2681\n",
      "Epoch 807/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.1886 - mse: 31.1886 - mae: 4.1658 - val_loss: 31.6284 - val_mse: 31.6284 - val_mae: 4.1027\n",
      "Epoch 808/1000\n",
      "325/325 [==============================] - 0s 988us/step - loss: 31.1433 - mse: 31.1433 - mae: 4.1599 - val_loss: 31.7044 - val_mse: 31.7044 - val_mae: 4.1443\n",
      "Epoch 809/1000\n",
      "325/325 [==============================] - 0s 990us/step - loss: 31.1532 - mse: 31.1532 - mae: 4.1558 - val_loss: 31.4922 - val_mse: 31.4922 - val_mae: 4.2392\n",
      "Epoch 810/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.2052 - mse: 31.2052 - mae: 4.1658 - val_loss: 31.6048 - val_mse: 31.6048 - val_mae: 4.1335\n",
      "Epoch 811/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.1856 - mse: 31.1856 - mae: 4.1591 - val_loss: 31.9854 - val_mse: 31.9854 - val_mae: 4.3722\n",
      "Epoch 812/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.1570 - mse: 31.1570 - mae: 4.1636 - val_loss: 31.7625 - val_mse: 31.7625 - val_mae: 4.3185\n",
      "Epoch 813/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 31.1925 - mse: 31.1925 - mae: 4.1590 - val_loss: 31.5361 - val_mse: 31.5361 - val_mae: 4.1651\n",
      "Epoch 814/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.2549 - mse: 31.2549 - mae: 4.1707 - val_loss: 31.4910 - val_mse: 31.4910 - val_mae: 4.2270\n",
      "Epoch 815/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 31.1903 - mse: 31.1903 - mae: 4.1587 - val_loss: 31.5939 - val_mse: 31.5939 - val_mae: 4.1920\n",
      "Epoch 816/1000\n",
      "325/325 [==============================] - 0s 950us/step - loss: 31.1902 - mse: 31.1902 - mae: 4.1616 - val_loss: 31.6313 - val_mse: 31.6313 - val_mae: 4.1184\n",
      "Epoch 817/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.1222 - mse: 31.1222 - mae: 4.1579 - val_loss: 31.4844 - val_mse: 31.4844 - val_mae: 4.1677\n",
      "Epoch 818/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.1632 - mse: 31.1632 - mae: 4.1605 - val_loss: 31.6665 - val_mse: 31.6665 - val_mae: 4.1301\n",
      "Epoch 819/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.1506 - mse: 31.1506 - mae: 4.1582 - val_loss: 31.5962 - val_mse: 31.5962 - val_mae: 4.2322\n",
      "Epoch 820/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.1247 - mse: 31.1247 - mae: 4.1591 - val_loss: 31.5264 - val_mse: 31.5264 - val_mae: 4.2549\n",
      "Epoch 821/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.1723 - mse: 31.1723 - mae: 4.1628 - val_loss: 31.4656 - val_mse: 31.4656 - val_mae: 4.1308\n",
      "Epoch 822/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.1074 - mse: 31.1074 - mae: 4.1524 - val_loss: 31.5333 - val_mse: 31.5333 - val_mae: 4.2621\n",
      "Epoch 823/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.1502 - mse: 31.1502 - mae: 4.1601 - val_loss: 31.5325 - val_mse: 31.5325 - val_mae: 4.1783\n",
      "Epoch 824/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.2014 - mse: 31.2014 - mae: 4.1610 - val_loss: 31.9276 - val_mse: 31.9276 - val_mae: 4.3384\n",
      "Epoch 825/1000\n",
      "325/325 [==============================] - 0s 956us/step - loss: 31.1342 - mse: 31.1342 - mae: 4.1586 - val_loss: 31.6413 - val_mse: 31.6413 - val_mae: 4.2597\n",
      "Epoch 826/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.2176 - mse: 31.2176 - mae: 4.1612 - val_loss: 31.5012 - val_mse: 31.5012 - val_mae: 4.1770\n",
      "Epoch 827/1000\n",
      "325/325 [==============================] - 0s 980us/step - loss: 31.2289 - mse: 31.2289 - mae: 4.1624 - val_loss: 31.6920 - val_mse: 31.6920 - val_mae: 4.2816\n",
      "Epoch 828/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.1542 - mse: 31.1542 - mae: 4.1587 - val_loss: 32.1029 - val_mse: 32.1029 - val_mae: 4.3483\n",
      "Epoch 829/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.2856 - mse: 31.2856 - mae: 4.1659 - val_loss: 31.4860 - val_mse: 31.4860 - val_mae: 4.2039\n",
      "Epoch 830/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.1803 - mse: 31.1803 - mae: 4.1627 - val_loss: 32.3741 - val_mse: 32.3741 - val_mae: 4.3883\n",
      "Epoch 831/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.1583 - mse: 31.1583 - mae: 4.1606 - val_loss: 31.4872 - val_mse: 31.4872 - val_mae: 4.1170\n",
      "Epoch 832/1000\n",
      "325/325 [==============================] - 0s 960us/step - loss: 31.1222 - mse: 31.1222 - mae: 4.1575 - val_loss: 31.6147 - val_mse: 31.6147 - val_mae: 4.2134\n",
      "Epoch 833/1000\n",
      "325/325 [==============================] - 0s 961us/step - loss: 31.1170 - mse: 31.1170 - mae: 4.1577 - val_loss: 31.4701 - val_mse: 31.4701 - val_mae: 4.2384\n",
      "Epoch 834/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.1407 - mse: 31.1407 - mae: 4.1572 - val_loss: 31.7197 - val_mse: 31.7197 - val_mae: 4.1192\n",
      "Epoch 835/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.1728 - mse: 31.1728 - mae: 4.1614 - val_loss: 31.5230 - val_mse: 31.5230 - val_mae: 4.1054\n",
      "Epoch 836/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.1629 - mse: 31.1629 - mae: 4.1576 - val_loss: 31.5722 - val_mse: 31.5722 - val_mae: 4.2016\n",
      "Epoch 837/1000\n",
      "325/325 [==============================] - 0s 958us/step - loss: 31.1559 - mse: 31.1559 - mae: 4.1584 - val_loss: 31.9285 - val_mse: 31.9285 - val_mae: 4.3819\n",
      "Epoch 838/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.1921 - mse: 31.1921 - mae: 4.1624 - val_loss: 31.8035 - val_mse: 31.8035 - val_mae: 4.3447\n",
      "Epoch 839/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.1857 - mse: 31.1857 - mae: 4.1623 - val_loss: 31.6911 - val_mse: 31.6911 - val_mae: 4.0800\n",
      "Epoch 840/1000\n",
      "325/325 [==============================] - 0s 996us/step - loss: 31.1375 - mse: 31.1375 - mae: 4.1570 - val_loss: 31.4977 - val_mse: 31.4977 - val_mae: 4.2361\n",
      "Epoch 841/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.1608 - mse: 31.1608 - mae: 4.1564 - val_loss: 31.5790 - val_mse: 31.5790 - val_mae: 4.1787\n",
      "Epoch 842/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.1553 - mse: 31.1553 - mae: 4.1607 - val_loss: 32.3337 - val_mse: 32.3337 - val_mae: 4.0828\n",
      "Epoch 843/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.1642 - mse: 31.1642 - mae: 4.1600 - val_loss: 31.4297 - val_mse: 31.4297 - val_mae: 4.1918\n",
      "Epoch 844/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.1470 - mse: 31.1470 - mae: 4.1595 - val_loss: 31.5639 - val_mse: 31.5639 - val_mae: 4.1157\n",
      "Epoch 845/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.1645 - mse: 31.1645 - mae: 4.1554 - val_loss: 31.5827 - val_mse: 31.5827 - val_mae: 4.1301\n",
      "Epoch 846/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.1811 - mse: 31.1811 - mae: 4.1633 - val_loss: 31.5004 - val_mse: 31.5004 - val_mae: 4.1625\n",
      "Epoch 847/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 973us/step - loss: 31.1615 - mse: 31.1615 - mae: 4.1584 - val_loss: 31.8648 - val_mse: 31.8648 - val_mae: 4.0973\n",
      "Epoch 848/1000\n",
      "325/325 [==============================] - 0s 959us/step - loss: 31.1834 - mse: 31.1834 - mae: 4.1621 - val_loss: 31.8450 - val_mse: 31.8450 - val_mae: 4.3018\n",
      "Epoch 849/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.2929 - mse: 31.2929 - mae: 4.1700 - val_loss: 31.4591 - val_mse: 31.4591 - val_mae: 4.1844\n",
      "Epoch 850/1000\n",
      "325/325 [==============================] - 0s 955us/step - loss: 31.1502 - mse: 31.1502 - mae: 4.1592 - val_loss: 31.5690 - val_mse: 31.5690 - val_mae: 4.0954\n",
      "Epoch 851/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.1941 - mse: 31.1941 - mae: 4.1605 - val_loss: 31.5768 - val_mse: 31.5768 - val_mae: 4.1555\n",
      "Epoch 852/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.1506 - mse: 31.1506 - mae: 4.1591 - val_loss: 31.8749 - val_mse: 31.8749 - val_mae: 4.3635\n",
      "Epoch 853/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.1999 - mse: 31.1999 - mae: 4.1640 - val_loss: 31.8358 - val_mse: 31.8358 - val_mae: 4.3595\n",
      "Epoch 854/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.1946 - mse: 31.1946 - mae: 4.1596 - val_loss: 31.5602 - val_mse: 31.5602 - val_mae: 4.1045\n",
      "Epoch 855/1000\n",
      "325/325 [==============================] - 0s 962us/step - loss: 31.1755 - mse: 31.1755 - mae: 4.1626 - val_loss: 31.5820 - val_mse: 31.5820 - val_mae: 4.2826\n",
      "Epoch 856/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.1286 - mse: 31.1286 - mae: 4.1615 - val_loss: 31.4997 - val_mse: 31.4997 - val_mae: 4.2515\n",
      "Epoch 857/1000\n",
      "325/325 [==============================] - 0s 996us/step - loss: 31.1463 - mse: 31.1463 - mae: 4.1603 - val_loss: 31.7031 - val_mse: 31.7031 - val_mae: 4.0953\n",
      "Epoch 858/1000\n",
      "325/325 [==============================] - 0s 991us/step - loss: 31.1302 - mse: 31.1302 - mae: 4.1573 - val_loss: 31.6544 - val_mse: 31.6544 - val_mae: 4.2649\n",
      "Epoch 859/1000\n",
      "325/325 [==============================] - 0s 983us/step - loss: 31.1561 - mse: 31.1561 - mae: 4.1628 - val_loss: 31.6070 - val_mse: 31.6070 - val_mae: 4.2451\n",
      "Epoch 860/1000\n",
      "325/325 [==============================] - 0s 990us/step - loss: 31.1160 - mse: 31.1160 - mae: 4.1561 - val_loss: 31.5404 - val_mse: 31.5404 - val_mae: 4.2649\n",
      "Epoch 861/1000\n",
      "325/325 [==============================] - 0s 993us/step - loss: 31.1299 - mse: 31.1299 - mae: 4.1563 - val_loss: 31.6814 - val_mse: 31.6814 - val_mae: 4.1126\n",
      "Epoch 862/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.1427 - mse: 31.1427 - mae: 4.1614 - val_loss: 31.8204 - val_mse: 31.8204 - val_mae: 4.2564\n",
      "Epoch 863/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.1867 - mse: 31.1867 - mae: 4.1616 - val_loss: 31.6111 - val_mse: 31.6111 - val_mae: 4.0952\n",
      "Epoch 864/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.1887 - mse: 31.1887 - mae: 4.1600 - val_loss: 31.5649 - val_mse: 31.5649 - val_mae: 4.2354\n",
      "Epoch 865/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.1576 - mse: 31.1576 - mae: 4.1635 - val_loss: 31.7282 - val_mse: 31.7282 - val_mae: 4.3124\n",
      "Epoch 866/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.1360 - mse: 31.1360 - mae: 4.1557 - val_loss: 31.4340 - val_mse: 31.4340 - val_mae: 4.1577\n",
      "Epoch 867/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.1058 - mse: 31.1058 - mae: 4.1583 - val_loss: 31.4264 - val_mse: 31.4264 - val_mae: 4.2010\n",
      "Epoch 868/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1617 - mse: 31.1617 - mae: 4.1586 - val_loss: 31.7954 - val_mse: 31.7954 - val_mae: 4.0792\n",
      "Epoch 869/1000\n",
      "325/325 [==============================] - 0s 999us/step - loss: 31.2050 - mse: 31.2050 - mae: 4.1640 - val_loss: 31.6514 - val_mse: 31.6514 - val_mae: 4.1429\n",
      "Epoch 870/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1694 - mse: 31.1694 - mae: 4.1575 - val_loss: 31.5154 - val_mse: 31.5154 - val_mae: 4.1247\n",
      "Epoch 871/1000\n",
      "325/325 [==============================] - 0s 996us/step - loss: 31.1287 - mse: 31.1287 - mae: 4.1567 - val_loss: 31.4716 - val_mse: 31.4716 - val_mae: 4.1568\n",
      "Epoch 872/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.1985 - mse: 31.1985 - mae: 4.1651 - val_loss: 31.4244 - val_mse: 31.4244 - val_mae: 4.2098\n",
      "Epoch 873/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.1588 - mse: 31.1588 - mae: 4.1613 - val_loss: 31.5797 - val_mse: 31.5797 - val_mae: 4.2329\n",
      "Epoch 874/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.0666 - mse: 31.0666 - mae: 4.1521 - val_loss: 31.4455 - val_mse: 31.4455 - val_mae: 4.1924\n",
      "Epoch 875/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.1255 - mse: 31.1255 - mae: 4.1554 - val_loss: 31.5371 - val_mse: 31.5371 - val_mae: 4.1882\n",
      "Epoch 876/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.1727 - mse: 31.1727 - mae: 4.1585 - val_loss: 31.5237 - val_mse: 31.5237 - val_mae: 4.2395\n",
      "Epoch 877/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.1537 - mse: 31.1537 - mae: 4.1587 - val_loss: 31.4835 - val_mse: 31.4835 - val_mae: 4.1367\n",
      "Epoch 878/1000\n",
      "325/325 [==============================] - 0s 980us/step - loss: 31.2281 - mse: 31.2281 - mae: 4.1653 - val_loss: 31.5572 - val_mse: 31.5572 - val_mae: 4.1645\n",
      "Epoch 879/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1217 - mse: 31.1217 - mae: 4.1573 - val_loss: 31.6778 - val_mse: 31.6778 - val_mae: 4.2924\n",
      "Epoch 880/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1414 - mse: 31.1414 - mae: 4.1576 - val_loss: 31.4996 - val_mse: 31.4996 - val_mae: 4.1709\n",
      "Epoch 881/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1472 - mse: 31.1472 - mae: 4.1574 - val_loss: 31.7331 - val_mse: 31.7331 - val_mae: 4.1271\n",
      "Epoch 882/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1410 - mse: 31.1410 - mae: 4.1592 - val_loss: 31.5348 - val_mse: 31.5348 - val_mae: 4.1506\n",
      "Epoch 883/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1482 - mse: 31.1482 - mae: 4.1588 - val_loss: 31.7423 - val_mse: 31.7423 - val_mae: 4.0919\n",
      "Epoch 884/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1112 - mse: 31.1112 - mae: 4.1583 - val_loss: 31.9046 - val_mse: 31.9046 - val_mae: 4.3468\n",
      "Epoch 885/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1749 - mse: 31.1749 - mae: 4.1619 - val_loss: 31.6850 - val_mse: 31.6850 - val_mae: 4.2059\n",
      "Epoch 886/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1090 - mse: 31.1090 - mae: 4.1540 - val_loss: 31.5600 - val_mse: 31.5600 - val_mae: 4.2267\n",
      "Epoch 887/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1227 - mse: 31.1227 - mae: 4.1604 - val_loss: 31.6417 - val_mse: 31.6417 - val_mae: 4.2087\n",
      "Epoch 888/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1802 - mse: 31.1802 - mae: 4.1649 - val_loss: 31.7740 - val_mse: 31.7740 - val_mae: 4.2895\n",
      "Epoch 889/1000\n",
      "325/325 [==============================] - 0s 983us/step - loss: 31.0977 - mse: 31.0977 - mae: 4.1556 - val_loss: 31.5464 - val_mse: 31.5464 - val_mae: 4.1948\n",
      "Epoch 890/1000\n",
      "325/325 [==============================] - 0s 994us/step - loss: 31.1393 - mse: 31.1393 - mae: 4.1603 - val_loss: 31.5489 - val_mse: 31.5489 - val_mae: 4.1315\n",
      "Epoch 891/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1355 - mse: 31.1355 - mae: 4.1576 - val_loss: 31.9352 - val_mse: 31.9352 - val_mae: 4.2971\n",
      "Epoch 892/1000\n",
      "325/325 [==============================] - 0s 996us/step - loss: 31.1806 - mse: 31.1806 - mae: 4.1636 - val_loss: 31.4404 - val_mse: 31.4404 - val_mae: 4.1703\n",
      "Epoch 893/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1198 - mse: 31.1198 - mae: 4.1565 - val_loss: 31.7412 - val_mse: 31.7412 - val_mae: 4.1336\n",
      "Epoch 894/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 994us/step - loss: 31.1457 - mse: 31.1457 - mae: 4.1577 - val_loss: 31.7697 - val_mse: 31.7697 - val_mae: 4.2441\n",
      "Epoch 895/1000\n",
      "325/325 [==============================] - 0s 992us/step - loss: 31.1361 - mse: 31.1361 - mae: 4.1590 - val_loss: 31.4661 - val_mse: 31.4661 - val_mae: 4.1514\n",
      "Epoch 896/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.1226 - mse: 31.1226 - mae: 4.1572 - val_loss: 31.5549 - val_mse: 31.5549 - val_mae: 4.1147\n",
      "Epoch 897/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1601 - mse: 31.1601 - mae: 4.1575 - val_loss: 31.5650 - val_mse: 31.5650 - val_mae: 4.1935\n",
      "Epoch 898/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.0956 - mse: 31.0956 - mae: 4.1556 - val_loss: 31.4834 - val_mse: 31.4834 - val_mae: 4.1555\n",
      "Epoch 899/1000\n",
      "325/325 [==============================] - 0s 989us/step - loss: 31.1388 - mse: 31.1388 - mae: 4.1579 - val_loss: 31.5114 - val_mse: 31.5114 - val_mae: 4.2051\n",
      "Epoch 900/1000\n",
      "325/325 [==============================] - 0s 989us/step - loss: 31.1653 - mse: 31.1653 - mae: 4.1578 - val_loss: 31.8207 - val_mse: 31.8207 - val_mae: 4.2476\n",
      "Epoch 901/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.1423 - mse: 31.1423 - mae: 4.1592 - val_loss: 31.7177 - val_mse: 31.7177 - val_mae: 4.2636\n",
      "Epoch 902/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.1242 - mse: 31.1242 - mae: 4.1603 - val_loss: 31.6691 - val_mse: 31.6691 - val_mae: 4.2402\n",
      "Epoch 903/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1362 - mse: 31.1362 - mae: 4.1570 - val_loss: 31.4181 - val_mse: 31.4181 - val_mae: 4.1757\n",
      "Epoch 904/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.3273 - mse: 31.3273 - mae: 4.1684 - val_loss: 31.4351 - val_mse: 31.4351 - val_mae: 4.2253\n",
      "Epoch 905/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.1581 - mse: 31.1581 - mae: 4.1591 - val_loss: 31.8022 - val_mse: 31.8022 - val_mae: 4.2150\n",
      "Epoch 906/1000\n",
      "325/325 [==============================] - 0s 991us/step - loss: 31.1516 - mse: 31.1516 - mae: 4.1615 - val_loss: 31.4126 - val_mse: 31.4126 - val_mae: 4.2042\n",
      "Epoch 907/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.1755 - mse: 31.1755 - mae: 4.1634 - val_loss: 31.5969 - val_mse: 31.5969 - val_mae: 4.0835\n",
      "Epoch 908/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1521 - mse: 31.1521 - mae: 4.1585 - val_loss: 31.4763 - val_mse: 31.4763 - val_mae: 4.1906\n",
      "Epoch 909/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1331 - mse: 31.1331 - mae: 4.1551 - val_loss: 31.6087 - val_mse: 31.6087 - val_mae: 4.2524\n",
      "Epoch 910/1000\n",
      "325/325 [==============================] - 0s 988us/step - loss: 31.1350 - mse: 31.1350 - mae: 4.1571 - val_loss: 31.4441 - val_mse: 31.4441 - val_mae: 4.1448\n",
      "Epoch 911/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.1952 - mse: 31.1952 - mae: 4.1650 - val_loss: 31.7089 - val_mse: 31.7089 - val_mae: 4.2981\n",
      "Epoch 912/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.1424 - mse: 31.1424 - mae: 4.1596 - val_loss: 31.4325 - val_mse: 31.4325 - val_mae: 4.1877\n",
      "Epoch 913/1000\n",
      "325/325 [==============================] - 0s 987us/step - loss: 31.1938 - mse: 31.1938 - mae: 4.1593 - val_loss: 31.7337 - val_mse: 31.7337 - val_mae: 4.3397\n",
      "Epoch 914/1000\n",
      "325/325 [==============================] - 0s 990us/step - loss: 31.1321 - mse: 31.1321 - mae: 4.1617 - val_loss: 31.8984 - val_mse: 31.8984 - val_mae: 4.3463\n",
      "Epoch 915/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1372 - mse: 31.1372 - mae: 4.1589 - val_loss: 31.7832 - val_mse: 31.7832 - val_mae: 4.1223\n",
      "Epoch 916/1000\n",
      "325/325 [==============================] - 0s 989us/step - loss: 31.2021 - mse: 31.2021 - mae: 4.1572 - val_loss: 31.4758 - val_mse: 31.4758 - val_mae: 4.1479\n",
      "Epoch 917/1000\n",
      "325/325 [==============================] - 0s 991us/step - loss: 31.1457 - mse: 31.1457 - mae: 4.1615 - val_loss: 32.3930 - val_mse: 32.3930 - val_mae: 4.3394\n",
      "Epoch 918/1000\n",
      "325/325 [==============================] - 0s 997us/step - loss: 31.2006 - mse: 31.2006 - mae: 4.1637 - val_loss: 31.4731 - val_mse: 31.4731 - val_mae: 4.2143\n",
      "Epoch 919/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1327 - mse: 31.1327 - mae: 4.1563 - val_loss: 31.6033 - val_mse: 31.6033 - val_mae: 4.3082\n",
      "Epoch 920/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.1448 - mse: 31.1448 - mae: 4.1568 - val_loss: 31.4560 - val_mse: 31.4560 - val_mae: 4.1982\n",
      "Epoch 921/1000\n",
      "325/325 [==============================] - 0s 985us/step - loss: 31.0650 - mse: 31.0650 - mae: 4.1528 - val_loss: 31.4746 - val_mse: 31.4746 - val_mae: 4.2445\n",
      "Epoch 922/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.1307 - mse: 31.1307 - mae: 4.1601 - val_loss: 31.4943 - val_mse: 31.4943 - val_mae: 4.1031\n",
      "Epoch 923/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1652 - mse: 31.1652 - mae: 4.1547 - val_loss: 31.5805 - val_mse: 31.5805 - val_mae: 4.2137\n",
      "Epoch 924/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1383 - mse: 31.1383 - mae: 4.1559 - val_loss: 31.7777 - val_mse: 31.7777 - val_mae: 4.3435\n",
      "Epoch 925/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.1300 - mse: 31.1300 - mae: 4.1590 - val_loss: 31.6891 - val_mse: 31.6891 - val_mae: 4.0943\n",
      "Epoch 926/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.1114 - mse: 31.1114 - mae: 4.1522 - val_loss: 32.2104 - val_mse: 32.2104 - val_mae: 4.4331\n",
      "Epoch 927/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.1987 - mse: 31.1987 - mae: 4.1649 - val_loss: 31.5020 - val_mse: 31.5020 - val_mae: 4.1208\n",
      "Epoch 928/1000\n",
      "325/325 [==============================] - 0s 996us/step - loss: 31.1416 - mse: 31.1416 - mae: 4.1574 - val_loss: 31.5121 - val_mse: 31.5121 - val_mae: 4.1425\n",
      "Epoch 929/1000\n",
      "325/325 [==============================] - 0s 999us/step - loss: 31.1128 - mse: 31.1128 - mae: 4.1556 - val_loss: 31.4565 - val_mse: 31.4565 - val_mae: 4.2202\n",
      "Epoch 930/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.1432 - mse: 31.1432 - mae: 4.1594 - val_loss: 31.7468 - val_mse: 31.7468 - val_mae: 4.1807\n",
      "Epoch 931/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.0983 - mse: 31.0983 - mae: 4.1531 - val_loss: 31.5388 - val_mse: 31.5388 - val_mae: 4.1375\n",
      "Epoch 932/1000\n",
      "325/325 [==============================] - 0s 983us/step - loss: 31.1322 - mse: 31.1322 - mae: 4.1595 - val_loss: 31.4556 - val_mse: 31.4556 - val_mae: 4.1841\n",
      "Epoch 933/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.1671 - mse: 31.1671 - mae: 4.1577 - val_loss: 31.6232 - val_mse: 31.6232 - val_mae: 4.2123\n",
      "Epoch 934/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 31.1278 - mse: 31.1278 - mae: 4.1619 - val_loss: 31.6041 - val_mse: 31.6041 - val_mae: 4.1566\n",
      "Epoch 935/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.1428 - mse: 31.1428 - mae: 4.1584 - val_loss: 31.6135 - val_mse: 31.6135 - val_mae: 4.1790\n",
      "Epoch 936/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.1779 - mse: 31.1779 - mae: 4.1602 - val_loss: 31.4934 - val_mse: 31.4934 - val_mae: 4.1785\n",
      "Epoch 937/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.2141 - mse: 31.2141 - mae: 4.1612 - val_loss: 31.5379 - val_mse: 31.5379 - val_mae: 4.1016\n",
      "Epoch 938/1000\n",
      "325/325 [==============================] - 0s 990us/step - loss: 31.1176 - mse: 31.1176 - mae: 4.1568 - val_loss: 31.7834 - val_mse: 31.7834 - val_mae: 4.0522\n",
      "Epoch 939/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.1538 - mse: 31.1538 - mae: 4.1602 - val_loss: 31.6055 - val_mse: 31.6055 - val_mae: 4.0982\n",
      "Epoch 940/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.1176 - mse: 31.1176 - mae: 4.1541 - val_loss: 31.5040 - val_mse: 31.5040 - val_mae: 4.2170\n",
      "Epoch 941/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 972us/step - loss: 31.1324 - mse: 31.1324 - mae: 4.1572 - val_loss: 31.5433 - val_mse: 31.5433 - val_mae: 4.1375\n",
      "Epoch 942/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.1011 - mse: 31.1011 - mae: 4.1566 - val_loss: 31.4637 - val_mse: 31.4637 - val_mae: 4.2408\n",
      "Epoch 943/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.0850 - mse: 31.0850 - mae: 4.1581 - val_loss: 31.5323 - val_mse: 31.5323 - val_mae: 4.1957\n",
      "Epoch 944/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.0904 - mse: 31.0904 - mae: 4.1535 - val_loss: 31.4935 - val_mse: 31.4935 - val_mae: 4.2093\n",
      "Epoch 945/1000\n",
      "325/325 [==============================] - 0s 952us/step - loss: 31.1572 - mse: 31.1572 - mae: 4.1575 - val_loss: 31.6119 - val_mse: 31.6119 - val_mae: 4.2871\n",
      "Epoch 946/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.1787 - mse: 31.1787 - mae: 4.1605 - val_loss: 31.8124 - val_mse: 31.8124 - val_mae: 4.3068\n",
      "Epoch 947/1000\n",
      "325/325 [==============================] - 0s 998us/step - loss: 31.2056 - mse: 31.2056 - mae: 4.1616 - val_loss: 31.6595 - val_mse: 31.6595 - val_mae: 4.3190\n",
      "Epoch 948/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.1729 - mse: 31.1729 - mae: 4.1602 - val_loss: 31.4744 - val_mse: 31.4744 - val_mae: 4.1987\n",
      "Epoch 949/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.0806 - mse: 31.0806 - mae: 4.1573 - val_loss: 31.8139 - val_mse: 31.8139 - val_mae: 4.0609\n",
      "Epoch 950/1000\n",
      "325/325 [==============================] - 0s 967us/step - loss: 31.0953 - mse: 31.0953 - mae: 4.1520 - val_loss: 31.9619 - val_mse: 31.9619 - val_mae: 4.3810\n",
      "Epoch 951/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.1326 - mse: 31.1326 - mae: 4.1586 - val_loss: 31.5444 - val_mse: 31.5444 - val_mae: 4.2615\n",
      "Epoch 952/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.1146 - mse: 31.1146 - mae: 4.1571 - val_loss: 31.4288 - val_mse: 31.4288 - val_mae: 4.1909\n",
      "Epoch 953/1000\n",
      "325/325 [==============================] - 0s 989us/step - loss: 31.1204 - mse: 31.1204 - mae: 4.1576 - val_loss: 31.5809 - val_mse: 31.5809 - val_mae: 4.1259\n",
      "Epoch 954/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.1767 - mse: 31.1767 - mae: 4.1611 - val_loss: 31.4143 - val_mse: 31.4143 - val_mae: 4.2097\n",
      "Epoch 955/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.1188 - mse: 31.1188 - mae: 4.1554 - val_loss: 31.5190 - val_mse: 31.5190 - val_mae: 4.1329\n",
      "Epoch 956/1000\n",
      "325/325 [==============================] - 0s 980us/step - loss: 31.1331 - mse: 31.1331 - mae: 4.1557 - val_loss: 31.5118 - val_mse: 31.5118 - val_mae: 4.1847\n",
      "Epoch 957/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.1318 - mse: 31.1318 - mae: 4.1594 - val_loss: 31.6816 - val_mse: 31.6816 - val_mae: 4.0812\n",
      "Epoch 958/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.1255 - mse: 31.1255 - mae: 4.1581 - val_loss: 31.6462 - val_mse: 31.6462 - val_mae: 4.1702\n",
      "Epoch 959/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.1336 - mse: 31.1336 - mae: 4.1607 - val_loss: 31.4919 - val_mse: 31.4919 - val_mae: 4.1485\n",
      "Epoch 960/1000\n",
      "325/325 [==============================] - 0s 995us/step - loss: 31.0913 - mse: 31.0913 - mae: 4.1558 - val_loss: 31.5345 - val_mse: 31.5345 - val_mae: 4.1055\n",
      "Epoch 961/1000\n",
      "325/325 [==============================] - 0s 963us/step - loss: 31.1397 - mse: 31.1397 - mae: 4.1538 - val_loss: 31.5254 - val_mse: 31.5254 - val_mae: 4.1559\n",
      "Epoch 962/1000\n",
      "325/325 [==============================] - 0s 974us/step - loss: 31.1143 - mse: 31.1143 - mae: 4.1541 - val_loss: 31.4604 - val_mse: 31.4604 - val_mae: 4.1415\n",
      "Epoch 963/1000\n",
      "325/325 [==============================] - 0s 983us/step - loss: 31.1793 - mse: 31.1793 - mae: 4.1613 - val_loss: 31.4430 - val_mse: 31.4430 - val_mae: 4.1508\n",
      "Epoch 964/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.1279 - mse: 31.1279 - mae: 4.1588 - val_loss: 31.4955 - val_mse: 31.4955 - val_mae: 4.1113\n",
      "Epoch 965/1000\n",
      "325/325 [==============================] - 0s 985us/step - loss: 31.1198 - mse: 31.1198 - mae: 4.1561 - val_loss: 31.5440 - val_mse: 31.5440 - val_mae: 4.1613\n",
      "Epoch 966/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.1205 - mse: 31.1205 - mae: 4.1553 - val_loss: 31.8503 - val_mse: 31.8503 - val_mae: 4.1289\n",
      "Epoch 967/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.1344 - mse: 31.1344 - mae: 4.1584 - val_loss: 31.5503 - val_mse: 31.5503 - val_mae: 4.1497\n",
      "Epoch 968/1000\n",
      "325/325 [==============================] - 0s 973us/step - loss: 31.1243 - mse: 31.1243 - mae: 4.1603 - val_loss: 31.4129 - val_mse: 31.4129 - val_mae: 4.1945\n",
      "Epoch 969/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.1059 - mse: 31.1059 - mae: 4.1531 - val_loss: 31.4519 - val_mse: 31.4519 - val_mae: 4.1409\n",
      "Epoch 970/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.1313 - mse: 31.1313 - mae: 4.1594 - val_loss: 31.5283 - val_mse: 31.5283 - val_mae: 4.2220\n",
      "Epoch 971/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.1143 - mse: 31.1143 - mae: 4.1553 - val_loss: 31.4695 - val_mse: 31.4695 - val_mae: 4.1456\n",
      "Epoch 972/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.1100 - mse: 31.1100 - mae: 4.1560 - val_loss: 31.6050 - val_mse: 31.6050 - val_mae: 4.1587\n",
      "Epoch 973/1000\n",
      "325/325 [==============================] - 0s 966us/step - loss: 31.1313 - mse: 31.1313 - mae: 4.1582 - val_loss: 31.4750 - val_mse: 31.4750 - val_mae: 4.1918\n",
      "Epoch 974/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.1275 - mse: 31.1275 - mae: 4.1575 - val_loss: 31.5222 - val_mse: 31.5222 - val_mae: 4.2117\n",
      "Epoch 975/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.0978 - mse: 31.0978 - mae: 4.1569 - val_loss: 31.5357 - val_mse: 31.5357 - val_mae: 4.1474\n",
      "Epoch 976/1000\n",
      "325/325 [==============================] - 0s 978us/step - loss: 31.1128 - mse: 31.1128 - mae: 4.1567 - val_loss: 31.5525 - val_mse: 31.5525 - val_mae: 4.2460\n",
      "Epoch 977/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.1124 - mse: 31.1124 - mae: 4.1548 - val_loss: 31.4954 - val_mse: 31.4954 - val_mae: 4.2290\n",
      "Epoch 978/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.0884 - mse: 31.0884 - mae: 4.1540 - val_loss: 31.4778 - val_mse: 31.4778 - val_mae: 4.1098\n",
      "Epoch 979/1000\n",
      "325/325 [==============================] - 0s 1ms/step - loss: 31.1026 - mse: 31.1026 - mae: 4.1545 - val_loss: 31.5551 - val_mse: 31.5550 - val_mae: 4.2330\n",
      "Epoch 980/1000\n",
      "325/325 [==============================] - 0s 953us/step - loss: 31.1282 - mse: 31.1282 - mae: 4.1609 - val_loss: 31.5366 - val_mse: 31.5366 - val_mae: 4.1113\n",
      "Epoch 981/1000\n",
      "325/325 [==============================] - 0s 971us/step - loss: 31.0663 - mse: 31.0663 - mae: 4.1494 - val_loss: 31.4129 - val_mse: 31.4129 - val_mae: 4.2077\n",
      "Epoch 982/1000\n",
      "325/325 [==============================] - 0s 964us/step - loss: 31.1007 - mse: 31.1007 - mae: 4.1552 - val_loss: 31.6596 - val_mse: 31.6596 - val_mae: 4.3149\n",
      "Epoch 983/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.0757 - mse: 31.0757 - mae: 4.1535 - val_loss: 31.5240 - val_mse: 31.5240 - val_mae: 4.1741\n",
      "Epoch 984/1000\n",
      "325/325 [==============================] - 0s 968us/step - loss: 31.1787 - mse: 31.1787 - mae: 4.1631 - val_loss: 31.6407 - val_mse: 31.6407 - val_mae: 4.2925\n",
      "Epoch 985/1000\n",
      "325/325 [==============================] - 0s 969us/step - loss: 31.1738 - mse: 31.1738 - mae: 4.1610 - val_loss: 31.4943 - val_mse: 31.4943 - val_mae: 4.1923\n",
      "Epoch 986/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.0541 - mse: 31.0541 - mae: 4.1521 - val_loss: 31.4226 - val_mse: 31.4226 - val_mae: 4.2196\n",
      "Epoch 987/1000\n",
      "325/325 [==============================] - 0s 992us/step - loss: 31.0634 - mse: 31.0634 - mae: 4.1525 - val_loss: 31.5412 - val_mse: 31.5412 - val_mae: 4.1005\n",
      "Epoch 988/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 0s 987us/step - loss: 31.1461 - mse: 31.1461 - mae: 4.1634 - val_loss: 31.6729 - val_mse: 31.6729 - val_mae: 4.2919\n",
      "Epoch 989/1000\n",
      "325/325 [==============================] - 0s 970us/step - loss: 31.1098 - mse: 31.1098 - mae: 4.1547 - val_loss: 31.3780 - val_mse: 31.3780 - val_mae: 4.1789\n",
      "Epoch 990/1000\n",
      "325/325 [==============================] - 0s 979us/step - loss: 31.0946 - mse: 31.0946 - mae: 4.1559 - val_loss: 31.4245 - val_mse: 31.4245 - val_mae: 4.1572\n",
      "Epoch 991/1000\n",
      "325/325 [==============================] - 0s 965us/step - loss: 31.1672 - mse: 31.1672 - mae: 4.1569 - val_loss: 31.6098 - val_mse: 31.6098 - val_mae: 4.2682\n",
      "Epoch 992/1000\n",
      "325/325 [==============================] - 0s 982us/step - loss: 31.1082 - mse: 31.1082 - mae: 4.1578 - val_loss: 31.5646 - val_mse: 31.5646 - val_mae: 4.0790\n",
      "Epoch 993/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.0946 - mse: 31.0946 - mae: 4.1499 - val_loss: 31.5240 - val_mse: 31.5240 - val_mae: 4.2412\n",
      "Epoch 994/1000\n",
      "325/325 [==============================] - 0s 977us/step - loss: 31.0770 - mse: 31.0770 - mae: 4.1559 - val_loss: 31.4752 - val_mse: 31.4752 - val_mae: 4.2033\n",
      "Epoch 995/1000\n",
      "325/325 [==============================] - 0s 981us/step - loss: 31.1298 - mse: 31.1298 - mae: 4.1575 - val_loss: 31.4657 - val_mse: 31.4657 - val_mae: 4.1940\n",
      "Epoch 996/1000\n",
      "325/325 [==============================] - 0s 984us/step - loss: 31.1347 - mse: 31.1347 - mae: 4.1549 - val_loss: 31.7928 - val_mse: 31.7928 - val_mae: 4.2719\n",
      "Epoch 997/1000\n",
      "325/325 [==============================] - 0s 972us/step - loss: 31.1286 - mse: 31.1286 - mae: 4.1618 - val_loss: 31.5968 - val_mse: 31.5968 - val_mae: 4.0834\n",
      "Epoch 998/1000\n",
      "325/325 [==============================] - 0s 975us/step - loss: 31.1530 - mse: 31.1530 - mae: 4.1547 - val_loss: 31.6035 - val_mse: 31.6035 - val_mae: 4.2250\n",
      "Epoch 999/1000\n",
      "325/325 [==============================] - 0s 986us/step - loss: 31.1106 - mse: 31.1106 - mae: 4.1575 - val_loss: 31.4454 - val_mse: 31.4454 - val_mae: 4.1650\n",
      "Epoch 1000/1000\n",
      "325/325 [==============================] - 0s 976us/step - loss: 31.1665 - mse: 31.1665 - mae: 4.1581 - val_loss: 31.5373 - val_mse: 31.5373 - val_mae: 4.2411\n"
     ]
    }
   ],
   "source": [
    "'''set here if you want to use re-scaled fetures or just original features, set x_train and y_train\n",
    "'''\n",
    "train_history = built_model.fit(\n",
    "                                x_train,\n",
    "                                y_train,\n",
    "                                epochs=1000,\n",
    "                                verbose=1,\n",
    "                                batch_size=100,\n",
    "                                validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f4a8f",
   "metadata": {},
   "source": [
    "## plot loss of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58d8c4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''a visual test to check for model overfitting (if validation departs from training==train decrease and validation starts to increase)\n",
    "'''\n",
    "print(train_history.history.keys())\n",
    "# \"Loss\" val==validation==test\n",
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbc343",
   "metadata": {},
   "source": [
    "## Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de123630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' set here if using re-scaled features\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' set here if using re-scaled features\n",
    "'''\n",
    "# y_pred_scaled = built_model.predict(x_test_scaled, verbose=1)  # Generates output predictions for the input samples\n",
    "\n",
    "## inverse y-pred to original scale\n",
    "# y_pred = scaler_y.inverse_transform(y_pred_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9266bf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544/544 [==============================] - 0s 491us/step\n",
      "RMSE predict= 5.64 (cm roughness)\"\n"
     ]
    }
   ],
   "source": [
    "''' set here if using not-scaled features\n",
    "'''\n",
    "y_pred = built_model.predict(x_test, verbose=1)  # Generates output predictions for the input samples\n",
    "\n",
    "\n",
    "'''compare y & y-hat in original scale\n",
    "''' \n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "rmse_pred_test = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('RMSE predict= %s (cm roughness)\"' %round(rmse_pred_test, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754d7d5",
   "metadata": {},
   "source": [
    "### Testing the NN model and report error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06767a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 0s 562us/step - loss: 31.7993 - mse: 31.7993 - mae: 4.2468\n",
      "RMSE evaluate: 5.64 (cm roughness)\n",
      "MAE evaluate: 4.25 (cm roughness)\n"
     ]
    }
   ],
   "source": [
    "'''Returns the loss value & metrics values for the model in test mode. similar to test? or predict?\n",
    "'''\n",
    "\n",
    "eval_metrics = built_model.evaluate(x_test, y_test, batch_size=20, verbose=1) # Returns the loss value & metrics values for the model in test mode\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "# print(eval_metrics)\n",
    "\n",
    "mse_eval = eval_metrics[1]\n",
    "mae_eval = eval_metrics[2]\n",
    "\n",
    "print(\"RMSE evaluate: %.2f (cm roughness)\" %math.sqrt(mse_eval))\n",
    "print(\"MAE evaluate: %.2f (cm roughness)\" %(mae_eval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c5c91",
   "metadata": {},
   "source": [
    "## Save the NN model???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f1ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b54a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ed33ef",
   "metadata": {},
   "source": [
    "### Training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "272451a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_history.history.keys())\n",
    "# print('\\n')\n",
    "\n",
    "# # print(\"-> model: %s\" % train_model)\n",
    "# print(\"Training Loss: %.2f (units?)\" % train_history.history['loss'][-1])  # will return the loss in the last training epoch\n",
    "# print(\"Training MSE: %.2f (cm^2 roughness)\" % (train_history.history['mse'][-1]))\n",
    "# print(\"Training MAE: %.2f (cm roughness)\" % (train_history.history['mae'][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d8124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd661ad",
   "metadata": {},
   "source": [
    "### predict on test and scale back resutls and report them\n",
    "\n",
    "when to use this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b2b63",
   "metadata": {},
   "source": [
    "### save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8382aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model and architecture to single file\n",
    "# model_name = \"trained_model_3L9N_50epoch.h5\"  # find a way to rename this online\n",
    "\n",
    "# model_fp = os.path.join(dataset_dir, model_name) \n",
    "# mlp_model.save(model_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
