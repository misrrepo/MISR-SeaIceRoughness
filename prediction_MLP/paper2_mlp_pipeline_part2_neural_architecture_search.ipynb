{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dbc7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- main machine learning libraries used\n",
    "\n",
    "# import tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras  \n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#- other libraries used\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from platform import python_version\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0b69f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.12'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02be279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest run on: 2022-06-14 17:25:33.550040\n"
     ]
    }
   ],
   "source": [
    "today = dt.today()\n",
    "print('latest run on:', today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30362b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 6, 14, 17, 25, 33, 555249)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now1 = dt.now()\n",
    "now1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcfe260",
   "metadata": {},
   "source": [
    "## check tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57a7442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb3750d",
   "metadata": {},
   "source": [
    "## my functions(.) here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aba0b3",
   "metadata": {},
   "source": [
    "## input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d30fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/Users/ehsanmos/MLP_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf041aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset directory FOUND!\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(dataset_dir) == False:\n",
    "    print(\"dataset directory NOT found!\")\n",
    "else:\n",
    "    print(\"dataset directory FOUND!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbffb9",
   "metadata": {},
   "source": [
    "## Load input/ training dataset\n",
    "\n",
    "before doing this section, process filter final input dataset with \"check_n_filter_final_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af39e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ds = \"atmmodel_april_2016_k_zero_9cams4bands_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6dc59",
   "metadata": {},
   "source": [
    "## check if input dataset file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ac74e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ehsanmos/MLP_dataset/atmmodel_april_2016_k_zero_9cams4bands_preprocessed.csv\n",
      "input dataset found!\n"
     ]
    }
   ],
   "source": [
    "in_ds_fullpath = os.path.join(dataset_dir, in_ds)\n",
    "print(in_ds_fullpath)\n",
    "\n",
    "if (not os.path.isfile(os.path.join(in_ds_fullpath))):\n",
    "    raise SystemExit()\n",
    "else:\n",
    "    print(\"input dataset found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259b6d7b",
   "metadata": {},
   "source": [
    "## Read in dataset and look at dataset columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4582b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv(in_ds_fullpath, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "790ea7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40775, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4164b4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['firstLat', 'firstLon', 'anr', 'ang', 'anb', 'annir', 'aa', 'af', 'ba',\n",
       "       'bf', 'ca', 'cf', 'da', 'df', 'rms'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170b8a4",
   "metadata": {},
   "source": [
    "Note: we will build an input dataset with 9 cameras to train the mlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "654bfd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are using 10 columns in our training dataset:\n",
      "Index(['anr', 'aa', 'af', 'ba', 'bf', 'ca', 'cf', 'da', 'df', 'rms'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#- build dataset with 9 cameras\n",
    "excluce_columns = ['firstLat', 'firstLon', 'ang', 'anb', 'annir']\n",
    "input_ds_for_training = df_orig.drop(excluce_columns, axis=1)\n",
    "\n",
    "print('we are using %s columns in our training dataset:' %len(input_ds_for_training.columns))\n",
    "print(input_ds_for_training.columns)  # columns should be only 9 cameras + rms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b8050",
   "metadata": {},
   "source": [
    "## shuffle rows of input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7f0f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "input_ds_for_training = shuffle(input_ds_for_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706d2b8",
   "metadata": {},
   "source": [
    "## Split dataset to train-test parts for training algorithms\n",
    "- we devided to plit our dataset to 2 parts (2-part split)\n",
    "- Here we use the ‘train_test_split’ to split the data in 80:20 ratio i.e. 80% of the data will be used for training the model while 20% will be used for testing the model that is built out of it.\n",
    "- note: last column should be label == rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d94ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40775, 9)\n",
      "(40775, 1)\n"
     ]
    }
   ],
   "source": [
    "#- split data set to X and Y\n",
    "\n",
    "X = input_ds_for_training.iloc[:, :-1] # to select up to last column of dataset OR [:, 0:3]\n",
    "Y = input_ds_for_training.iloc[:, -1:] # to select last column of DF\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76e41811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size= 30 percent\n",
      "train:\n",
      "(28542, 9)\n",
      "(28542, 1)\n",
      "test:\n",
      "(12233, 9)\n",
      "(12233, 1)\n"
     ]
    }
   ],
   "source": [
    "#- now split dataset to train-test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#- we use this function to split data-- from here because we are usiong SKlearn library, we change all data structures from Pandas DF to numpy\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), Y.to_numpy(), test_size=0.2, random_state=123) # Q- input is DF or numpy array?\n",
    "\n",
    "test_data_size = 0.3\n",
    "print(\"test size= %d percent\" %(test_data_size*100))\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_data_size, random_state=123) # Q- input is DF or numpy array?\n",
    "\n",
    "\n",
    "print(\"train:\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"test:\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93fe1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ba85e",
   "metadata": {},
   "source": [
    "## >>> Linear regression >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72075a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# # Create linear regression object \n",
    "# mlr_model = LinearRegression() # create an estimator obj\n",
    "# ##- Train the model using the training sets- here we use X_train(3 features)\n",
    "# mlr_model.fit(x_train, y_train) # fit(X,y) will fit data to our algortihm and makes it learn from data == training step\n",
    "\n",
    "\n",
    "# # Predict using the linear model- on specific block data- on transect data\n",
    "# x_test = 'open 3 MISR cameras that create transect block, create x_test dataset from 3 cameras (an, ca, cf), and then make prediction '\n",
    "# y_pred_mlr = mlr_model.predict(x_test)\n",
    "\n",
    "\n",
    "# r2_model = r2_score(y_test, y_pred_mlr)\n",
    "# print('R2: %s' %round(r2_model,2))\n",
    "\n",
    "# print('Test RMSE: %.2f' %math.sqrt(mean_squared_error(y_test, y_pred_mlr))) # square root of MSE.\n",
    "\n",
    "# ############ scatter plot #####\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(y_test, y_pred_mlr)\n",
    "\n",
    "# # ax.plot([y_actual.min(), y_actual.max()], [y_predicted.min(), y_predicted.max()], 'k--', lw=4)\n",
    "# # ax.set_xlabel('Actual')\n",
    "# # ax.set_ylabel('Predicted')\n",
    "\n",
    "\n",
    "# ########### QQ plot ###########\n",
    "# import scipy.stats as stats\n",
    "\n",
    "# # residuals = y_test - y_pred_mlr\n",
    "# # residuals\n",
    "\n",
    "# # plt.figure(figsize=(7,7))\n",
    "# # stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "# # plt.title(\"Normal Q-Q Plot\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b177d3",
   "metadata": {},
   "source": [
    "Qn- how about train-val-test (3 sections)? is this for DL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b7538",
   "metadata": {},
   "source": [
    "## Feature scalling/ rescaling\n",
    "\n",
    "change the scale/range of input features from their original range to a new range. So changed features will have mean=0 and std=1.\n",
    "\n",
    "source: https://www.enjoyalgorithms.com/blog/need-of-feature-scaling-in-machine-learning\n",
    "\n",
    "- We rescale data after we split data to train-test\n",
    "- all features have the same range to reduce bias in data \n",
    "- perform this step before splitting data into train-test split\n",
    "- We normalize data using the training data\n",
    "\n",
    "Qn- why FS is important? why we do FS?\n",
    "\n",
    "Qn- which method? \n",
    "\n",
    "1) standardization/ Z-score/ StandardScaler() ==> rescale data to mean=0 & std=1; Standardize features by removing the mean and scaling to unit variance; good for datasets w/ outliers;\n",
    "\n",
    "2) MinMaxScalar() == Transform/rescale features by scaling each feature to a given range (usually [0,1])\n",
    "\n",
    "3) normalize() == Scale/rescale input vectors individually to unit norm (vector length).\n",
    "\n",
    "source: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ebb65",
   "metadata": {},
   "source": [
    "### 1) Using MinMaxScaler() method to rescale input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "102b941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #- import necessary libraries for Neural Nets\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# #- fit data\n",
    "\n",
    "# scaler_x = MinMaxScaler()\n",
    "# scaler_y = MinMaxScaler()\n",
    "\n",
    "# scaler_x.fit(X_train) # returns Fitted scaler\n",
    "# X_train_scaled = scaler_x.transform(X_train)  # transforms data\n",
    "\n",
    "# scaler_x.fit(X_test)\n",
    "# X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "# scaler_y.fit(y_train)\n",
    "# y_train_scaled = scaler_y.transform(y_train)\n",
    "\n",
    "# scaler_y.fit(y_test)\n",
    "# y_test_scaled = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d345c",
   "metadata": {},
   "source": [
    "### 2) Using StandardScaler() method \n",
    "\n",
    "Rescale input features to mean=0 and std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1eae3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler_x.fit(x_train).transform(x_train) # returns daata w/ mean 0 & std 1\n",
    "y_train_scaled = scaler_y.fit(y_train).transform(y_train)\n",
    "x_test_scaled = scaler_x.fit(x_test).transform(x_test)\n",
    "y_test_scaled = scaler_y.fit(y_test).transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20410efa",
   "metadata": {},
   "source": [
    "Check types of input dataset data structure; should be 2D arrays, or Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5083172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "-1.2126454400813924e-16\n",
      "0.8819171036881966\n",
      "7.617758991752177e-17\n",
      "1.0\n",
      "-4.040071505906816e-17\n",
      "0.8819171036881968\n",
      "3.8219334597412404e-16\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train_scaled))\n",
    "# print(type(y_train_scaled))\n",
    "# print(type(x_test_scaled))\n",
    "# print(type(y_test_scaled))\n",
    "\n",
    "print(x_train_scaled.mean())\n",
    "print(x_train_scaled.std())\n",
    "print(y_train_scaled.mean())\n",
    "print(y_train_scaled.std())\n",
    "print(x_test_scaled.mean())\n",
    "print(x_test_scaled.std())\n",
    "print(y_test_scaled.mean())\n",
    "print(y_test_scaled.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2db9f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28542, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa2e9d",
   "metadata": {},
   "source": [
    "## >>> Neural Network (Regression) >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18088f76",
   "metadata": {},
   "source": [
    "### Building the NN model\n",
    "Q- how find the best architecture? for mlp? \n",
    "\n",
    "let's do a architecture search\n",
    "\n",
    "source: https://towardsdatascience.com/how-to-find-optimal-neural-network-architecture-with-tensorflow-the-easy-way-50575a03d060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd212990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(num_layers: int,\n",
    "               min_nodes_per_layer: int,\n",
    "               max_nodes_per_layer: int,\n",
    "               node_step_size: int,\n",
    "               input_shape: tuple,\n",
    "               hidden_layer_activation: str = 'relu',\n",
    "               num_nodes_at_output: int = 1,\n",
    "               output_layer_activation: str = 'linear') -> list:   \n",
    "        \n",
    "        # https://stackoverflow.com/questions/14379753/what-does-mean-in-python-function-definitions\n",
    "        # allowing you to attach metadata to functions, describe their parameters (their expected types) and return values\n",
    "    \n",
    "    # creates a list from nodes that we have defined [min, max, step]\n",
    "    node_options = list(range(min_nodes_per_layer, \n",
    "                              max_nodes_per_layer + 1, \n",
    "                              node_step_size))\n",
    "    \n",
    "    # make a collection of sets of nodes for each hidden layer\n",
    "    layer_possibilities = [node_options] * num_layers\n",
    "    layer_node_permutations = list(itertools.product(*layer_possibilities))\n",
    "#     print('permutations:')\n",
    "#     print(layer_node_permutations)\n",
    "    \n",
    "    models = []\n",
    "    for permutation in layer_node_permutations:\n",
    "        \n",
    "        # setup input layer\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=input_shape, name=\"input-layer\"))\n",
    "        model_name = ''\n",
    "        \n",
    "        # setup hidden layers\n",
    "        for nodes_at_layer in permutation:\n",
    "            model.add(tf.keras.layers.Dense(nodes_at_layer, activation=hidden_layer_activation))\n",
    "            model_name += 'dense_'+str(nodes_at_layer)+'_'\n",
    "\n",
    "        # setup output layer\n",
    "        model.add(tf.keras.layers.Dense(num_nodes_at_output, activation=output_layer_activation, name=\"output-layer-SIR\"))\n",
    "        model._name = model_name[:-1]\n",
    "        models.append(model)\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03eec5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(models: list,\n",
    "             X_train: np.array,\n",
    "             y_train: np.array,\n",
    "             X_test: np.array,\n",
    "             y_test: np.array,\n",
    "             epochs: int,\n",
    "             batch_size: int, \n",
    "             validation_split: float,\n",
    "             verbose: int = 0) -> pd.DataFrame:\n",
    "    \n",
    "    # We will store the results in this list\n",
    "    results = []\n",
    "    def train(model: tf.keras.Sequential) -> dict:\n",
    "        \n",
    "        adam_lrt = 0.001\n",
    "        opt_alg = tf.keras.optimizers.Adam(\n",
    "                    learning_rate=adam_lrt,   # then everu step * 10 to get to 10\n",
    "                    beta_1=0.9,\n",
    "                    beta_2=0.999,\n",
    "                    epsilon=1e-07,\n",
    "                    amsgrad=False,\n",
    "                    name='Adam'\n",
    "        )\n",
    "        \n",
    "        # Compile each model\n",
    "        model.compile(\n",
    "            loss='mse', \n",
    "            optimizer=opt_alg, \n",
    "            metrics=['mse','mae']\n",
    "        )\n",
    "\n",
    "        # Train the model/ fit alg. to dataset\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split\n",
    "            \n",
    "        )\n",
    "        \n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred_scaled = model.predict(X_test, verbose=verbose)  # Generates output predictions for the input samples\n",
    "        # inverse y-pred to original scale\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "        # compare y & y-hat in original scale\n",
    "        rmse_pred_test = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "##########################################       \n",
    "#         # Evaluation on test data\n",
    "#         preds = model.evaluate(\n",
    "#             X_test, \n",
    "#             y_test, \n",
    "#             batch_size=20, \n",
    "#             verbose=0) # Returns the loss value & metrics values for the model in test mode\n",
    "#         print('return of evaluate(.)')\n",
    "#         print(preds)\n",
    "#         mse_test = preds[1]\n",
    "#         mae_test = preds[2]\n",
    "##########################################         \n",
    "\n",
    "        # Return evaluation metrics on the test set\n",
    "        return {\n",
    "            'model_name': model.name,\n",
    "            #'test_rmse': math.sqrt(mse_test),\n",
    "            #'test_mae': mae_test,\n",
    "            'test_pred_rmse (cm)': round(rmse_pred_test, 1)\n",
    "        }\n",
    "\n",
    "    # Train every model and save results in the list above and tur it to DF\n",
    "    for model in models:\n",
    "        try:\n",
    "            print(model.name, end=' ... \\n')\n",
    "            res = train(model=model)\n",
    "            results.append(res)\n",
    "        except Exception as e:\n",
    "            print(f'{model.name} --> {str(e)}') # how change to nornal print?\n",
    "        \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6538bb",
   "metadata": {},
   "source": [
    "## Building all models\n",
    "\n",
    "iterate with different hidden layers + iterate over several node options in one single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "168f3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers = 6\n",
    "\n",
    "# all_models = get_models(\n",
    "#     num_layers=num_layers, \n",
    "#     min_nodes_per_layer=9, \n",
    "#     max_nodes_per_layer=27, \n",
    "#     node_step_size=9, \n",
    "#     input_shape=(9,) # should be tuple\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2459dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models[0].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a863e",
   "metadata": {},
   "source": [
    "## Running the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbd1bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization_results = optimize(\n",
    "#     models=all_models,\n",
    "#     X_train=x_train_scaled,\n",
    "#     y_train=y_train_scaled,\n",
    "#     X_test=x_test_scaled,\n",
    "#     y_test=y_test,\n",
    "#     epochs=50,\n",
    "#     batch_size=100,\n",
    "#     validation_split=0.2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "971e78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39927f",
   "metadata": {},
   "source": [
    "## write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7559f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = dataset_dir\n",
    "# output_file = 'mlp_search_results_num_layers_'+str(num_layers)+'.csv'\n",
    "# output_fp = os.path.join(output_dir, output_file)\n",
    "# optimization_results.to_csv(output_fp, index=False)\n",
    "# output_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0642131",
   "metadata": {},
   "source": [
    "# >>> run DNN again for one run >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcf4b8",
   "metadata": {},
   "source": [
    "## build/setup layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ed4a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape: int,\n",
    "                hidden_layers_nodes: list,                \n",
    "                hidden_layer_activation: str = 'relu',\n",
    "                num_nodes_at_output: int = 1,\n",
    "                output_layer_activation: str = 'linear'):\n",
    "        \n",
    "    # setup input layer\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=input_shape, name=\"input-layer\"))\n",
    "    model_name = ''\n",
    "\n",
    "    # setup hidden layers\n",
    "    for nodes_at_layer in hidden_layers_nodes:\n",
    "        model.add(tf.keras.layers.Dense(nodes_at_layer, activation=hidden_layer_activation))\n",
    "        model_name += 'dense_'+str(nodes_at_layer)+'_'\n",
    "\n",
    "    # setup output layer\n",
    "    model.add(tf.keras.layers.Dense(num_nodes_at_output, activation=output_layer_activation, name=\"output-layer-SIR\"))\n",
    "    model._name = model_name[:-1]\n",
    "    \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c81eb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 17:25:34.295351: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fdcf037e190>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_node_list = [18,27,36,27,18]\n",
    "built_model = build_model(input_shape = 9, \n",
    "                        hidden_layers_nodes = layers_node_list)\n",
    "\n",
    "built_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea149b1",
   "metadata": {},
   "source": [
    "## compile DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ab48977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dense_18_dense_27_dense_36_dense_27_dense_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 18)                180       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 27)                513       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 36)                1008      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 27)                999       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 18)                504       \n",
      "                                                                 \n",
      " output-layer-SIR (Dense)    (None, 1)                 19        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,223\n",
      "Trainable params: 3,223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adam_lrt = 0.001\n",
    "opt_alg = tf.keras.optimizers.Adam(\n",
    "            learning_rate=adam_lrt,   # then everu step * 10 to get to 10\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-07,\n",
    "            amsgrad=False,\n",
    "            name='Adam'\n",
    ")\n",
    "\n",
    "# Compile each model\n",
    "built_model.compile(\n",
    "    loss='mse', \n",
    "    optimizer=opt_alg, \n",
    "    metrics=['mse','mae']\n",
    ")\n",
    "\n",
    "built_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d8426",
   "metadata": {},
   "source": [
    "## train our model/ fit algorithm to train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6516ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "229/229 [==============================] - 1s 2ms/step - loss: 0.9320 - mse: 0.9320 - mae: 0.7402 - val_loss: 0.8162 - val_mse: 0.8162 - val_mae: 0.6782\n",
      "Epoch 2/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.8516 - mse: 0.8516 - mae: 0.6981 - val_loss: 0.8176 - val_mse: 0.8176 - val_mae: 0.7025\n",
      "Epoch 3/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.8313 - mse: 0.8313 - mae: 0.6874 - val_loss: 0.7785 - val_mse: 0.7785 - val_mae: 0.6766\n",
      "Epoch 4/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.8205 - mse: 0.8205 - mae: 0.6796 - val_loss: 0.7826 - val_mse: 0.7826 - val_mae: 0.6941\n",
      "Epoch 5/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.8054 - mse: 0.8054 - mae: 0.6722 - val_loss: 0.7588 - val_mse: 0.7588 - val_mae: 0.6495\n",
      "Epoch 6/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7978 - mse: 0.7978 - mae: 0.6668 - val_loss: 0.7523 - val_mse: 0.7523 - val_mae: 0.6525\n",
      "Epoch 7/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7927 - mse: 0.7927 - mae: 0.6648 - val_loss: 0.7521 - val_mse: 0.7521 - val_mae: 0.6452\n",
      "Epoch 8/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7918 - mse: 0.7918 - mae: 0.6630 - val_loss: 0.7557 - val_mse: 0.7557 - val_mae: 0.6467\n",
      "Epoch 9/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7917 - mse: 0.7917 - mae: 0.6618 - val_loss: 0.7557 - val_mse: 0.7557 - val_mae: 0.6261\n",
      "Epoch 10/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7829 - mse: 0.7829 - mae: 0.6571 - val_loss: 0.7528 - val_mse: 0.7528 - val_mae: 0.6659\n",
      "Epoch 11/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7769 - mse: 0.7769 - mae: 0.6543 - val_loss: 0.7428 - val_mse: 0.7428 - val_mae: 0.6423\n",
      "Epoch 12/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7787 - mse: 0.7787 - mae: 0.6557 - val_loss: 0.7395 - val_mse: 0.7395 - val_mae: 0.6379\n",
      "Epoch 13/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7777 - mse: 0.7777 - mae: 0.6531 - val_loss: 0.7382 - val_mse: 0.7382 - val_mae: 0.6331\n",
      "Epoch 14/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7734 - mse: 0.7734 - mae: 0.6518 - val_loss: 0.7486 - val_mse: 0.7486 - val_mae: 0.6617\n",
      "Epoch 15/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7641 - mse: 0.7641 - mae: 0.6461 - val_loss: 0.7333 - val_mse: 0.7333 - val_mae: 0.6369\n",
      "Epoch 16/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7651 - mse: 0.7651 - mae: 0.6473 - val_loss: 0.7376 - val_mse: 0.7376 - val_mae: 0.6582\n",
      "Epoch 17/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7623 - mse: 0.7623 - mae: 0.6462 - val_loss: 0.7299 - val_mse: 0.7299 - val_mae: 0.6436\n",
      "Epoch 18/200\n",
      "229/229 [==============================] - 0s 2ms/step - loss: 0.7609 - mse: 0.7609 - mae: 0.6438 - val_loss: 0.7292 - val_mse: 0.7292 - val_mae: 0.6213\n",
      "Epoch 19/200\n",
      "229/229 [==============================] - 0s 2ms/step - loss: 0.7594 - mse: 0.7594 - mae: 0.6428 - val_loss: 0.7224 - val_mse: 0.7224 - val_mae: 0.6311\n",
      "Epoch 20/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7545 - mse: 0.7545 - mae: 0.6418 - val_loss: 0.7163 - val_mse: 0.7163 - val_mae: 0.6350\n",
      "Epoch 21/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7564 - mse: 0.7564 - mae: 0.6427 - val_loss: 0.7202 - val_mse: 0.7202 - val_mae: 0.6242\n",
      "Epoch 22/200\n",
      "229/229 [==============================] - 0s 2ms/step - loss: 0.7468 - mse: 0.7468 - mae: 0.6372 - val_loss: 0.7349 - val_mse: 0.7349 - val_mae: 0.6089\n",
      "Epoch 23/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7466 - mse: 0.7466 - mae: 0.6366 - val_loss: 0.7247 - val_mse: 0.7247 - val_mae: 0.6453\n",
      "Epoch 24/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7481 - mse: 0.7481 - mae: 0.6373 - val_loss: 0.7249 - val_mse: 0.7249 - val_mae: 0.6532\n",
      "Epoch 25/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7438 - mse: 0.7438 - mae: 0.6356 - val_loss: 0.7166 - val_mse: 0.7166 - val_mae: 0.6400\n",
      "Epoch 26/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7390 - mse: 0.7390 - mae: 0.6336 - val_loss: 0.7138 - val_mse: 0.7138 - val_mae: 0.6156\n",
      "Epoch 27/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7386 - mse: 0.7386 - mae: 0.6319 - val_loss: 0.7312 - val_mse: 0.7312 - val_mae: 0.6630\n",
      "Epoch 28/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7396 - mse: 0.7396 - mae: 0.6329 - val_loss: 0.7163 - val_mse: 0.7163 - val_mae: 0.6379\n",
      "Epoch 29/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7349 - mse: 0.7349 - mae: 0.6317 - val_loss: 0.7113 - val_mse: 0.7113 - val_mae: 0.6057\n",
      "Epoch 30/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7334 - mse: 0.7334 - mae: 0.6296 - val_loss: 0.7096 - val_mse: 0.7096 - val_mae: 0.6371\n",
      "Epoch 31/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7311 - mse: 0.7311 - mae: 0.6299 - val_loss: 0.6980 - val_mse: 0.6980 - val_mae: 0.6101\n",
      "Epoch 32/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7312 - mse: 0.7312 - mae: 0.6286 - val_loss: 0.7130 - val_mse: 0.7130 - val_mae: 0.6103\n",
      "Epoch 33/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7272 - mse: 0.7272 - mae: 0.6261 - val_loss: 0.7039 - val_mse: 0.7039 - val_mae: 0.6373\n",
      "Epoch 34/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7293 - mse: 0.7293 - mae: 0.6277 - val_loss: 0.6966 - val_mse: 0.6966 - val_mae: 0.6155\n",
      "Epoch 35/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7214 - mse: 0.7214 - mae: 0.6235 - val_loss: 0.6959 - val_mse: 0.6959 - val_mae: 0.6174\n",
      "Epoch 36/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7193 - mse: 0.7193 - mae: 0.6220 - val_loss: 0.7075 - val_mse: 0.7075 - val_mae: 0.6422\n",
      "Epoch 37/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7186 - mse: 0.7186 - mae: 0.6222 - val_loss: 0.7224 - val_mse: 0.7224 - val_mae: 0.6499\n",
      "Epoch 38/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7174 - mse: 0.7174 - mae: 0.6212 - val_loss: 0.6946 - val_mse: 0.6946 - val_mae: 0.6166\n",
      "Epoch 39/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7132 - mse: 0.7132 - mae: 0.6188 - val_loss: 0.7020 - val_mse: 0.7020 - val_mae: 0.6350\n",
      "Epoch 40/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7169 - mse: 0.7169 - mae: 0.6200 - val_loss: 0.6893 - val_mse: 0.6893 - val_mae: 0.6047\n",
      "Epoch 41/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7140 - mse: 0.7140 - mae: 0.6186 - val_loss: 0.6935 - val_mse: 0.6935 - val_mae: 0.6096\n",
      "Epoch 42/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7118 - mse: 0.7118 - mae: 0.6181 - val_loss: 0.6886 - val_mse: 0.6886 - val_mae: 0.5992\n",
      "Epoch 43/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7113 - mse: 0.7113 - mae: 0.6174 - val_loss: 0.6859 - val_mse: 0.6859 - val_mae: 0.6091\n",
      "Epoch 44/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7060 - mse: 0.7060 - mae: 0.6144 - val_loss: 0.6875 - val_mse: 0.6875 - val_mae: 0.6016\n",
      "Epoch 45/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7042 - mse: 0.7042 - mae: 0.6133 - val_loss: 0.6852 - val_mse: 0.6852 - val_mae: 0.6030\n",
      "Epoch 46/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7040 - mse: 0.7040 - mae: 0.6140 - val_loss: 0.6871 - val_mse: 0.6871 - val_mae: 0.6190\n",
      "Epoch 47/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7016 - mse: 0.7016 - mae: 0.6131 - val_loss: 0.6823 - val_mse: 0.6823 - val_mae: 0.6110\n",
      "Epoch 48/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7001 - mse: 0.7001 - mae: 0.6122 - val_loss: 0.6799 - val_mse: 0.6799 - val_mae: 0.6141\n",
      "Epoch 49/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6966 - mse: 0.6966 - mae: 0.6099 - val_loss: 0.6867 - val_mse: 0.6867 - val_mae: 0.6013\n",
      "Epoch 50/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.7004 - mse: 0.7004 - mae: 0.6116 - val_loss: 0.7109 - val_mse: 0.7109 - val_mae: 0.6507\n",
      "Epoch 51/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6936 - mse: 0.6936 - mae: 0.6088 - val_loss: 0.6837 - val_mse: 0.6837 - val_mae: 0.6002\n",
      "Epoch 52/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6931 - mse: 0.6931 - mae: 0.6085 - val_loss: 0.6827 - val_mse: 0.6827 - val_mae: 0.6002\n",
      "Epoch 53/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6933 - mse: 0.6933 - mae: 0.6071 - val_loss: 0.6750 - val_mse: 0.6750 - val_mae: 0.6137\n",
      "Epoch 54/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6874 - mse: 0.6874 - mae: 0.6056 - val_loss: 0.7015 - val_mse: 0.7015 - val_mae: 0.6254\n",
      "Epoch 55/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6915 - mse: 0.6915 - mae: 0.6080 - val_loss: 0.6814 - val_mse: 0.6814 - val_mae: 0.6023\n",
      "Epoch 56/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6880 - mse: 0.6880 - mae: 0.6055 - val_loss: 0.6788 - val_mse: 0.6788 - val_mae: 0.6115\n",
      "Epoch 57/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6903 - mse: 0.6903 - mae: 0.6061 - val_loss: 0.6731 - val_mse: 0.6731 - val_mae: 0.6012\n",
      "Epoch 58/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6829 - mse: 0.6829 - mae: 0.6028 - val_loss: 0.6716 - val_mse: 0.6716 - val_mae: 0.6041\n",
      "Epoch 59/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6830 - mse: 0.6830 - mae: 0.6024 - val_loss: 0.6743 - val_mse: 0.6743 - val_mae: 0.5920\n",
      "Epoch 60/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6845 - mse: 0.6845 - mae: 0.6037 - val_loss: 0.6928 - val_mse: 0.6928 - val_mae: 0.6229\n",
      "Epoch 61/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6810 - mse: 0.6810 - mae: 0.6013 - val_loss: 0.6720 - val_mse: 0.6720 - val_mae: 0.6055\n",
      "Epoch 62/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6794 - mse: 0.6794 - mae: 0.6006 - val_loss: 0.6685 - val_mse: 0.6685 - val_mae: 0.5952\n",
      "Epoch 63/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6817 - mse: 0.6817 - mae: 0.6013 - val_loss: 0.6746 - val_mse: 0.6746 - val_mae: 0.6019\n",
      "Epoch 64/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6788 - mse: 0.6788 - mae: 0.5992 - val_loss: 0.6700 - val_mse: 0.6700 - val_mae: 0.6021\n",
      "Epoch 65/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6799 - mse: 0.6799 - mae: 0.6010 - val_loss: 0.6708 - val_mse: 0.6708 - val_mae: 0.5850\n",
      "Epoch 66/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6746 - mse: 0.6746 - mae: 0.5970 - val_loss: 0.6905 - val_mse: 0.6905 - val_mae: 0.6304\n",
      "Epoch 67/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6776 - mse: 0.6776 - mae: 0.5996 - val_loss: 0.6800 - val_mse: 0.6800 - val_mae: 0.6217\n",
      "Epoch 68/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6811 - mse: 0.6811 - mae: 0.6009 - val_loss: 0.6791 - val_mse: 0.6791 - val_mae: 0.6037\n",
      "Epoch 69/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6723 - mse: 0.6723 - mae: 0.5966 - val_loss: 0.6664 - val_mse: 0.6664 - val_mae: 0.6066\n",
      "Epoch 70/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6700 - mse: 0.6700 - mae: 0.5956 - val_loss: 0.6815 - val_mse: 0.6815 - val_mae: 0.6146\n",
      "Epoch 71/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6729 - mse: 0.6729 - mae: 0.5977 - val_loss: 0.6695 - val_mse: 0.6695 - val_mae: 0.6038\n",
      "Epoch 72/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6709 - mse: 0.6709 - mae: 0.5966 - val_loss: 0.6761 - val_mse: 0.6761 - val_mae: 0.6055\n",
      "Epoch 73/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6685 - mse: 0.6685 - mae: 0.5948 - val_loss: 0.6687 - val_mse: 0.6687 - val_mae: 0.5952\n",
      "Epoch 74/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6682 - mse: 0.6682 - mae: 0.5938 - val_loss: 0.6707 - val_mse: 0.6707 - val_mae: 0.6065\n",
      "Epoch 75/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6643 - mse: 0.6643 - mae: 0.5923 - val_loss: 0.6684 - val_mse: 0.6684 - val_mae: 0.5835\n",
      "Epoch 76/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6631 - mse: 0.6631 - mae: 0.5927 - val_loss: 0.6736 - val_mse: 0.6736 - val_mae: 0.5820\n",
      "Epoch 77/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6653 - mse: 0.6653 - mae: 0.5925 - val_loss: 0.6778 - val_mse: 0.6778 - val_mae: 0.5935\n",
      "Epoch 78/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6710 - mse: 0.6710 - mae: 0.5963 - val_loss: 0.6588 - val_mse: 0.6588 - val_mae: 0.5882\n",
      "Epoch 79/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6641 - mse: 0.6641 - mae: 0.5928 - val_loss: 0.6706 - val_mse: 0.6706 - val_mae: 0.5991\n",
      "Epoch 80/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6642 - mse: 0.6642 - mae: 0.5922 - val_loss: 0.6577 - val_mse: 0.6577 - val_mae: 0.5829\n",
      "Epoch 81/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6580 - mse: 0.6580 - mae: 0.5902 - val_loss: 0.6655 - val_mse: 0.6655 - val_mae: 0.5819\n",
      "Epoch 82/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6620 - mse: 0.6620 - mae: 0.5901 - val_loss: 0.6713 - val_mse: 0.6713 - val_mae: 0.6105\n",
      "Epoch 83/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6600 - mse: 0.6600 - mae: 0.5907 - val_loss: 0.6643 - val_mse: 0.6643 - val_mae: 0.6073\n",
      "Epoch 84/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6608 - mse: 0.6608 - mae: 0.5913 - val_loss: 0.6725 - val_mse: 0.6725 - val_mae: 0.5956\n",
      "Epoch 85/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6622 - mse: 0.6622 - mae: 0.5918 - val_loss: 0.6710 - val_mse: 0.6710 - val_mae: 0.6140\n",
      "Epoch 86/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6572 - mse: 0.6572 - mae: 0.5897 - val_loss: 0.6564 - val_mse: 0.6564 - val_mae: 0.5896\n",
      "Epoch 87/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6564 - mse: 0.6564 - mae: 0.5880 - val_loss: 0.6874 - val_mse: 0.6874 - val_mae: 0.6226\n",
      "Epoch 88/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6582 - mse: 0.6582 - mae: 0.5892 - val_loss: 0.6671 - val_mse: 0.6671 - val_mae: 0.5841\n",
      "Epoch 89/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6536 - mse: 0.6536 - mae: 0.5867 - val_loss: 0.6781 - val_mse: 0.6781 - val_mae: 0.6126\n",
      "Epoch 90/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6540 - mse: 0.6540 - mae: 0.5874 - val_loss: 0.6587 - val_mse: 0.6587 - val_mae: 0.5979\n",
      "Epoch 91/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6515 - mse: 0.6515 - mae: 0.5859 - val_loss: 0.6652 - val_mse: 0.6652 - val_mae: 0.5926\n",
      "Epoch 92/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6543 - mse: 0.6543 - mae: 0.5871 - val_loss: 0.6660 - val_mse: 0.6660 - val_mae: 0.6080\n",
      "Epoch 93/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6562 - mse: 0.6562 - mae: 0.5880 - val_loss: 0.6661 - val_mse: 0.6661 - val_mae: 0.6003\n",
      "Epoch 94/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6514 - mse: 0.6514 - mae: 0.5859 - val_loss: 0.6722 - val_mse: 0.6722 - val_mae: 0.6028\n",
      "Epoch 95/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6559 - mse: 0.6559 - mae: 0.5889 - val_loss: 0.6615 - val_mse: 0.6615 - val_mae: 0.5966\n",
      "Epoch 96/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6504 - mse: 0.6504 - mae: 0.5847 - val_loss: 0.6760 - val_mse: 0.6760 - val_mae: 0.5999\n",
      "Epoch 97/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6470 - mse: 0.6470 - mae: 0.5844 - val_loss: 0.6613 - val_mse: 0.6613 - val_mae: 0.5911\n",
      "Epoch 98/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6525 - mse: 0.6525 - mae: 0.5863 - val_loss: 0.6570 - val_mse: 0.6570 - val_mae: 0.5848\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6501 - mse: 0.6501 - mae: 0.5860 - val_loss: 0.6630 - val_mse: 0.6630 - val_mae: 0.5818\n",
      "Epoch 100/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6474 - mse: 0.6474 - mae: 0.5842 - val_loss: 0.6554 - val_mse: 0.6554 - val_mae: 0.5790\n",
      "Epoch 101/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6505 - mse: 0.6505 - mae: 0.5851 - val_loss: 0.6752 - val_mse: 0.6752 - val_mae: 0.6124\n",
      "Epoch 102/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6488 - mse: 0.6488 - mae: 0.5847 - val_loss: 0.6707 - val_mse: 0.6707 - val_mae: 0.5929\n",
      "Epoch 103/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6472 - mse: 0.6472 - mae: 0.5843 - val_loss: 0.6755 - val_mse: 0.6755 - val_mae: 0.6151\n",
      "Epoch 104/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6515 - mse: 0.6515 - mae: 0.5862 - val_loss: 0.6617 - val_mse: 0.6617 - val_mae: 0.5803\n",
      "Epoch 105/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6470 - mse: 0.6470 - mae: 0.5834 - val_loss: 0.6592 - val_mse: 0.6592 - val_mae: 0.5729\n",
      "Epoch 106/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6460 - mse: 0.6460 - mae: 0.5825 - val_loss: 0.6638 - val_mse: 0.6638 - val_mae: 0.5918\n",
      "Epoch 107/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6453 - mse: 0.6453 - mae: 0.5836 - val_loss: 0.6776 - val_mse: 0.6776 - val_mae: 0.6091\n",
      "Epoch 108/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6409 - mse: 0.6409 - mae: 0.5805 - val_loss: 0.6633 - val_mse: 0.6633 - val_mae: 0.5839\n",
      "Epoch 109/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6393 - mse: 0.6393 - mae: 0.5797 - val_loss: 0.6617 - val_mse: 0.6617 - val_mae: 0.5956\n",
      "Epoch 110/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6416 - mse: 0.6416 - mae: 0.5801 - val_loss: 0.6785 - val_mse: 0.6785 - val_mae: 0.6048\n",
      "Epoch 111/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6418 - mse: 0.6418 - mae: 0.5803 - val_loss: 0.6653 - val_mse: 0.6653 - val_mae: 0.6126\n",
      "Epoch 112/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6418 - mse: 0.6418 - mae: 0.5814 - val_loss: 0.6617 - val_mse: 0.6617 - val_mae: 0.5927\n",
      "Epoch 113/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6570 - mse: 0.6570 - mae: 0.5875 - val_loss: 0.6663 - val_mse: 0.6663 - val_mae: 0.6011\n",
      "Epoch 114/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6439 - mse: 0.6439 - mae: 0.5819 - val_loss: 0.6660 - val_mse: 0.6660 - val_mae: 0.5984\n",
      "Epoch 115/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6428 - mse: 0.6428 - mae: 0.5811 - val_loss: 0.6642 - val_mse: 0.6642 - val_mae: 0.6144\n",
      "Epoch 116/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6419 - mse: 0.6419 - mae: 0.5821 - val_loss: 0.6581 - val_mse: 0.6581 - val_mae: 0.5822\n",
      "Epoch 117/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6371 - mse: 0.6371 - mae: 0.5781 - val_loss: 0.6681 - val_mse: 0.6681 - val_mae: 0.6083\n",
      "Epoch 118/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6404 - mse: 0.6404 - mae: 0.5807 - val_loss: 0.6610 - val_mse: 0.6610 - val_mae: 0.5866\n",
      "Epoch 119/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6412 - mse: 0.6412 - mae: 0.5808 - val_loss: 0.6609 - val_mse: 0.6609 - val_mae: 0.5862\n",
      "Epoch 120/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6399 - mse: 0.6399 - mae: 0.5805 - val_loss: 0.6685 - val_mse: 0.6685 - val_mae: 0.5775\n",
      "Epoch 121/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6368 - mse: 0.6368 - mae: 0.5780 - val_loss: 0.6566 - val_mse: 0.6566 - val_mae: 0.5712\n",
      "Epoch 122/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6384 - mse: 0.6384 - mae: 0.5789 - val_loss: 0.6516 - val_mse: 0.6516 - val_mae: 0.5738\n",
      "Epoch 123/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6347 - mse: 0.6347 - mae: 0.5773 - val_loss: 0.6635 - val_mse: 0.6635 - val_mae: 0.6076\n",
      "Epoch 124/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6370 - mse: 0.6370 - mae: 0.5779 - val_loss: 0.6507 - val_mse: 0.6507 - val_mae: 0.5950\n",
      "Epoch 125/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6365 - mse: 0.6365 - mae: 0.5801 - val_loss: 0.6539 - val_mse: 0.6539 - val_mae: 0.5789\n",
      "Epoch 126/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6356 - mse: 0.6356 - mae: 0.5771 - val_loss: 0.6631 - val_mse: 0.6631 - val_mae: 0.6030\n",
      "Epoch 127/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6308 - mse: 0.6308 - mae: 0.5760 - val_loss: 0.6653 - val_mse: 0.6653 - val_mae: 0.5785\n",
      "Epoch 128/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6318 - mse: 0.6318 - mae: 0.5761 - val_loss: 0.6472 - val_mse: 0.6472 - val_mae: 0.5835\n",
      "Epoch 129/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6340 - mse: 0.6340 - mae: 0.5768 - val_loss: 0.6826 - val_mse: 0.6826 - val_mae: 0.6285\n",
      "Epoch 130/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6306 - mse: 0.6306 - mae: 0.5763 - val_loss: 0.6522 - val_mse: 0.6522 - val_mae: 0.5855\n",
      "Epoch 131/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6325 - mse: 0.6325 - mae: 0.5767 - val_loss: 0.6593 - val_mse: 0.6593 - val_mae: 0.5803\n",
      "Epoch 132/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6301 - mse: 0.6301 - mae: 0.5755 - val_loss: 0.6609 - val_mse: 0.6609 - val_mae: 0.5954\n",
      "Epoch 133/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6334 - mse: 0.6334 - mae: 0.5767 - val_loss: 0.6723 - val_mse: 0.6723 - val_mae: 0.6092\n",
      "Epoch 134/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6264 - mse: 0.6264 - mae: 0.5737 - val_loss: 0.6528 - val_mse: 0.6528 - val_mae: 0.5769\n",
      "Epoch 135/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6288 - mse: 0.6288 - mae: 0.5753 - val_loss: 0.6539 - val_mse: 0.6539 - val_mae: 0.5877\n",
      "Epoch 136/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6279 - mse: 0.6279 - mae: 0.5746 - val_loss: 0.6533 - val_mse: 0.6533 - val_mae: 0.5856\n",
      "Epoch 137/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6269 - mse: 0.6269 - mae: 0.5739 - val_loss: 0.6641 - val_mse: 0.6641 - val_mae: 0.5688\n",
      "Epoch 138/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6297 - mse: 0.6297 - mae: 0.5751 - val_loss: 0.6618 - val_mse: 0.6618 - val_mae: 0.6064\n",
      "Epoch 139/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6258 - mse: 0.6258 - mae: 0.5733 - val_loss: 0.6658 - val_mse: 0.6658 - val_mae: 0.5973\n",
      "Epoch 140/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6272 - mse: 0.6272 - mae: 0.5737 - val_loss: 0.6553 - val_mse: 0.6553 - val_mae: 0.5769\n",
      "Epoch 141/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6224 - mse: 0.6224 - mae: 0.5716 - val_loss: 0.6688 - val_mse: 0.6688 - val_mae: 0.5914\n",
      "Epoch 142/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6254 - mse: 0.6254 - mae: 0.5725 - val_loss: 0.6579 - val_mse: 0.6579 - val_mae: 0.5936\n",
      "Epoch 143/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6215 - mse: 0.6215 - mae: 0.5722 - val_loss: 0.6593 - val_mse: 0.6593 - val_mae: 0.5781\n",
      "Epoch 144/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6236 - mse: 0.6236 - mae: 0.5710 - val_loss: 0.6534 - val_mse: 0.6534 - val_mae: 0.5822\n",
      "Epoch 145/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6247 - mse: 0.6247 - mae: 0.5727 - val_loss: 0.6634 - val_mse: 0.6634 - val_mae: 0.6048\n",
      "Epoch 146/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6194 - mse: 0.6194 - mae: 0.5703 - val_loss: 0.6655 - val_mse: 0.6655 - val_mae: 0.5861\n",
      "Epoch 147/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6192 - mse: 0.6192 - mae: 0.5702 - val_loss: 0.6740 - val_mse: 0.6740 - val_mae: 0.6076\n",
      "Epoch 148/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6225 - mse: 0.6225 - mae: 0.5722 - val_loss: 0.6780 - val_mse: 0.6780 - val_mae: 0.6227\n",
      "Epoch 149/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6216 - mse: 0.6216 - mae: 0.5704 - val_loss: 0.6596 - val_mse: 0.6596 - val_mae: 0.5980\n",
      "Epoch 150/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6181 - mse: 0.6181 - mae: 0.5691 - val_loss: 0.6479 - val_mse: 0.6479 - val_mae: 0.5814\n",
      "Epoch 151/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6195 - mse: 0.6195 - mae: 0.5696 - val_loss: 0.6576 - val_mse: 0.6576 - val_mae: 0.5778\n",
      "Epoch 152/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6197 - mse: 0.6197 - mae: 0.5703 - val_loss: 0.6603 - val_mse: 0.6603 - val_mae: 0.6033\n",
      "Epoch 153/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6193 - mse: 0.6193 - mae: 0.5709 - val_loss: 0.6457 - val_mse: 0.6457 - val_mae: 0.5826\n",
      "Epoch 154/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6163 - mse: 0.6163 - mae: 0.5677 - val_loss: 0.6546 - val_mse: 0.6546 - val_mae: 0.5800\n",
      "Epoch 155/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6172 - mse: 0.6172 - mae: 0.5685 - val_loss: 0.6484 - val_mse: 0.6484 - val_mae: 0.5760\n",
      "Epoch 156/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6159 - mse: 0.6159 - mae: 0.5690 - val_loss: 0.6460 - val_mse: 0.6460 - val_mae: 0.5794\n",
      "Epoch 157/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6181 - mse: 0.6181 - mae: 0.5685 - val_loss: 0.6430 - val_mse: 0.6430 - val_mae: 0.5839\n",
      "Epoch 158/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6159 - mse: 0.6159 - mae: 0.5682 - val_loss: 0.6477 - val_mse: 0.6477 - val_mae: 0.5883\n",
      "Epoch 159/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6122 - mse: 0.6122 - mae: 0.5673 - val_loss: 0.6538 - val_mse: 0.6538 - val_mae: 0.5773\n",
      "Epoch 160/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6144 - mse: 0.6144 - mae: 0.5679 - val_loss: 0.6541 - val_mse: 0.6541 - val_mae: 0.5894\n",
      "Epoch 161/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6143 - mse: 0.6143 - mae: 0.5683 - val_loss: 0.6535 - val_mse: 0.6535 - val_mae: 0.5827\n",
      "Epoch 162/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6101 - mse: 0.6101 - mae: 0.5656 - val_loss: 0.6505 - val_mse: 0.6505 - val_mae: 0.5869\n",
      "Epoch 163/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6112 - mse: 0.6112 - mae: 0.5655 - val_loss: 0.6585 - val_mse: 0.6585 - val_mae: 0.6110\n",
      "Epoch 164/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6146 - mse: 0.6146 - mae: 0.5677 - val_loss: 0.6716 - val_mse: 0.6716 - val_mae: 0.6170\n",
      "Epoch 165/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6125 - mse: 0.6125 - mae: 0.5667 - val_loss: 0.6755 - val_mse: 0.6755 - val_mae: 0.6167\n",
      "Epoch 166/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6125 - mse: 0.6125 - mae: 0.5673 - val_loss: 0.6497 - val_mse: 0.6497 - val_mae: 0.5770\n",
      "Epoch 167/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6118 - mse: 0.6118 - mae: 0.5648 - val_loss: 0.6544 - val_mse: 0.6544 - val_mae: 0.5745\n",
      "Epoch 168/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6091 - mse: 0.6091 - mae: 0.5659 - val_loss: 0.6754 - val_mse: 0.6754 - val_mae: 0.6110\n",
      "Epoch 169/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6178 - mse: 0.6178 - mae: 0.5688 - val_loss: 0.6456 - val_mse: 0.6456 - val_mae: 0.5906\n",
      "Epoch 170/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6097 - mse: 0.6097 - mae: 0.5659 - val_loss: 0.6530 - val_mse: 0.6530 - val_mae: 0.5788\n",
      "Epoch 171/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6104 - mse: 0.6104 - mae: 0.5655 - val_loss: 0.6536 - val_mse: 0.6536 - val_mae: 0.5788\n",
      "Epoch 172/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6139 - mse: 0.6139 - mae: 0.5664 - val_loss: 0.6487 - val_mse: 0.6487 - val_mae: 0.5695\n",
      "Epoch 173/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6098 - mse: 0.6098 - mae: 0.5637 - val_loss: 0.6552 - val_mse: 0.6552 - val_mae: 0.5927\n",
      "Epoch 174/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6090 - mse: 0.6090 - mae: 0.5660 - val_loss: 0.6531 - val_mse: 0.6531 - val_mae: 0.5765\n",
      "Epoch 175/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6046 - mse: 0.6046 - mae: 0.5626 - val_loss: 0.6449 - val_mse: 0.6449 - val_mae: 0.5855\n",
      "Epoch 176/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6062 - mse: 0.6062 - mae: 0.5625 - val_loss: 0.6665 - val_mse: 0.6665 - val_mae: 0.5954\n",
      "Epoch 177/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6066 - mse: 0.6066 - mae: 0.5641 - val_loss: 0.6560 - val_mse: 0.6560 - val_mae: 0.5760\n",
      "Epoch 178/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6117 - mse: 0.6117 - mae: 0.5651 - val_loss: 0.6530 - val_mse: 0.6530 - val_mae: 0.5935\n",
      "Epoch 179/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6065 - mse: 0.6065 - mae: 0.5632 - val_loss: 0.6511 - val_mse: 0.6511 - val_mae: 0.5751\n",
      "Epoch 180/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6073 - mse: 0.6073 - mae: 0.5631 - val_loss: 0.6520 - val_mse: 0.6520 - val_mae: 0.5860\n",
      "Epoch 181/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6084 - mse: 0.6084 - mae: 0.5642 - val_loss: 0.6618 - val_mse: 0.6618 - val_mae: 0.5999\n",
      "Epoch 182/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6032 - mse: 0.6032 - mae: 0.5619 - val_loss: 0.6543 - val_mse: 0.6543 - val_mae: 0.5806\n",
      "Epoch 183/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6059 - mse: 0.6059 - mae: 0.5618 - val_loss: 0.6501 - val_mse: 0.6501 - val_mae: 0.5699\n",
      "Epoch 184/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6052 - mse: 0.6052 - mae: 0.5627 - val_loss: 0.6494 - val_mse: 0.6494 - val_mae: 0.5786\n",
      "Epoch 185/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6067 - mse: 0.6067 - mae: 0.5625 - val_loss: 0.6501 - val_mse: 0.6501 - val_mae: 0.5796\n",
      "Epoch 186/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6050 - mse: 0.6050 - mae: 0.5623 - val_loss: 0.6492 - val_mse: 0.6492 - val_mae: 0.5839\n",
      "Epoch 187/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6026 - mse: 0.6026 - mae: 0.5621 - val_loss: 0.6464 - val_mse: 0.6464 - val_mae: 0.5807\n",
      "Epoch 188/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6023 - mse: 0.6023 - mae: 0.5605 - val_loss: 0.6564 - val_mse: 0.6564 - val_mae: 0.5924\n",
      "Epoch 189/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6057 - mse: 0.6057 - mae: 0.5636 - val_loss: 0.6563 - val_mse: 0.6563 - val_mae: 0.5934\n",
      "Epoch 190/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6033 - mse: 0.6033 - mae: 0.5610 - val_loss: 0.6592 - val_mse: 0.6592 - val_mae: 0.5975\n",
      "Epoch 191/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6012 - mse: 0.6012 - mae: 0.5611 - val_loss: 0.6568 - val_mse: 0.6568 - val_mae: 0.5816\n",
      "Epoch 192/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6040 - mse: 0.6040 - mae: 0.5614 - val_loss: 0.6421 - val_mse: 0.6421 - val_mae: 0.5853\n",
      "Epoch 193/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6023 - mse: 0.6023 - mae: 0.5608 - val_loss: 0.6531 - val_mse: 0.6531 - val_mae: 0.5793\n",
      "Epoch 194/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6038 - mse: 0.6038 - mae: 0.5613 - val_loss: 0.6657 - val_mse: 0.6657 - val_mae: 0.5695\n",
      "Epoch 195/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.5983 - mse: 0.5983 - mae: 0.5579 - val_loss: 0.6827 - val_mse: 0.6827 - val_mae: 0.6235\n",
      "Epoch 196/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.5996 - mse: 0.5996 - mae: 0.5591 - val_loss: 0.6701 - val_mse: 0.6701 - val_mae: 0.6098\n",
      "Epoch 197/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 0s 1ms/step - loss: 0.5994 - mse: 0.5994 - mae: 0.5603 - val_loss: 0.6534 - val_mse: 0.6534 - val_mae: 0.5916\n",
      "Epoch 198/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.5989 - mse: 0.5989 - mae: 0.5593 - val_loss: 0.6505 - val_mse: 0.6505 - val_mae: 0.5834\n",
      "Epoch 199/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.6006 - mse: 0.6006 - mae: 0.5606 - val_loss: 0.6582 - val_mse: 0.6582 - val_mae: 0.5826\n",
      "Epoch 200/200\n",
      "229/229 [==============================] - 0s 1ms/step - loss: 0.5953 - mse: 0.5953 - mae: 0.5569 - val_loss: 0.6602 - val_mse: 0.6602 - val_mae: 0.5820\n"
     ]
    }
   ],
   "source": [
    "train_history = built_model.fit(\n",
    "    x_train_scaled,\n",
    "    y_train_scaled,\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    batch_size=100,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "026f7c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c0522",
   "metadata": {},
   "source": [
    "## plot loss of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6406b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_history.history.keys())\n",
    "# \"Loss\" val==validation==test\n",
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f686bf",
   "metadata": {},
   "source": [
    "## Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0ffa864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - 0s 554us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.6851612427924305"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_scaled = built_model.predict(x_test_scaled, verbose=1)  # Generates output predictions for the input samples\n",
    "# inverse y-pred to original scale\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "# compare y & y-hat in original scale\n",
    "rmse_pred_test = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rmse_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba00cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c6f4ab2",
   "metadata": {},
   "source": [
    "### time run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e418e2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:02.754526\n"
     ]
    }
   ],
   "source": [
    "now2 = dt.now()\n",
    "runtime = now2-now1\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc91f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607cb1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
